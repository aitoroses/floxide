{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Floxide \u00b6 This is the documentation site for the Floxide framework. Navigate using the menu to find: Floxide: Core concepts and tutorials. Floxide Tutorial: A step-by-step guide to distributed workflows. Contributing: Information on how to contribute to the project.","title":"Home"},{"location":"#welcome-to-floxide","text":"This is the documentation site for the Floxide framework. Navigate using the menu to find: Floxide: Core concepts and tutorials. Floxide Tutorial: A step-by-step guide to distributed workflows. Contributing: Information on how to contribute to the project.","title":"Welcome to Floxide"},{"location":"core-concepts/","text":"Floxide Core Concepts \u00b6 This section covers the fundamental building blocks of the Floxide framework. Understanding these concepts is essential for building robust and efficient workflows. Key Components \u00b6 Node : The basic unit of work in a workflow. Represents a single step or task. (See Node Concept ) Workflow : Defines the overall structure, connecting Nodes together in a directed graph. (See Workflow Concept ) Transition : The outcome of a Node's execution, determining the next step. (See Transition Concept ) Context ( WorkflowCtx ) : Shared data or state accessible throughout a workflow run. (See Context Concept ) Macros ( node! , workflow! ) : Declarative macros for defining Nodes and Workflows easily. Distributed Execution \u00b6 While the concepts above form the basis of any Floxide workflow, the framework provides additional components specifically designed for running workflows across multiple processes or machines: WorkQueue : A shared queue where pending tasks ( WorkItem s) are placed. Independent DistributedWorker processes pick up tasks from this queue. CheckpointStore : Stores the state ( Checkpoint ) of ongoing workflow runs, including the shared Context and the remaining tasks in the WorkQueue . This enables fault tolerance and recovery. DistributedWorker : A separate process responsible for dequeuing WorkItem s, loading the corresponding Checkpoint , executing the Node , saving the updated Checkpoint , and enqueuing successor WorkItem s. DistributedOrchestrator : Manages the overall lifecycle of distributed runs, providing an API to start, monitor, pause, resume, and cancel workflows. It often interacts with various Distributed Stores (like RunInfoStore , MetricsStore , ErrorStore ) for observability. These components work together to enable scalable and resilient workflow execution. You can learn more about them in the Distributed Tutorial . Further details on each core concept will be added here.","title":"Overview"},{"location":"core-concepts/#floxide-core-concepts","text":"This section covers the fundamental building blocks of the Floxide framework. Understanding these concepts is essential for building robust and efficient workflows.","title":"Floxide Core Concepts"},{"location":"core-concepts/#key-components","text":"Node : The basic unit of work in a workflow. Represents a single step or task. (See Node Concept ) Workflow : Defines the overall structure, connecting Nodes together in a directed graph. (See Workflow Concept ) Transition : The outcome of a Node's execution, determining the next step. (See Transition Concept ) Context ( WorkflowCtx ) : Shared data or state accessible throughout a workflow run. (See Context Concept ) Macros ( node! , workflow! ) : Declarative macros for defining Nodes and Workflows easily.","title":"Key Components"},{"location":"core-concepts/#distributed-execution","text":"While the concepts above form the basis of any Floxide workflow, the framework provides additional components specifically designed for running workflows across multiple processes or machines: WorkQueue : A shared queue where pending tasks ( WorkItem s) are placed. Independent DistributedWorker processes pick up tasks from this queue. CheckpointStore : Stores the state ( Checkpoint ) of ongoing workflow runs, including the shared Context and the remaining tasks in the WorkQueue . This enables fault tolerance and recovery. DistributedWorker : A separate process responsible for dequeuing WorkItem s, loading the corresponding Checkpoint , executing the Node , saving the updated Checkpoint , and enqueuing successor WorkItem s. DistributedOrchestrator : Manages the overall lifecycle of distributed runs, providing an API to start, monitor, pause, resume, and cancel workflows. It often interacts with various Distributed Stores (like RunInfoStore , MetricsStore , ErrorStore ) for observability. These components work together to enable scalable and resilient workflow execution. You can learn more about them in the Distributed Tutorial . Further details on each core concept will be added here.","title":"Distributed Execution"},{"location":"floxide-tutorial/","text":"Tutorial: Floxide, an easy distributed workflow engine \u00b6 Floxide is a workflow engine for Rust designed to make building and running distributed task sequences easy. Think of it like a digital assembly line where different steps ( Nodes ) can run on different computers. It uses a central WorkQueue to assign tasks to Workers and Checkpointing to recover from failures, making your workflows scalable and fault-tolerant . An Orchestrator manages starting, stopping, and monitoring these distributed runs. Source Repository: None flowchart TD A0[\"Node Trait & node! Macro\"] A1[\"Workflow Trait & workflow! Macro\"] A2[\"Transition Enum\"] A3[\"WorkflowCtx & Context Trait\"] A4[\"Checkpoint & CheckpointStore Trait\"] A5[\"WorkQueue Trait\"] A6[\"DistributedWorker\"] A7[\"DistributedOrchestrator\"] A8[\"Distributed Stores (RunInfoStore, MetricsStore, ErrorStore, LivenessStore, WorkItemStateStore)\"] A9[\"RetryPolicy & RetryNode\"] A1 -- \"Contains / Defines Flow Bet...\" --> A0 A0 -- \"Returns Outcome As\" --> A2 A0 -- \"Uses Shared Context\" --> A3 A9 -- \"Wraps Node For Retries\" --> A0 A1 -- \"Uses Workflow Context\" --> A3 A1 -- \"Uses For Distributed State\" --> A4 A1 -- \"Uses For Distributed Steps\" --> A5 A6 -- \"Executes Workflow Step\" --> A1 A6 -- \"Dequeues/Enqueues Work Items\" --> A5 A6 -- \"Loads/Saves Checkpoints\" --> A4 A6 -- \"Updates Run State/Metrics\" --> A8 A7 -- \"Starts Distributed Run\" --> A1 A7 -- \"Manages Run via Queue\" --> A5 A7 -- \"Manages Run via Checkpoints\" --> A4 A7 -- \"Queries Run State/Metrics\" --> A8 A1 -- \"Applies Retry Policy (via m...\" --> A9 A9 -- \"Uses Context for Delay\" --> A3 Chapters \u00b6 Transition Enum Node Trait & node! Macro WorkflowCtx & Context Trait Workflow Trait & workflow! Macro WorkQueue Trait Checkpoint & CheckpointStore Trait DistributedWorker DistributedOrchestrator Distributed Stores ( RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore ) RetryPolicy & RetryNode","title":"Introduction"},{"location":"floxide-tutorial/#tutorial-floxide-an-easy-distributed-workflow-engine","text":"Floxide is a workflow engine for Rust designed to make building and running distributed task sequences easy. Think of it like a digital assembly line where different steps ( Nodes ) can run on different computers. It uses a central WorkQueue to assign tasks to Workers and Checkpointing to recover from failures, making your workflows scalable and fault-tolerant . An Orchestrator manages starting, stopping, and monitoring these distributed runs. Source Repository: None flowchart TD A0[\"Node Trait & node! Macro\"] A1[\"Workflow Trait & workflow! Macro\"] A2[\"Transition Enum\"] A3[\"WorkflowCtx & Context Trait\"] A4[\"Checkpoint & CheckpointStore Trait\"] A5[\"WorkQueue Trait\"] A6[\"DistributedWorker\"] A7[\"DistributedOrchestrator\"] A8[\"Distributed Stores (RunInfoStore, MetricsStore, ErrorStore, LivenessStore, WorkItemStateStore)\"] A9[\"RetryPolicy & RetryNode\"] A1 -- \"Contains / Defines Flow Bet...\" --> A0 A0 -- \"Returns Outcome As\" --> A2 A0 -- \"Uses Shared Context\" --> A3 A9 -- \"Wraps Node For Retries\" --> A0 A1 -- \"Uses Workflow Context\" --> A3 A1 -- \"Uses For Distributed State\" --> A4 A1 -- \"Uses For Distributed Steps\" --> A5 A6 -- \"Executes Workflow Step\" --> A1 A6 -- \"Dequeues/Enqueues Work Items\" --> A5 A6 -- \"Loads/Saves Checkpoints\" --> A4 A6 -- \"Updates Run State/Metrics\" --> A8 A7 -- \"Starts Distributed Run\" --> A1 A7 -- \"Manages Run via Queue\" --> A5 A7 -- \"Manages Run via Checkpoints\" --> A4 A7 -- \"Queries Run State/Metrics\" --> A8 A1 -- \"Applies Retry Policy (via m...\" --> A9 A9 -- \"Uses Context for Delay\" --> A3","title":"Tutorial: Floxide, an easy distributed workflow engine"},{"location":"floxide-tutorial/#chapters","text":"Transition Enum Node Trait & node! Macro WorkflowCtx & Context Trait Workflow Trait & workflow! Macro WorkQueue Trait Checkpoint & CheckpointStore Trait DistributedWorker DistributedOrchestrator Distributed Stores ( RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore ) RetryPolicy & RetryNode","title":"Chapters"},{"location":"floxide-tutorial/01__transition__enum_/","text":"Chapter 1: Transition Enum \u00b6 Welcome to the Floxide tutorial! Floxide is designed to make building distributed workflows easy. Imagine you have a complex task, like processing a large video file. You might break it down into steps: download the video, extract audio, generate subtitles, re-encode, and upload. Floxide helps you define these steps and run them, potentially across many computers working together. This \"distributed\" nature is Floxide's superpower! In this first chapter, we'll look at a fundamental building block: the Transition enum. What's the Big Idea? Controlling the Flow \u00b6 Think about an assembly line in a factory. Each station performs a task on an item. When a worker finishes their task, they need to signal what should happen next. Should the item move to the next station? Did the task produce multiple items that need to go to the next stations? (Maybe splitting a large task?) Does the item need to wait at the current station? Did something go wrong , requiring the whole line to stop? In Floxide, each step in your workflow is called a \"Node\" (we'll cover this in the next chapter ). When a Node finishes processing an item, it needs to signal the outcome, just like the assembly line worker. This signal is represented by the Transition enum. It tells the Floxide engine how the workflow should proceed. The Transition Enum: Signals for Your Workflow \u00b6 The Transition enum has four possible variants, each representing a different signal: Transition::Next(output) : \"All good! Pass this single item to the next step(s).\" Analogy: The worker finishes assembling a part and puts the single completed part onto the conveyor belt for the next station. output : This is the data produced by the current Node that the next Node(s) will receive as input. Transition::NextAll(outputs) : \"I've split the work! Send each of these multiple items to the next step(s).\" Analogy: The worker takes a large sheet of metal, cuts it into several smaller pieces, and puts all those pieces onto the belt for the next stations. outputs : This is a Vec (a list) of data items. Floxide will typically create a separate task for each item in the list, potentially running them in parallel across different workers. This is key for distributed processing! Distributed Emphasis: This is how you can achieve parallelism. One Node can generate many pieces of work to be processed independently. Transition::Hold : \"Wait! Keep this item here for now.\" Analogy: The worker needs to wait for glue to dry or for another part to arrive before continuing. The item stays at their station. The workflow pauses processing for this specific item at this Node. It might be resumed later (though mechanisms for resumption are more advanced topics). Transition::Abort(error) : \"Emergency Stop! Something went wrong.\" Analogy: The worker discovers a critical defect or their machine breaks down. They hit the emergency stop button for the whole assembly line. error : Contains information about what went wrong. This typically stops the execution of the entire workflow run associated with this item. How Do Nodes Use Transition ? \u00b6 Nodes are the individual processing steps in your workflow. In Floxide, a Node is essentially a function (or more precisely, an object with a processing method) that takes some input and returns a Result containing a Transition . Let's imagine a very simple Node that just adds 1 to a number. use floxide_core :: transition :: Transition ; use floxide_core :: error :: FloxideError ; // This is a simplified function representing a Node's logic. // It takes an input number `i32`. // It returns Ok(Transition<i32>) on success or Err(FloxideError) on failure. fn process_number ( input : i32 ) -> Result < Transition < i32 > , FloxideError > { let result = input + 1 ; println! ( \"Processed {} -> {}\" , input , result ); // Signal that processing is done, and pass the 'result' to the next step. Ok ( Transition :: Next ( result )) } // --- Example Usage --- let outcome = process_number ( 5 ); // outcome will be Ok(Transition::Next(6)) println! ( \"{:?}\" , outcome ); This simple function calculates result and then returns Ok(Transition::Next(result)) . This tells the Floxide engine: \"Success! The result is 6 . Send this value ( 6 ) to the next Node(s) in the workflow.\" Now, let's see how other transitions might look: Example: Returning NextAll (Splitting Work) Imagine a Node that takes a sentence and splits it into words. use floxide_core :: transition :: Transition ; use floxide_core :: error :: FloxideError ; // Simplified Node logic: takes a sentence, returns individual words. fn split_sentence ( sentence : String ) -> Result < Transition < String > , FloxideError > { let words : Vec < String > = sentence . split_whitespace (). map ( String :: from ). collect (); if words . is_empty () { // If there are no words, maybe that's an error condition? Err ( FloxideError :: Generic ( \"Input sentence was empty\" . to_string ())) } else { println! ( \"Splitting '{}' into: {:?}\" , sentence , words ); // Signal to send *each word* as a separate item to the next step(s). Ok ( Transition :: NextAll ( words )) } } // --- Example Usage --- let outcome = split_sentence ( \"Floxide is distributed\" . to_string ()); // outcome will be Ok(Transition::NextAll(vec![\"Floxide\".to_string(), \"is\".to_string(), \"distributed\".to_string()])) println! ( \"{:?}\" , outcome ); Here, Transition::NextAll(words) tells the engine: \"I've produced multiple results ( Floxide , is , distributed ). Create separate tasks for each of these and send them to the next Node(s).\" This is powerful for distributing the processing of each word. Example: Returning Abort use floxide_core :: transition :: Transition ; use floxide_core :: error :: FloxideError ; // Simplified Node logic: checks if a number is too large. fn check_number ( input : i32 ) -> Result < Transition < i32 > , FloxideError > { if input > 100 { println! ( \"Input {} is too large! Aborting.\" , input ); // Signal that a critical error occurred. Ok ( Transition :: Abort ( FloxideError :: Generic ( \"Value exceeded threshold\" . to_string ()))) } else { println! ( \"Input {} is okay. Proceeding.\" , input ); // Signal to proceed normally. Ok ( Transition :: Next ( input )) // Pass the original input along } } // --- Example Usage --- let outcome1 = check_number ( 50 ); // Ok(Transition::Next(50)) let outcome2 = check_number ( 150 ); // Ok(Transition::Abort(FloxideError::Generic(...))) println! ( \"Outcome 1: {:?}\" , outcome1 ); println! ( \"Outcome 2: {:?}\" , outcome2 ); If the input is too large, Transition::Abort signals that the workflow run should stop due to an error. How Floxide Uses the Transition Signal \u00b6 So, your Node returns a Transition . What does the Floxide engine do with it? Receive: The engine calls your Node's processing logic and gets back the Result<Transition<Output>, FloxideError> . Check Result: It first checks if the Result itself is an Err . If so, it usually treats it like an Abort . Interpret Transition: If the Result is Ok(transition) , the engine looks at the transition variant: Next(output) : The engine looks up which Node(s) should run next according to your workflow definition ( workflow! macro ). It then prepares a new task for the next Node(s), giving them the output data. In a distributed setup, it might place this task onto a shared WorkQueue . NextAll(outputs) : Similar to Next , but it iterates through the outputs list. For each item in the list, it prepares a new task for the next Node(s) and adds it to the WorkQueue . This fans out the work. Hold : The engine notes that this item is paused at this Node. No further tasks are scheduled for this item immediately. Abort(error) : The engine stops processing for this specific workflow run. It logs the error (perhaps using a distributed ErrorStore ) and potentially cleans up resources. Here's a simplified diagram showing how the engine reacts to Next and NextAll : sequenceDiagram participant NodeLogic as Node Logic participant Engine as Floxide Engine participant Queue as Work Queue Note over NodeLogic, Engine: Process item 'A' NodeLogic->>Engine: Return Ok(Transition::Next(data_B)) Engine->>Engine: Lookup next node(s) for item 'A' (e.g., NodeY) Engine->>Queue: Enqueue task (NodeY, data_B) for run 'A' Note over NodeLogic, Engine: Process item 'X' NodeLogic->>Engine: Return Ok(Transition::NextAll([data_Y1, data_Y2])) Engine->>Engine: Lookup next node(s) for item 'X' (e.g., NodeZ) Engine->>Queue: Enqueue task (NodeZ, data_Y1) for run 'X' Engine->>Queue: Enqueue task (NodeZ, data_Y2) for run 'X' This diagram illustrates how Next leads to one new task, while NextAll leads to multiple new tasks, enabling parallel and distributed execution. Under the Hood: The Enum Definition \u00b6 The actual definition in Floxide is straightforward: // From: crates/floxide-core/src/transition.rs /// Transition result of a node. pub enum Transition < Output > { /// Emit a single output to successors. Next ( Output ), /// Emit multiple outputs to successors (split / fan-out). NextAll ( Vec < Output > ), /// Hold this work item; do not emit any outputs until a condition is met. Hold , /// Abort the workflow with an error. Abort ( crate :: error :: FloxideError ), } It's a standard Rust enum . It's generic over <Output> . This means the Next and NextAll variants can carry any type of data needed for the next step, as long as that type is specified by the Node. Hold and Abort don't carry a successful output, so they don't use the Output type. Abort carries a FloxideError , which is Floxide's standard error type. Conclusion \u00b6 The Transition enum is the fundamental way that individual steps (Nodes) in a Floxide workflow communicate their outcome and control the flow of execution. By returning Next , NextAll , Hold , or Abort , a Node tells the Floxide engine whether to proceed with a single item, split the work into multiple items (crucial for distributed processing), pause, or stop due to an error. Now that you understand how a Node signals its result, let's dive into how to actually define these Nodes! Next: Chapter 2: Node Trait & node! Macro","title":"Chapter 1: Transition Enum"},{"location":"floxide-tutorial/01__transition__enum_/#chapter-1-transition-enum","text":"Welcome to the Floxide tutorial! Floxide is designed to make building distributed workflows easy. Imagine you have a complex task, like processing a large video file. You might break it down into steps: download the video, extract audio, generate subtitles, re-encode, and upload. Floxide helps you define these steps and run them, potentially across many computers working together. This \"distributed\" nature is Floxide's superpower! In this first chapter, we'll look at a fundamental building block: the Transition enum.","title":"Chapter 1: Transition Enum"},{"location":"floxide-tutorial/01__transition__enum_/#whats-the-big-idea-controlling-the-flow","text":"Think about an assembly line in a factory. Each station performs a task on an item. When a worker finishes their task, they need to signal what should happen next. Should the item move to the next station? Did the task produce multiple items that need to go to the next stations? (Maybe splitting a large task?) Does the item need to wait at the current station? Did something go wrong , requiring the whole line to stop? In Floxide, each step in your workflow is called a \"Node\" (we'll cover this in the next chapter ). When a Node finishes processing an item, it needs to signal the outcome, just like the assembly line worker. This signal is represented by the Transition enum. It tells the Floxide engine how the workflow should proceed.","title":"What's the Big Idea? Controlling the Flow"},{"location":"floxide-tutorial/01__transition__enum_/#the-transition-enum-signals-for-your-workflow","text":"The Transition enum has four possible variants, each representing a different signal: Transition::Next(output) : \"All good! Pass this single item to the next step(s).\" Analogy: The worker finishes assembling a part and puts the single completed part onto the conveyor belt for the next station. output : This is the data produced by the current Node that the next Node(s) will receive as input. Transition::NextAll(outputs) : \"I've split the work! Send each of these multiple items to the next step(s).\" Analogy: The worker takes a large sheet of metal, cuts it into several smaller pieces, and puts all those pieces onto the belt for the next stations. outputs : This is a Vec (a list) of data items. Floxide will typically create a separate task for each item in the list, potentially running them in parallel across different workers. This is key for distributed processing! Distributed Emphasis: This is how you can achieve parallelism. One Node can generate many pieces of work to be processed independently. Transition::Hold : \"Wait! Keep this item here for now.\" Analogy: The worker needs to wait for glue to dry or for another part to arrive before continuing. The item stays at their station. The workflow pauses processing for this specific item at this Node. It might be resumed later (though mechanisms for resumption are more advanced topics). Transition::Abort(error) : \"Emergency Stop! Something went wrong.\" Analogy: The worker discovers a critical defect or their machine breaks down. They hit the emergency stop button for the whole assembly line. error : Contains information about what went wrong. This typically stops the execution of the entire workflow run associated with this item.","title":"The Transition Enum: Signals for Your Workflow"},{"location":"floxide-tutorial/01__transition__enum_/#how-do-nodes-use-transition","text":"Nodes are the individual processing steps in your workflow. In Floxide, a Node is essentially a function (or more precisely, an object with a processing method) that takes some input and returns a Result containing a Transition . Let's imagine a very simple Node that just adds 1 to a number. use floxide_core :: transition :: Transition ; use floxide_core :: error :: FloxideError ; // This is a simplified function representing a Node's logic. // It takes an input number `i32`. // It returns Ok(Transition<i32>) on success or Err(FloxideError) on failure. fn process_number ( input : i32 ) -> Result < Transition < i32 > , FloxideError > { let result = input + 1 ; println! ( \"Processed {} -> {}\" , input , result ); // Signal that processing is done, and pass the 'result' to the next step. Ok ( Transition :: Next ( result )) } // --- Example Usage --- let outcome = process_number ( 5 ); // outcome will be Ok(Transition::Next(6)) println! ( \"{:?}\" , outcome ); This simple function calculates result and then returns Ok(Transition::Next(result)) . This tells the Floxide engine: \"Success! The result is 6 . Send this value ( 6 ) to the next Node(s) in the workflow.\" Now, let's see how other transitions might look: Example: Returning NextAll (Splitting Work) Imagine a Node that takes a sentence and splits it into words. use floxide_core :: transition :: Transition ; use floxide_core :: error :: FloxideError ; // Simplified Node logic: takes a sentence, returns individual words. fn split_sentence ( sentence : String ) -> Result < Transition < String > , FloxideError > { let words : Vec < String > = sentence . split_whitespace (). map ( String :: from ). collect (); if words . is_empty () { // If there are no words, maybe that's an error condition? Err ( FloxideError :: Generic ( \"Input sentence was empty\" . to_string ())) } else { println! ( \"Splitting '{}' into: {:?}\" , sentence , words ); // Signal to send *each word* as a separate item to the next step(s). Ok ( Transition :: NextAll ( words )) } } // --- Example Usage --- let outcome = split_sentence ( \"Floxide is distributed\" . to_string ()); // outcome will be Ok(Transition::NextAll(vec![\"Floxide\".to_string(), \"is\".to_string(), \"distributed\".to_string()])) println! ( \"{:?}\" , outcome ); Here, Transition::NextAll(words) tells the engine: \"I've produced multiple results ( Floxide , is , distributed ). Create separate tasks for each of these and send them to the next Node(s).\" This is powerful for distributing the processing of each word. Example: Returning Abort use floxide_core :: transition :: Transition ; use floxide_core :: error :: FloxideError ; // Simplified Node logic: checks if a number is too large. fn check_number ( input : i32 ) -> Result < Transition < i32 > , FloxideError > { if input > 100 { println! ( \"Input {} is too large! Aborting.\" , input ); // Signal that a critical error occurred. Ok ( Transition :: Abort ( FloxideError :: Generic ( \"Value exceeded threshold\" . to_string ()))) } else { println! ( \"Input {} is okay. Proceeding.\" , input ); // Signal to proceed normally. Ok ( Transition :: Next ( input )) // Pass the original input along } } // --- Example Usage --- let outcome1 = check_number ( 50 ); // Ok(Transition::Next(50)) let outcome2 = check_number ( 150 ); // Ok(Transition::Abort(FloxideError::Generic(...))) println! ( \"Outcome 1: {:?}\" , outcome1 ); println! ( \"Outcome 2: {:?}\" , outcome2 ); If the input is too large, Transition::Abort signals that the workflow run should stop due to an error.","title":"How Do Nodes Use Transition?"},{"location":"floxide-tutorial/01__transition__enum_/#how-floxide-uses-the-transition-signal","text":"So, your Node returns a Transition . What does the Floxide engine do with it? Receive: The engine calls your Node's processing logic and gets back the Result<Transition<Output>, FloxideError> . Check Result: It first checks if the Result itself is an Err . If so, it usually treats it like an Abort . Interpret Transition: If the Result is Ok(transition) , the engine looks at the transition variant: Next(output) : The engine looks up which Node(s) should run next according to your workflow definition ( workflow! macro ). It then prepares a new task for the next Node(s), giving them the output data. In a distributed setup, it might place this task onto a shared WorkQueue . NextAll(outputs) : Similar to Next , but it iterates through the outputs list. For each item in the list, it prepares a new task for the next Node(s) and adds it to the WorkQueue . This fans out the work. Hold : The engine notes that this item is paused at this Node. No further tasks are scheduled for this item immediately. Abort(error) : The engine stops processing for this specific workflow run. It logs the error (perhaps using a distributed ErrorStore ) and potentially cleans up resources. Here's a simplified diagram showing how the engine reacts to Next and NextAll : sequenceDiagram participant NodeLogic as Node Logic participant Engine as Floxide Engine participant Queue as Work Queue Note over NodeLogic, Engine: Process item 'A' NodeLogic->>Engine: Return Ok(Transition::Next(data_B)) Engine->>Engine: Lookup next node(s) for item 'A' (e.g., NodeY) Engine->>Queue: Enqueue task (NodeY, data_B) for run 'A' Note over NodeLogic, Engine: Process item 'X' NodeLogic->>Engine: Return Ok(Transition::NextAll([data_Y1, data_Y2])) Engine->>Engine: Lookup next node(s) for item 'X' (e.g., NodeZ) Engine->>Queue: Enqueue task (NodeZ, data_Y1) for run 'X' Engine->>Queue: Enqueue task (NodeZ, data_Y2) for run 'X' This diagram illustrates how Next leads to one new task, while NextAll leads to multiple new tasks, enabling parallel and distributed execution.","title":"How Floxide Uses the Transition Signal"},{"location":"floxide-tutorial/01__transition__enum_/#under-the-hood-the-enum-definition","text":"The actual definition in Floxide is straightforward: // From: crates/floxide-core/src/transition.rs /// Transition result of a node. pub enum Transition < Output > { /// Emit a single output to successors. Next ( Output ), /// Emit multiple outputs to successors (split / fan-out). NextAll ( Vec < Output > ), /// Hold this work item; do not emit any outputs until a condition is met. Hold , /// Abort the workflow with an error. Abort ( crate :: error :: FloxideError ), } It's a standard Rust enum . It's generic over <Output> . This means the Next and NextAll variants can carry any type of data needed for the next step, as long as that type is specified by the Node. Hold and Abort don't carry a successful output, so they don't use the Output type. Abort carries a FloxideError , which is Floxide's standard error type.","title":"Under the Hood: The Enum Definition"},{"location":"floxide-tutorial/01__transition__enum_/#conclusion","text":"The Transition enum is the fundamental way that individual steps (Nodes) in a Floxide workflow communicate their outcome and control the flow of execution. By returning Next , NextAll , Hold , or Abort , a Node tells the Floxide engine whether to proceed with a single item, split the work into multiple items (crucial for distributed processing), pause, or stop due to an error. Now that you understand how a Node signals its result, let's dive into how to actually define these Nodes! Next: Chapter 2: Node Trait & node! Macro","title":"Conclusion"},{"location":"floxide-tutorial/02__node__trait____node___macro_/","text":"Chapter 2: Node Trait & node! Macro \u00b6 In the previous chapter , we learned about the Transition enum, which is like a signal sent by a workflow step to tell the Floxide engine what to do next. Now, let's talk about the source of that signal: the Node . What's the Problem? Defining the Steps \u00b6 Imagine our video processing workflow again: Download -> Extract Audio -> Generate Subtitles -> Re-encode -> Upload. Each of these actions is a distinct step. We need a way to define: What specific task does this step perform? (e.g., \"Extract Audio\") What kind of data does it need to start? (e.g., a path to the downloaded video file) What kind of data does it produce? (e.g., a path to the extracted audio file) Does it need any shared information? (e.g., API keys for an external service) In Floxide, the Node is the fundamental building block that represents exactly such a single, self-contained step or task within your workflow. Think of it as one specific station on our distributed assembly line. What is a Node ? \u00b6 A Node is essentially a piece of code that: Receives an input: The data it needs to work on. Performs processing: Runs its specific logic (like capitalizing text, calling an API, processing a file chunk). Uses Context (Optional): Might access shared information available to the whole workflow run (we'll cover this in detail in Chapter 3: WorkflowCtx & Context Trait ). Returns a Transition : Signals the outcome ( Transition::Next(output) , Transition::NextAll(outputs) , etc.), potentially including the output data for the next step(s). Distributed Emphasis: Each Node defines a unit of work. When you run a Floxide workflow, especially in a distributed manner, different Nodes (or different instances of the same Node processing different data items) can be executed by different DistributedWorker processes, potentially running on separate machines. This is how Floxide distributes the workload. The Node Trait: The Blueprint \u00b6 How does Floxide know what a \"Node\" looks like? It uses a Rust trait . A trait is like a contract or a blueprint that defines a set of methods a type must implement. The Node trait specifies the core functionality required for any workflow step. The most important part of the Node trait is the process method signature (simplified): // Simplified concept trait Node < Context > { type Input ; // What data type does this Node expect? type Output ; // What data type does this Node produce? // The core logic: takes context and input, returns a Transition async fn process ( & self , ctx : & Context , input : Self :: Input ) -> Result < Transition < Self :: Output > , FloxideError > ; } This tells us: A Node is associated with a specific Context type (shared info). It defines an Input type and an Output type. It has an async fn process method. This method: Is async because Nodes often perform I/O (like network requests or file access) which benefits from asynchronous execution. Receives a reference to the Context ( ctx ) and the Input data ( input ). Returns a Result which, on success ( Ok ), contains a Transition carrying the Output data, or an Err containing a FloxideError if something went wrong during processing. You could implement this trait manually for your structs, but Floxide provides a much easier way! The node! Macro: Your Node-Building Assistant \u00b6 Manually implementing the Node trait for every step can be repetitive. Floxide offers the node! macro to streamline this process significantly. It lets you define the Node's structure (its data fields), its input/output types, the context it needs, and its processing logic all in one place. Here's the basic syntax: // General syntax of the node! macro node ! { // 1. Define the struct (visibility, name, fields) pub struct MyNode { // Fields store configuration or state for this node instance config_value : String , retry_count : u32 , } // 2. Specify the Context type this node needs context = MyWorkflowContext ; // 3. Specify the Input data type input = InputDataType ; // 4. Specify the Output data type (inside Transition::Next/NextAll) output = OutputDataType ; // 5. Define the processing logic as a closure // |context_arg, input_arg| -> Result<Transition<OutputDataType>, FloxideError> | ctx , data | { // Access node fields using `self.field_name` println! ( \"Config: {}\" , self . config_value ); // Access context using the `ctx` argument // let shared_info = ctx.get_shared_info(); // Access input using the `data` argument println! ( \"Processing input: {:?}\" , data ); // --- Your Node's specific logic goes here --- let result = perform_task ( data ) ? ; // Example task // Return a Transition Ok ( Transition :: Next ( result )) } } Let's break down the key parts: Struct Definition: pub struct MyNode { ... } defines a regular Rust struct. You can add fields here to store configuration specific to this Node instance (like an API key, a file path template, retry settings, etc.). context = Type; : Specifies the type of the shared workflow context this Node expects. We'll cover context in the next chapter . For simple nodes, you might use () (the unit type, meaning no context). input = Type; : Specifies the data type the process logic will receive as input. output = Type; : Specifies the data type that will be inside the Transition::Next or Transition::NextAll if the Node succeeds. Processing Logic Closure: |ctx, data| { ... } is where you write the core logic of your Node. The arguments ( ctx , data ) give you access to the workflow context and the input data. Their types match what you specified in context = ... and input = ... . Inside the closure, you can access the Node's own fields using self (e.g., self.config_value ). The closure must return a Result<Transition<OutputType>, FloxideError> , matching the Node trait's process signature. Example: A Simple CapitalizeTextNode \u00b6 Let's create a Node that takes a String , capitalizes it, and passes it on. // Needed imports for the example use floxide :: node ; // The macro itself use floxide :: Node ; // The trait (needed for type bounds) use floxide :: Context ; // Trait for context types use floxide :: Transition ; // The enum from Chapter 1 use floxide :: FloxideError ; // Standard error type // Define a simple empty context struct (more in Chapter 3) #[derive(Clone, Debug)] struct SimpleContext {} impl Context for SimpleContext {} // Mark it as a valid context type // Use the node! macro to define our capitalizing node node ! { pub struct CapitalizeTextNode {} // No fields needed for this simple node context = SimpleContext ; // Uses our simple context input = String ; // Expects a String as input output = String ; // Produces a String as output // The processing logic: | ctx , text | { // We get context (ctx) and input (text) println! ( \"Node: Capitalizing '{}'\" , text ); let capitalized_text = text . to_uppercase (); // Signal success and pass the capitalized text to the next step Ok ( Transition :: Next ( capitalized_text )) } } // --- How you might use it (conceptual) --- // let node_instance = CapitalizeTextNode {}; // let context_instance = SimpleContext {}; // let input_data = \"hello distributed world\".to_string(); // // // Floxide engine would call this internally: // // let result = node_instance.process(&context_instance, input_data).await; // // result would be Ok(Transition::Next(\"HELLO DISTRIBUTED WORLD\".to_string())) Explanation: We define CapitalizeTextNode with no fields. We specify it uses SimpleContext (an empty context for now), takes a String input, and produces a String output. The closure |ctx, text| { ... } receives the context and the input string ( text ). Inside, it performs the to_uppercase() operation. It returns Ok(Transition::Next(capitalized_text)) , signaling success and providing the result for the next Node(s). Distributed Emphasis: When this CapitalizeTextNode is part of a larger workflow, the Floxide engine can schedule its execution. If you use Transition::NextAll in a previous node to generate many strings, the engine could potentially run multiple instances of CapitalizeTextNode in parallel on different workers, each capitalizing a different string. Example: Using Node Fields for Configuration \u00b6 Let's make the previous example slightly more complex. We'll add a prefix that the Node should add before capitalizing. This prefix will be stored in a field. use floxide ::{ node , Node , Context , Transition , FloxideError }; #[derive(Clone, Debug)] struct SimpleContext {} impl Context for SimpleContext {} node ! { // Define the struct with a field pub struct PrefixAndCapitalizeNode { prefix : String , // Store the prefix here } context = SimpleContext ; input = String ; output = String ; | ctx , text | { // Access the node's field using `self.prefix` println! ( \"Node: Adding prefix '{}' and capitalizing '{}'\" , self . prefix , text ); let combined = format! ( \"{}: {}\" , self . prefix , text ); let result = combined . to_uppercase (); Ok ( Transition :: Next ( result )) } } // --- How you might use it (conceptual) --- // let node_instance = PrefixAndCapitalizeNode { // prefix: \"LOG\".to_string(), // Configure the node instance! // }; // let context_instance = SimpleContext {}; // let input_data = \"task completed\".to_string(); // // // Floxide engine calls: // // let result = node_instance.process(&context_instance, input_data).await; // // result would be Ok(Transition::Next(\"LOG: TASK COMPLETED\".to_string())) Explanation: We added a prefix: String field to the PrefixAndCapitalizeNode struct definition within the macro. Inside the processing logic closure, we can now access this field using self.prefix . When creating an instance of this Node ( PrefixAndCapitalizeNode { ... } ), we provide the specific prefix value. This allows different instances of the same type of Node to behave differently based on their configuration. Under the Hood: What Does node! Do? \u00b6 The node! macro is a piece of \"metaprogramming\" \u2013 code that writes code. When you use node! , the Rust compiler expands it into more detailed code before compiling the rest of your program. Essentially, node! generates two main things: The Struct: The pub struct YourNodeName { ... } definition you provided. The impl Node Block: It automatically writes the impl Node<ContextType> for YourNodeName { ... } block for you. It fills in the Input and Output associated types and creates the async fn process(...) method, placing the closure code you wrote inside that method's body. Here's a simplified view of the generated code for PrefixAndCapitalizeNode : // --- Code GENERATED by the `node!` macro (conceptual) --- // 1. The struct definition #[derive(Clone, Debug)] // Attributes added by the macro pub struct PrefixAndCapitalizeNode { pub prefix : String , // Fields become public } // 2. The implementation of the Node trait #[::async_trait::async_trait] // Macro adds async_trait support impl :: floxide_core :: node :: Node < SimpleContext > for PrefixAndCapitalizeNode where SimpleContext : Clone + Send + Sync // Adds necessary trait bounds { type Input = String ; // Sets the Input type type Output = String ; // Sets the Output type // Defines the process method async fn process ( & self , // `self` gives access to fields like `self.prefix` ctx : & SimpleContext , // Context argument text : String // Input argument (matches `input = ...`) ) -> Result < :: floxide_core :: transition :: Transition < String > , :: floxide_core :: error :: FloxideError > { // Your closure code is placed inside here! let ctx = ctx ; // Makes args available by original names let text = text ; // --- Your logic from the macro --- println! ( \"Node: Adding prefix '{}' and capitalizing '{}'\" , self . prefix , text ); let combined = format! ( \"{}: {}\" , self . prefix , text ); let result = combined . to_uppercase (); Ok ( Transition :: Next ( result )) // --- End of your logic --- } } // --- End of generated code --- You don't need to write this boilerplate code yourself; the node! macro handles it for you! Here's a diagram showing how the engine interacts with a Node created by the macro: sequenceDiagram participant Engine as Floxide Engine participant NodeInstance as YourNode (e.g., PrefixAndCapitalizeNode) participant NodeLogic as Your Logic (inside macro) Engine->>NodeInstance: Call process(context, input_data) Note over NodeInstance: process method generated by node! macro NodeInstance->>NodeLogic: Execute the closure code you wrote Note over NodeLogic: Accesses self.fields, context, input_data NodeLogic-->>NodeInstance: Return Result<Transition, Error> NodeInstance-->>Engine: Forward the Result<Transition, Error> Engine->>Engine: Act based on the Transition (e.g., schedule next step) The actual macro implementation lives in floxide-macros/src/node.rs and the Node trait definition is in floxide-core/src/node.rs . Conclusion \u00b6 The Node is the core unit of work in a Floxide workflow, representing a single processing step. The Node trait defines the contract for these steps, specifying their input, output, context needs, and the essential process method. While you can implement the Node trait manually, the node! macro provides a convenient and concise way to define a Node's structure (fields) and its processing logic all at once, generating the necessary boilerplate code for you. These Nodes are the building blocks that Floxide can distribute across different workers for parallel execution. Now that we know how to define individual steps (Nodes), how do they access shared information relevant to the entire workflow run? That's where context comes in. Next: Chapter 3: WorkflowCtx & Context Trait","title":"Chapter 2: Node Trait & node! Macro"},{"location":"floxide-tutorial/02__node__trait____node___macro_/#chapter-2-node-trait-node-macro","text":"In the previous chapter , we learned about the Transition enum, which is like a signal sent by a workflow step to tell the Floxide engine what to do next. Now, let's talk about the source of that signal: the Node .","title":"Chapter 2: Node Trait &amp; node! Macro"},{"location":"floxide-tutorial/02__node__trait____node___macro_/#whats-the-problem-defining-the-steps","text":"Imagine our video processing workflow again: Download -> Extract Audio -> Generate Subtitles -> Re-encode -> Upload. Each of these actions is a distinct step. We need a way to define: What specific task does this step perform? (e.g., \"Extract Audio\") What kind of data does it need to start? (e.g., a path to the downloaded video file) What kind of data does it produce? (e.g., a path to the extracted audio file) Does it need any shared information? (e.g., API keys for an external service) In Floxide, the Node is the fundamental building block that represents exactly such a single, self-contained step or task within your workflow. Think of it as one specific station on our distributed assembly line.","title":"What's the Problem? Defining the Steps"},{"location":"floxide-tutorial/02__node__trait____node___macro_/#what-is-a-node","text":"A Node is essentially a piece of code that: Receives an input: The data it needs to work on. Performs processing: Runs its specific logic (like capitalizing text, calling an API, processing a file chunk). Uses Context (Optional): Might access shared information available to the whole workflow run (we'll cover this in detail in Chapter 3: WorkflowCtx & Context Trait ). Returns a Transition : Signals the outcome ( Transition::Next(output) , Transition::NextAll(outputs) , etc.), potentially including the output data for the next step(s). Distributed Emphasis: Each Node defines a unit of work. When you run a Floxide workflow, especially in a distributed manner, different Nodes (or different instances of the same Node processing different data items) can be executed by different DistributedWorker processes, potentially running on separate machines. This is how Floxide distributes the workload.","title":"What is a Node?"},{"location":"floxide-tutorial/02__node__trait____node___macro_/#the-node-trait-the-blueprint","text":"How does Floxide know what a \"Node\" looks like? It uses a Rust trait . A trait is like a contract or a blueprint that defines a set of methods a type must implement. The Node trait specifies the core functionality required for any workflow step. The most important part of the Node trait is the process method signature (simplified): // Simplified concept trait Node < Context > { type Input ; // What data type does this Node expect? type Output ; // What data type does this Node produce? // The core logic: takes context and input, returns a Transition async fn process ( & self , ctx : & Context , input : Self :: Input ) -> Result < Transition < Self :: Output > , FloxideError > ; } This tells us: A Node is associated with a specific Context type (shared info). It defines an Input type and an Output type. It has an async fn process method. This method: Is async because Nodes often perform I/O (like network requests or file access) which benefits from asynchronous execution. Receives a reference to the Context ( ctx ) and the Input data ( input ). Returns a Result which, on success ( Ok ), contains a Transition carrying the Output data, or an Err containing a FloxideError if something went wrong during processing. You could implement this trait manually for your structs, but Floxide provides a much easier way!","title":"The Node Trait: The Blueprint"},{"location":"floxide-tutorial/02__node__trait____node___macro_/#the-node-macro-your-node-building-assistant","text":"Manually implementing the Node trait for every step can be repetitive. Floxide offers the node! macro to streamline this process significantly. It lets you define the Node's structure (its data fields), its input/output types, the context it needs, and its processing logic all in one place. Here's the basic syntax: // General syntax of the node! macro node ! { // 1. Define the struct (visibility, name, fields) pub struct MyNode { // Fields store configuration or state for this node instance config_value : String , retry_count : u32 , } // 2. Specify the Context type this node needs context = MyWorkflowContext ; // 3. Specify the Input data type input = InputDataType ; // 4. Specify the Output data type (inside Transition::Next/NextAll) output = OutputDataType ; // 5. Define the processing logic as a closure // |context_arg, input_arg| -> Result<Transition<OutputDataType>, FloxideError> | ctx , data | { // Access node fields using `self.field_name` println! ( \"Config: {}\" , self . config_value ); // Access context using the `ctx` argument // let shared_info = ctx.get_shared_info(); // Access input using the `data` argument println! ( \"Processing input: {:?}\" , data ); // --- Your Node's specific logic goes here --- let result = perform_task ( data ) ? ; // Example task // Return a Transition Ok ( Transition :: Next ( result )) } } Let's break down the key parts: Struct Definition: pub struct MyNode { ... } defines a regular Rust struct. You can add fields here to store configuration specific to this Node instance (like an API key, a file path template, retry settings, etc.). context = Type; : Specifies the type of the shared workflow context this Node expects. We'll cover context in the next chapter . For simple nodes, you might use () (the unit type, meaning no context). input = Type; : Specifies the data type the process logic will receive as input. output = Type; : Specifies the data type that will be inside the Transition::Next or Transition::NextAll if the Node succeeds. Processing Logic Closure: |ctx, data| { ... } is where you write the core logic of your Node. The arguments ( ctx , data ) give you access to the workflow context and the input data. Their types match what you specified in context = ... and input = ... . Inside the closure, you can access the Node's own fields using self (e.g., self.config_value ). The closure must return a Result<Transition<OutputType>, FloxideError> , matching the Node trait's process signature.","title":"The node! Macro: Your Node-Building Assistant"},{"location":"floxide-tutorial/02__node__trait____node___macro_/#example-a-simple-capitalizetextnode","text":"Let's create a Node that takes a String , capitalizes it, and passes it on. // Needed imports for the example use floxide :: node ; // The macro itself use floxide :: Node ; // The trait (needed for type bounds) use floxide :: Context ; // Trait for context types use floxide :: Transition ; // The enum from Chapter 1 use floxide :: FloxideError ; // Standard error type // Define a simple empty context struct (more in Chapter 3) #[derive(Clone, Debug)] struct SimpleContext {} impl Context for SimpleContext {} // Mark it as a valid context type // Use the node! macro to define our capitalizing node node ! { pub struct CapitalizeTextNode {} // No fields needed for this simple node context = SimpleContext ; // Uses our simple context input = String ; // Expects a String as input output = String ; // Produces a String as output // The processing logic: | ctx , text | { // We get context (ctx) and input (text) println! ( \"Node: Capitalizing '{}'\" , text ); let capitalized_text = text . to_uppercase (); // Signal success and pass the capitalized text to the next step Ok ( Transition :: Next ( capitalized_text )) } } // --- How you might use it (conceptual) --- // let node_instance = CapitalizeTextNode {}; // let context_instance = SimpleContext {}; // let input_data = \"hello distributed world\".to_string(); // // // Floxide engine would call this internally: // // let result = node_instance.process(&context_instance, input_data).await; // // result would be Ok(Transition::Next(\"HELLO DISTRIBUTED WORLD\".to_string())) Explanation: We define CapitalizeTextNode with no fields. We specify it uses SimpleContext (an empty context for now), takes a String input, and produces a String output. The closure |ctx, text| { ... } receives the context and the input string ( text ). Inside, it performs the to_uppercase() operation. It returns Ok(Transition::Next(capitalized_text)) , signaling success and providing the result for the next Node(s). Distributed Emphasis: When this CapitalizeTextNode is part of a larger workflow, the Floxide engine can schedule its execution. If you use Transition::NextAll in a previous node to generate many strings, the engine could potentially run multiple instances of CapitalizeTextNode in parallel on different workers, each capitalizing a different string.","title":"Example: A Simple CapitalizeTextNode"},{"location":"floxide-tutorial/02__node__trait____node___macro_/#example-using-node-fields-for-configuration","text":"Let's make the previous example slightly more complex. We'll add a prefix that the Node should add before capitalizing. This prefix will be stored in a field. use floxide ::{ node , Node , Context , Transition , FloxideError }; #[derive(Clone, Debug)] struct SimpleContext {} impl Context for SimpleContext {} node ! { // Define the struct with a field pub struct PrefixAndCapitalizeNode { prefix : String , // Store the prefix here } context = SimpleContext ; input = String ; output = String ; | ctx , text | { // Access the node's field using `self.prefix` println! ( \"Node: Adding prefix '{}' and capitalizing '{}'\" , self . prefix , text ); let combined = format! ( \"{}: {}\" , self . prefix , text ); let result = combined . to_uppercase (); Ok ( Transition :: Next ( result )) } } // --- How you might use it (conceptual) --- // let node_instance = PrefixAndCapitalizeNode { // prefix: \"LOG\".to_string(), // Configure the node instance! // }; // let context_instance = SimpleContext {}; // let input_data = \"task completed\".to_string(); // // // Floxide engine calls: // // let result = node_instance.process(&context_instance, input_data).await; // // result would be Ok(Transition::Next(\"LOG: TASK COMPLETED\".to_string())) Explanation: We added a prefix: String field to the PrefixAndCapitalizeNode struct definition within the macro. Inside the processing logic closure, we can now access this field using self.prefix . When creating an instance of this Node ( PrefixAndCapitalizeNode { ... } ), we provide the specific prefix value. This allows different instances of the same type of Node to behave differently based on their configuration.","title":"Example: Using Node Fields for Configuration"},{"location":"floxide-tutorial/02__node__trait____node___macro_/#under-the-hood-what-does-node-do","text":"The node! macro is a piece of \"metaprogramming\" \u2013 code that writes code. When you use node! , the Rust compiler expands it into more detailed code before compiling the rest of your program. Essentially, node! generates two main things: The Struct: The pub struct YourNodeName { ... } definition you provided. The impl Node Block: It automatically writes the impl Node<ContextType> for YourNodeName { ... } block for you. It fills in the Input and Output associated types and creates the async fn process(...) method, placing the closure code you wrote inside that method's body. Here's a simplified view of the generated code for PrefixAndCapitalizeNode : // --- Code GENERATED by the `node!` macro (conceptual) --- // 1. The struct definition #[derive(Clone, Debug)] // Attributes added by the macro pub struct PrefixAndCapitalizeNode { pub prefix : String , // Fields become public } // 2. The implementation of the Node trait #[::async_trait::async_trait] // Macro adds async_trait support impl :: floxide_core :: node :: Node < SimpleContext > for PrefixAndCapitalizeNode where SimpleContext : Clone + Send + Sync // Adds necessary trait bounds { type Input = String ; // Sets the Input type type Output = String ; // Sets the Output type // Defines the process method async fn process ( & self , // `self` gives access to fields like `self.prefix` ctx : & SimpleContext , // Context argument text : String // Input argument (matches `input = ...`) ) -> Result < :: floxide_core :: transition :: Transition < String > , :: floxide_core :: error :: FloxideError > { // Your closure code is placed inside here! let ctx = ctx ; // Makes args available by original names let text = text ; // --- Your logic from the macro --- println! ( \"Node: Adding prefix '{}' and capitalizing '{}'\" , self . prefix , text ); let combined = format! ( \"{}: {}\" , self . prefix , text ); let result = combined . to_uppercase (); Ok ( Transition :: Next ( result )) // --- End of your logic --- } } // --- End of generated code --- You don't need to write this boilerplate code yourself; the node! macro handles it for you! Here's a diagram showing how the engine interacts with a Node created by the macro: sequenceDiagram participant Engine as Floxide Engine participant NodeInstance as YourNode (e.g., PrefixAndCapitalizeNode) participant NodeLogic as Your Logic (inside macro) Engine->>NodeInstance: Call process(context, input_data) Note over NodeInstance: process method generated by node! macro NodeInstance->>NodeLogic: Execute the closure code you wrote Note over NodeLogic: Accesses self.fields, context, input_data NodeLogic-->>NodeInstance: Return Result<Transition, Error> NodeInstance-->>Engine: Forward the Result<Transition, Error> Engine->>Engine: Act based on the Transition (e.g., schedule next step) The actual macro implementation lives in floxide-macros/src/node.rs and the Node trait definition is in floxide-core/src/node.rs .","title":"Under the Hood: What Does node! Do?"},{"location":"floxide-tutorial/02__node__trait____node___macro_/#conclusion","text":"The Node is the core unit of work in a Floxide workflow, representing a single processing step. The Node trait defines the contract for these steps, specifying their input, output, context needs, and the essential process method. While you can implement the Node trait manually, the node! macro provides a convenient and concise way to define a Node's structure (fields) and its processing logic all at once, generating the necessary boilerplate code for you. These Nodes are the building blocks that Floxide can distribute across different workers for parallel execution. Now that we know how to define individual steps (Nodes), how do they access shared information relevant to the entire workflow run? That's where context comes in. Next: Chapter 3: WorkflowCtx & Context Trait","title":"Conclusion"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/","text":"Chapter 3: WorkflowCtx & Context Trait \u00b6 In the previous chapter , we learned how to define individual steps in our workflow using the Node trait and the handy node! macro. Each Node performs a specific task. But what if different Nodes in our workflow need to access the same piece of information, or share some common resources? Imagine our video processing workflow again. Maybe multiple steps (Nodes) need the same API key to talk to an external service, or perhaps we want to keep a running count of processed frames across different steps. How can we share this information safely, especially when our workflow might be running distributed across many computers? This is where the concepts of Context and WorkflowCtx come in! What's the Problem? Sharing Information Between Steps \u00b6 Think of our distributed assembly line. Each worker (Node) operates independently, maybe even in different buildings (computers). How do they all know which version of the product they are building? (Shared configuration) How do they access shared tools, like a specific calibration device? (Shared resources like database connections) How does the manager tell everyone to stop if there's a major issue? (Cancellation signal) How do we ensure everyone finishes before a deadline? (Timeout) We need a mechanism for: Shared Data/Resources: A common place to store information that all Nodes in a single run of the workflow might need. Control Signals: Ways to manage the workflow run as a whole, like stopping it early. Floxide provides this through the Context trait and the WorkflowCtx struct. The Context Trait: Your Shared Toolbox Blueprint \u00b6 The Context trait itself is very simple. It doesn't define what goes into the shared toolbox, it just marks a Rust struct or type as being suitable to be used as the shared toolbox content for a workflow run. You , the developer, define the actual struct that holds the shared data. This struct needs to implement certain standard Rust traits so that Floxide can manage it effectively: Clone : Floxide might need to copy the context. Debug : For logging and debugging. Serialize / Deserialize : Crucial for saving state ( CheckpointStore ) and for distributed workflows ! The context needs to be saved and potentially sent over the network to different workers. serde is the standard Rust library for this. Send / Sync : Necessary for safely using the context across different threads or async tasks. Default : Needed to create an initial empty context when a workflow starts. floxide_core::merge::Merge : This is vital, especially for distributed workflows. It defines how to combine different versions of the context. For example, if two parallel steps modify the context, the Merge trait dictates how those changes are consolidated into a single, consistent state. Floxide provides a derive macro floxide_macros::Merge to help implement this. Structuring Context Data: Event Sourcing & Merging \u00b6 How should you structure the data inside your context? While you could put simple mutable fields like processed_items_count: u32 , this quickly becomes problematic, especially in distributed scenarios. How do you safely increment a counter when multiple workers might try to do it concurrently? A more robust and recommended approach is Event Sourcing : Events: Define an enum representing all possible changes or facts that can occur in your workflow's shared state (e.g., ItemProcessed(ItemId) , ApiKeySet(String) ). Event Log: Store a log of these events within your context struct. Floxide provides floxide_core::distributed::event_log::EventLog<YourEventEnum> for this. State Reconstruction: Instead of storing the current state directly (like the count), store the log of events. The current state can be reconstructed at any time by \"replaying\" the events from the log. Modification: Nodes don't modify state directly; they append new events to the log. Why Event Sourcing? Concurrency: Appending to a log is often easier to make safe and efficient than directly modifying shared values. Merging: The EventLog implements the Merge trait intelligently. When merging two versions of a context (e.g., from parallel branches), it combines their event logs, often preserving the history from both. Audit Trail: The event log provides a complete history of how the shared state evolved. The Merge Trait and Fixed Wrapper \u00b6 The Merge trait is key to handling concurrent updates. EventLog implements it. What about simple configuration values like an API key that shouldn't change or be merged in complex ways? Floxide provides floxide_core::merge::Fixed<T> . If you wrap a field in Fixed (e.g., api_key: Fixed<String> ), its Merge implementation will simply keep the first value it encountered. This is useful for configuration set at the start. You can implement Merge manually for your context struct, but the floxide_macros::Merge derive macro handles the common case: it merges each field using that field's own Merge implementation (like EventLog 's or Fixed 's merge). // Needed imports use serde ::{ Serialize , Deserialize }; use floxide_core :: context :: Context ; // The trait itself use floxide_core :: distributed :: event_log :: EventLog ; use floxide_core :: merge ::{ Merge , Fixed }; use floxide_macros :: Merge ; // The derive macro // 1. Define the events that can happen #[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)] pub enum MyWorkflowEvent { ItemProcessed ( u32 ), // Record which item ID was processed ProcessingStarted , // Add other relevant events } // 2. Define YOUR shared data structure using EventLog and Fixed #[derive(Clone, Debug, Default, Serialize, Deserialize, Merge)] // <-- Derive Merge! pub struct MyWorkflowData { // Configuration set once - use Fixed #[merge(strategy = \"fixed\" )] // Optional: Explicitly use Fixed strategy via attribute pub api_key : Fixed < String > , // Log of changes - use EventLog pub event_log : EventLog < MyWorkflowEvent > , // Other fields MUST also implement Merge or be wrapped (e.g., in Fixed) } // 3. Optionally, add a helper to get the current state from the log #[derive(Default, Debug)] // Temporary struct to hold the calculated state pub struct CurrentState { pub processed_items_count : u32 , pub started : bool , } impl MyWorkflowData { // Replays events to calculate the current state pub fn replay ( & self ) -> CurrentState { self . event_log . apply_all_default ( | event , state : & mut CurrentState | { match event { MyWorkflowEvent :: ItemProcessed ( _ ) => state . processed_items_count += 1 , MyWorkflowEvent :: ProcessingStarted => state . started = true , } }) } } // MyWorkflowData now satisfies the requirements of the Context trait // because it derives/impls Clone, Debug, Default, Serde, Merge, // and EventLog/Fixed handle Send/Sync internally. In this improved example: * We define MyWorkflowEvent . * MyWorkflowData uses Fixed<String> for the unchanging api_key and EventLog<MyWorkflowEvent> for the history. * We derive Merge for MyWorkflowData . * We add a replay method to calculate the CurrentState on demand. WorkflowCtx : The Toolbox Holder with Controls \u00b6 Okay, so we've defined what goes in our shared toolbox ( MyWorkflowData ). Now, how does Floxide manage it and add those control signals (like cancellation)? Floxide wraps your custom Context type inside its own struct called WorkflowCtx<C> . Think of WorkflowCtx as the manager holding your toolbox ( C represents your context type, like MyWorkflowData ) and also carrying walkie-talkies (cancellation) and a stopwatch (timeout). Here's a conceptual look at WorkflowCtx : // Simplified structure of WorkflowCtx pub struct WorkflowCtx < C : Context > { // Generic over YOUR context type C // 1. Your shared data store pub store : C , // 2. Cancellation signal (like a walkie-talkie) cancel : CancellationToken , // From the 'tokio-util' crate // 3. Optional overall deadline (stopwatch) timeout : Option < Duration > , } Key Parts: store: C : This public field holds the actual instance of your Context struct (e.g., an instance of MyWorkflowData , likely containing an EventLog ). Nodes interact with your context primarily through this field. cancel: CancellationToken : This is used internally to signal if the workflow should be stopped prematurely. Nodes can check this token via ctx.is_cancelled() . timeout: Option<Duration> : An optional overall time limit for the workflow run. Distributed Emphasis: When a workflow step runs on a remote worker, Floxide ensures that worker gets the correct WorkflowCtx , including the potentially updated store (often loaded from a Checkpoint & CheckpointStore Trait ) and the shared cancellation signal. This allows coordination across the distributed system. How Nodes Use WorkflowCtx \u00b6 Remember the node! macro from Chapter 2 ? Let's update the example using our event-sourced context: use floxide :: node ; use floxide ::{ Transition , FloxideError }; use serde ::{ Serialize , Deserialize }; use floxide_core :: context :: Context ; use floxide_core :: distributed :: event_log :: EventLog ; use floxide_core :: merge ::{ Merge , Fixed }; use floxide_macros :: Merge ; // The derive macro use std :: sync :: Arc ; // Needed if api_key is behind Arc in Fixed // --- Assume MyWorkflowEvent, MyWorkflowData, CurrentState from above --- #[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)] pub enum MyWorkflowEvent { ItemProcessed ( u32 ), ProcessingStarted } #[derive(Default, Debug)] pub struct CurrentState { pub processed_items_count : u32 , pub started : bool } #[derive(Clone, Debug, Default, Serialize, Deserialize, Merge)] pub struct MyWorkflowData { #[merge(strategy = \"fixed\" )] pub api_key : Fixed < Arc < String >> , // Use Arc for cheap clones pub event_log : EventLog < MyWorkflowEvent > , } impl MyWorkflowData { pub fn replay ( & self ) -> CurrentState { self . event_log . apply_all_default ( | event , state : & mut CurrentState | { match event { MyWorkflowEvent :: ItemProcessed ( _ ) => state . processed_items_count += 1 , MyWorkflowEvent :: ProcessingStarted => state . started = true , } }) } // Helper to create a new context pub fn new ( api_key : String ) -> Self { Self { api_key : Fixed :: new ( Arc :: new ( api_key )), event_log : EventLog :: new (), } } } // --- End of Context Definition --- // Let's define a Node that uses this context node ! { pub struct ProcessDataItemNode { // Node-specific config, if any item_id : u32 , // Let's assume the node knows which item ID it's processing } // *** Tell the node which context type to expect *** context = MyWorkflowData ; input = (); // Example input: maybe just a trigger output = (); // Example output: nothing significant // The closure now receives `&WorkflowCtx<MyWorkflowData>` as `ctx` async | ctx , _input | { // Make it async if needed (e.g., for replay potentially) // --- Accessing Current State (via Replay) --- let current_state = ctx . store . replay (); let api_key = ctx . store . api_key . get (); // Get the Arc<String> from Fixed println! ( \"Node [Item {}]: Processing. Current count: {}. Using API key starting with: {}\" , self . item_id , current_state . processed_items_count , api_key . chars (). take ( 5 ). collect :: < String > () // Show first 5 chars ); // --- Using Context Control Features --- // Check if the workflow run has been cancelled elsewhere if ctx . is_cancelled () { println! ( \"Node [Item {}]: Workflow cancelled, stopping processing.\" , self . item_id ); // Abort this step if cancellation was requested // Optionally, append a 'Cancelled' event to the log here? // ctx.store.event_log.append(MyWorkflowEvent::ProcessingCancelled(self.item_id)); return Err ( FloxideError :: Cancelled ); } // --- Node's Own Logic --- // Do some work... (using api_key if needed) println! ( \"Node [Item {}]: Finished processing.\" , self . item_id ); // Simulating work tokio :: time :: sleep ( std :: time :: Duration :: from_millis ( 10 )). await ; // --- Modifying Context (Append Event) --- // Instead of direct mutation, append an event to the log. // The actual context object `ctx.store` is immutable here (&WorkflowCtx). // The engine handles taking this event and merging it into the // persistent context state using the Merge trait. ctx . store . event_log . append ( MyWorkflowEvent :: ItemProcessed ( self . item_id )); // We don't return the modified context directly. Floxide handles // persisting the appended events via CheckpointStore or ContextStore. Ok ( Transition :: Next (())) // Pass nothing significant forward } } Explanation: context = MyWorkflowData; : Still tells node! the context type. |ctx, _input| : Receives &WorkflowCtx<MyWorkflowData> . Reading State : We call ctx.store.replay() to get the calculated CurrentState . We access configuration via ctx.store.api_key.get() . ctx.is_cancelled() : Works as before. Modifying Context : This is the key change! We call ctx.store.event_log.append(...) to record what happened. We do not directly change fields in ctx.store . The Floxide engine uses the Merge implementation of MyWorkflowData (which uses the Merge impl of EventLog ) to combine these appended events with the state saved in the CheckpointStore or ContextStore after the node successfully completes. How WorkflowCtx Enables Distribution (Revisited) \u00b6 Serialization: Still relies on serde , now serializing the EventLog and other Merge -able fields within your context. State Loading: When a worker loads state (from CheckpointStore or ContextStore ), it gets the context including the event log up to that point. Concurrency & Merging: This is where the Merge trait shines. If parallel branches of a workflow run, or if retries occur, Floxide uses the Merge implementation of your context struct (and thus the Merge impl of EventLog ) to correctly combine the different histories or updates into a consistent state in the persistent store. EventLog 's merge strategy helps ensure events aren't lost or unnecessarily duplicated. Cancellation Propagation: Works as previously described. Under the Hood: Creation and Usage (Updated) \u00b6 Instantiation: When you start a workflow run (e.g., using methods we'll see in Chapter 4: Workflow Trait & workflow! Macro ), you typically provide an initial instance of your Context struct. Floxide wraps this into a WorkflowCtx . Passing to Nodes: The Floxide engine takes care of passing the appropriate &WorkflowCtx to each Node's process method when it's executed. Node Execution & Event Appending: The engine passes &WorkflowCtx to the node. The node appends events to ctx.store.event_log (or other Merge -able fields). State Persistence & Merging: After a node finishes successfully, the engine takes the original context state loaded at the beginning of the step and the new context state containing the appended events (as returned conceptually by the node logic) and uses the Merge trait to combine them. This merged state is then saved back to the CheckpointStore (for local runs) or ContextStore (for distributed runs). Resuming/Distributed Step: When resuming or starting the next step (potentially on another worker), the engine loads the latest merged state from the store, ensuring the effects of the previous step (the appended events) are included. sequenceDiagram participant User as User Code participant Engine as Floxide Engine participant Ctx as WorkflowCtx participant Node as Your Node Logic participant Store as Checkpoint/Context Store User->>Engine: Start Workflow(initial_context_data) Engine->>Ctx: Create WorkflowCtx(initial_context_data) Engine->>Engine: Schedule first Node task Note over Engine, Node: Later, time to run Node A... Engine->>Store: Load Context state Store-->>Engine: Return saved Context state (contains event log) Engine->>Ctx: Update WorkflowCtx with loaded state Engine->>Node: Execute process(&ctx, input) Node->>Ctx: Access ctx.store.replay() (read state) Node->>Ctx: Check ctx.is_cancelled() Node->>Node: Perform Node logic Node->>Ctx: Append event(s) to ctx.store.event_log Node-->>Engine: Return Transition::Next(output) Engine->>Engine: Get context state with appended events from Node run Engine->>Engine: Merge(loaded state, state with new events) using Merge trait Engine->>Store: Save Merged Context state Engine->>Engine: Schedule next Node task based on Transition The updated diagram emphasizes that the node appends events, and the engine performs a merge operation before saving the state back. Looking at the code ( floxide-core/src/context.rs ): The Context trait definition likely remains the same marker trait, but the expectation is that the type T implementing it also implements floxide_core::merge::Merge . // From: crates/floxide-core/src/context.rs use serde ::{ Serialize , de :: DeserializeOwned }; use std :: fmt :: Debug ; use floxide_core :: merge :: Merge ; // Import Merge // Marker trait for user-defined context types // Note: Merge is now implicitly expected for robust use. pub trait Context : Default + DeserializeOwned + Serialize + Debug + Clone + Send + Sync + Merge {} // Blanket implementation: Any type meeting the bounds IS a Context impl < T : Default + DeserializeOwned + Serialize + Debug + Clone + Send + Sync + Merge > Context for T {} (Self-correction: Need to check if the official Context trait actually requires Merge now, or if it's just a strong recommendation/necessity for types used with ContextStore/CheckpointStore merge operations) . A quick check of floxide-core source might be needed to confirm the exact trait bound. Assuming Merge is essential for now. The WorkflowCtx struct holds the store and controls: // From: crates/floxide-core/src/context.rs (simplified) use std :: time :: Duration ; use tokio_util :: sync :: CancellationToken ; // For cancellation // The wrapper struct #[derive(Clone, Debug)] pub struct WorkflowCtx < S : Context > { // Generic over the user's Context type 'S' /// The store for the workflow. Holds the user's data. pub store : S , /// The cancellation token for the workflow. cancel : CancellationToken , /// The optional timeout for the workflow. timeout : Option < Duration > , } impl < S : Context > WorkflowCtx < S > { /// Creates a new context with the user's initial store data. pub fn new ( store : S ) -> Self { Self { store , cancel : CancellationToken :: new (), // Create a new token timeout : None , // No timeout initially } } /// Returns true if the workflow has been cancelled. pub fn is_cancelled ( & self ) -> bool { self . cancel . is_cancelled () } /// Cancel the workflow execution. pub fn cancel ( & self ) { self . cancel . cancel (); } // ... other methods for timeouts, running futures with cancellation ... } This shows the core structure: your store is held alongside Floxide's control mechanisms like the CancellationToken . Conclusion \u00b6 The Context trait and WorkflowCtx struct are essential for managing shared state and control across the Nodes of a workflow run in Floxide. You define your shared data in a struct that implements the Context trait requirements (often via #[derive(...)] ). Floxide wraps your context data in WorkflowCtx , adding control features like cancellation tokens and timeouts. Nodes declare the Context type they expect using context = ... in the node! macro. Inside a Node's logic, the ctx argument provides access to both your shared data ( ctx.store ) and the control methods ( ctx.is_cancelled() ). The requirement for Context to be Serialize / Deserialize is key for enabling distributed execution and checkpointing, allowing state to be saved and loaded across different workers and runs. Now that we understand individual steps ( Node ) and how they share information ( WorkflowCtx ), how do we actually connect these Nodes together to define the sequence and structure of our entire workflow? Next: Chapter 4: Workflow Trait & workflow! Macro","title":"Chapter 3: WorkflowCtx & Context Trait"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/#chapter-3-workflowctx-context-trait","text":"In the previous chapter , we learned how to define individual steps in our workflow using the Node trait and the handy node! macro. Each Node performs a specific task. But what if different Nodes in our workflow need to access the same piece of information, or share some common resources? Imagine our video processing workflow again. Maybe multiple steps (Nodes) need the same API key to talk to an external service, or perhaps we want to keep a running count of processed frames across different steps. How can we share this information safely, especially when our workflow might be running distributed across many computers? This is where the concepts of Context and WorkflowCtx come in!","title":"Chapter 3: WorkflowCtx &amp; Context Trait"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/#whats-the-problem-sharing-information-between-steps","text":"Think of our distributed assembly line. Each worker (Node) operates independently, maybe even in different buildings (computers). How do they all know which version of the product they are building? (Shared configuration) How do they access shared tools, like a specific calibration device? (Shared resources like database connections) How does the manager tell everyone to stop if there's a major issue? (Cancellation signal) How do we ensure everyone finishes before a deadline? (Timeout) We need a mechanism for: Shared Data/Resources: A common place to store information that all Nodes in a single run of the workflow might need. Control Signals: Ways to manage the workflow run as a whole, like stopping it early. Floxide provides this through the Context trait and the WorkflowCtx struct.","title":"What's the Problem? Sharing Information Between Steps"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/#the-context-trait-your-shared-toolbox-blueprint","text":"The Context trait itself is very simple. It doesn't define what goes into the shared toolbox, it just marks a Rust struct or type as being suitable to be used as the shared toolbox content for a workflow run. You , the developer, define the actual struct that holds the shared data. This struct needs to implement certain standard Rust traits so that Floxide can manage it effectively: Clone : Floxide might need to copy the context. Debug : For logging and debugging. Serialize / Deserialize : Crucial for saving state ( CheckpointStore ) and for distributed workflows ! The context needs to be saved and potentially sent over the network to different workers. serde is the standard Rust library for this. Send / Sync : Necessary for safely using the context across different threads or async tasks. Default : Needed to create an initial empty context when a workflow starts. floxide_core::merge::Merge : This is vital, especially for distributed workflows. It defines how to combine different versions of the context. For example, if two parallel steps modify the context, the Merge trait dictates how those changes are consolidated into a single, consistent state. Floxide provides a derive macro floxide_macros::Merge to help implement this.","title":"The Context Trait: Your Shared Toolbox Blueprint"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/#structuring-context-data-event-sourcing-merging","text":"How should you structure the data inside your context? While you could put simple mutable fields like processed_items_count: u32 , this quickly becomes problematic, especially in distributed scenarios. How do you safely increment a counter when multiple workers might try to do it concurrently? A more robust and recommended approach is Event Sourcing : Events: Define an enum representing all possible changes or facts that can occur in your workflow's shared state (e.g., ItemProcessed(ItemId) , ApiKeySet(String) ). Event Log: Store a log of these events within your context struct. Floxide provides floxide_core::distributed::event_log::EventLog<YourEventEnum> for this. State Reconstruction: Instead of storing the current state directly (like the count), store the log of events. The current state can be reconstructed at any time by \"replaying\" the events from the log. Modification: Nodes don't modify state directly; they append new events to the log. Why Event Sourcing? Concurrency: Appending to a log is often easier to make safe and efficient than directly modifying shared values. Merging: The EventLog implements the Merge trait intelligently. When merging two versions of a context (e.g., from parallel branches), it combines their event logs, often preserving the history from both. Audit Trail: The event log provides a complete history of how the shared state evolved.","title":"Structuring Context Data: Event Sourcing &amp; Merging"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/#the-merge-trait-and-fixed-wrapper","text":"The Merge trait is key to handling concurrent updates. EventLog implements it. What about simple configuration values like an API key that shouldn't change or be merged in complex ways? Floxide provides floxide_core::merge::Fixed<T> . If you wrap a field in Fixed (e.g., api_key: Fixed<String> ), its Merge implementation will simply keep the first value it encountered. This is useful for configuration set at the start. You can implement Merge manually for your context struct, but the floxide_macros::Merge derive macro handles the common case: it merges each field using that field's own Merge implementation (like EventLog 's or Fixed 's merge). // Needed imports use serde ::{ Serialize , Deserialize }; use floxide_core :: context :: Context ; // The trait itself use floxide_core :: distributed :: event_log :: EventLog ; use floxide_core :: merge ::{ Merge , Fixed }; use floxide_macros :: Merge ; // The derive macro // 1. Define the events that can happen #[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)] pub enum MyWorkflowEvent { ItemProcessed ( u32 ), // Record which item ID was processed ProcessingStarted , // Add other relevant events } // 2. Define YOUR shared data structure using EventLog and Fixed #[derive(Clone, Debug, Default, Serialize, Deserialize, Merge)] // <-- Derive Merge! pub struct MyWorkflowData { // Configuration set once - use Fixed #[merge(strategy = \"fixed\" )] // Optional: Explicitly use Fixed strategy via attribute pub api_key : Fixed < String > , // Log of changes - use EventLog pub event_log : EventLog < MyWorkflowEvent > , // Other fields MUST also implement Merge or be wrapped (e.g., in Fixed) } // 3. Optionally, add a helper to get the current state from the log #[derive(Default, Debug)] // Temporary struct to hold the calculated state pub struct CurrentState { pub processed_items_count : u32 , pub started : bool , } impl MyWorkflowData { // Replays events to calculate the current state pub fn replay ( & self ) -> CurrentState { self . event_log . apply_all_default ( | event , state : & mut CurrentState | { match event { MyWorkflowEvent :: ItemProcessed ( _ ) => state . processed_items_count += 1 , MyWorkflowEvent :: ProcessingStarted => state . started = true , } }) } } // MyWorkflowData now satisfies the requirements of the Context trait // because it derives/impls Clone, Debug, Default, Serde, Merge, // and EventLog/Fixed handle Send/Sync internally. In this improved example: * We define MyWorkflowEvent . * MyWorkflowData uses Fixed<String> for the unchanging api_key and EventLog<MyWorkflowEvent> for the history. * We derive Merge for MyWorkflowData . * We add a replay method to calculate the CurrentState on demand.","title":"The Merge Trait and Fixed Wrapper"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/#workflowctx-the-toolbox-holder-with-controls","text":"Okay, so we've defined what goes in our shared toolbox ( MyWorkflowData ). Now, how does Floxide manage it and add those control signals (like cancellation)? Floxide wraps your custom Context type inside its own struct called WorkflowCtx<C> . Think of WorkflowCtx as the manager holding your toolbox ( C represents your context type, like MyWorkflowData ) and also carrying walkie-talkies (cancellation) and a stopwatch (timeout). Here's a conceptual look at WorkflowCtx : // Simplified structure of WorkflowCtx pub struct WorkflowCtx < C : Context > { // Generic over YOUR context type C // 1. Your shared data store pub store : C , // 2. Cancellation signal (like a walkie-talkie) cancel : CancellationToken , // From the 'tokio-util' crate // 3. Optional overall deadline (stopwatch) timeout : Option < Duration > , } Key Parts: store: C : This public field holds the actual instance of your Context struct (e.g., an instance of MyWorkflowData , likely containing an EventLog ). Nodes interact with your context primarily through this field. cancel: CancellationToken : This is used internally to signal if the workflow should be stopped prematurely. Nodes can check this token via ctx.is_cancelled() . timeout: Option<Duration> : An optional overall time limit for the workflow run. Distributed Emphasis: When a workflow step runs on a remote worker, Floxide ensures that worker gets the correct WorkflowCtx , including the potentially updated store (often loaded from a Checkpoint & CheckpointStore Trait ) and the shared cancellation signal. This allows coordination across the distributed system.","title":"WorkflowCtx: The Toolbox Holder with Controls"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/#how-nodes-use-workflowctx","text":"Remember the node! macro from Chapter 2 ? Let's update the example using our event-sourced context: use floxide :: node ; use floxide ::{ Transition , FloxideError }; use serde ::{ Serialize , Deserialize }; use floxide_core :: context :: Context ; use floxide_core :: distributed :: event_log :: EventLog ; use floxide_core :: merge ::{ Merge , Fixed }; use floxide_macros :: Merge ; // The derive macro use std :: sync :: Arc ; // Needed if api_key is behind Arc in Fixed // --- Assume MyWorkflowEvent, MyWorkflowData, CurrentState from above --- #[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)] pub enum MyWorkflowEvent { ItemProcessed ( u32 ), ProcessingStarted } #[derive(Default, Debug)] pub struct CurrentState { pub processed_items_count : u32 , pub started : bool } #[derive(Clone, Debug, Default, Serialize, Deserialize, Merge)] pub struct MyWorkflowData { #[merge(strategy = \"fixed\" )] pub api_key : Fixed < Arc < String >> , // Use Arc for cheap clones pub event_log : EventLog < MyWorkflowEvent > , } impl MyWorkflowData { pub fn replay ( & self ) -> CurrentState { self . event_log . apply_all_default ( | event , state : & mut CurrentState | { match event { MyWorkflowEvent :: ItemProcessed ( _ ) => state . processed_items_count += 1 , MyWorkflowEvent :: ProcessingStarted => state . started = true , } }) } // Helper to create a new context pub fn new ( api_key : String ) -> Self { Self { api_key : Fixed :: new ( Arc :: new ( api_key )), event_log : EventLog :: new (), } } } // --- End of Context Definition --- // Let's define a Node that uses this context node ! { pub struct ProcessDataItemNode { // Node-specific config, if any item_id : u32 , // Let's assume the node knows which item ID it's processing } // *** Tell the node which context type to expect *** context = MyWorkflowData ; input = (); // Example input: maybe just a trigger output = (); // Example output: nothing significant // The closure now receives `&WorkflowCtx<MyWorkflowData>` as `ctx` async | ctx , _input | { // Make it async if needed (e.g., for replay potentially) // --- Accessing Current State (via Replay) --- let current_state = ctx . store . replay (); let api_key = ctx . store . api_key . get (); // Get the Arc<String> from Fixed println! ( \"Node [Item {}]: Processing. Current count: {}. Using API key starting with: {}\" , self . item_id , current_state . processed_items_count , api_key . chars (). take ( 5 ). collect :: < String > () // Show first 5 chars ); // --- Using Context Control Features --- // Check if the workflow run has been cancelled elsewhere if ctx . is_cancelled () { println! ( \"Node [Item {}]: Workflow cancelled, stopping processing.\" , self . item_id ); // Abort this step if cancellation was requested // Optionally, append a 'Cancelled' event to the log here? // ctx.store.event_log.append(MyWorkflowEvent::ProcessingCancelled(self.item_id)); return Err ( FloxideError :: Cancelled ); } // --- Node's Own Logic --- // Do some work... (using api_key if needed) println! ( \"Node [Item {}]: Finished processing.\" , self . item_id ); // Simulating work tokio :: time :: sleep ( std :: time :: Duration :: from_millis ( 10 )). await ; // --- Modifying Context (Append Event) --- // Instead of direct mutation, append an event to the log. // The actual context object `ctx.store` is immutable here (&WorkflowCtx). // The engine handles taking this event and merging it into the // persistent context state using the Merge trait. ctx . store . event_log . append ( MyWorkflowEvent :: ItemProcessed ( self . item_id )); // We don't return the modified context directly. Floxide handles // persisting the appended events via CheckpointStore or ContextStore. Ok ( Transition :: Next (())) // Pass nothing significant forward } } Explanation: context = MyWorkflowData; : Still tells node! the context type. |ctx, _input| : Receives &WorkflowCtx<MyWorkflowData> . Reading State : We call ctx.store.replay() to get the calculated CurrentState . We access configuration via ctx.store.api_key.get() . ctx.is_cancelled() : Works as before. Modifying Context : This is the key change! We call ctx.store.event_log.append(...) to record what happened. We do not directly change fields in ctx.store . The Floxide engine uses the Merge implementation of MyWorkflowData (which uses the Merge impl of EventLog ) to combine these appended events with the state saved in the CheckpointStore or ContextStore after the node successfully completes.","title":"How Nodes Use WorkflowCtx"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/#how-workflowctx-enables-distribution-revisited","text":"Serialization: Still relies on serde , now serializing the EventLog and other Merge -able fields within your context. State Loading: When a worker loads state (from CheckpointStore or ContextStore ), it gets the context including the event log up to that point. Concurrency & Merging: This is where the Merge trait shines. If parallel branches of a workflow run, or if retries occur, Floxide uses the Merge implementation of your context struct (and thus the Merge impl of EventLog ) to correctly combine the different histories or updates into a consistent state in the persistent store. EventLog 's merge strategy helps ensure events aren't lost or unnecessarily duplicated. Cancellation Propagation: Works as previously described.","title":"How WorkflowCtx Enables Distribution (Revisited)"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/#under-the-hood-creation-and-usage-updated","text":"Instantiation: When you start a workflow run (e.g., using methods we'll see in Chapter 4: Workflow Trait & workflow! Macro ), you typically provide an initial instance of your Context struct. Floxide wraps this into a WorkflowCtx . Passing to Nodes: The Floxide engine takes care of passing the appropriate &WorkflowCtx to each Node's process method when it's executed. Node Execution & Event Appending: The engine passes &WorkflowCtx to the node. The node appends events to ctx.store.event_log (or other Merge -able fields). State Persistence & Merging: After a node finishes successfully, the engine takes the original context state loaded at the beginning of the step and the new context state containing the appended events (as returned conceptually by the node logic) and uses the Merge trait to combine them. This merged state is then saved back to the CheckpointStore (for local runs) or ContextStore (for distributed runs). Resuming/Distributed Step: When resuming or starting the next step (potentially on another worker), the engine loads the latest merged state from the store, ensuring the effects of the previous step (the appended events) are included. sequenceDiagram participant User as User Code participant Engine as Floxide Engine participant Ctx as WorkflowCtx participant Node as Your Node Logic participant Store as Checkpoint/Context Store User->>Engine: Start Workflow(initial_context_data) Engine->>Ctx: Create WorkflowCtx(initial_context_data) Engine->>Engine: Schedule first Node task Note over Engine, Node: Later, time to run Node A... Engine->>Store: Load Context state Store-->>Engine: Return saved Context state (contains event log) Engine->>Ctx: Update WorkflowCtx with loaded state Engine->>Node: Execute process(&ctx, input) Node->>Ctx: Access ctx.store.replay() (read state) Node->>Ctx: Check ctx.is_cancelled() Node->>Node: Perform Node logic Node->>Ctx: Append event(s) to ctx.store.event_log Node-->>Engine: Return Transition::Next(output) Engine->>Engine: Get context state with appended events from Node run Engine->>Engine: Merge(loaded state, state with new events) using Merge trait Engine->>Store: Save Merged Context state Engine->>Engine: Schedule next Node task based on Transition The updated diagram emphasizes that the node appends events, and the engine performs a merge operation before saving the state back. Looking at the code ( floxide-core/src/context.rs ): The Context trait definition likely remains the same marker trait, but the expectation is that the type T implementing it also implements floxide_core::merge::Merge . // From: crates/floxide-core/src/context.rs use serde ::{ Serialize , de :: DeserializeOwned }; use std :: fmt :: Debug ; use floxide_core :: merge :: Merge ; // Import Merge // Marker trait for user-defined context types // Note: Merge is now implicitly expected for robust use. pub trait Context : Default + DeserializeOwned + Serialize + Debug + Clone + Send + Sync + Merge {} // Blanket implementation: Any type meeting the bounds IS a Context impl < T : Default + DeserializeOwned + Serialize + Debug + Clone + Send + Sync + Merge > Context for T {} (Self-correction: Need to check if the official Context trait actually requires Merge now, or if it's just a strong recommendation/necessity for types used with ContextStore/CheckpointStore merge operations) . A quick check of floxide-core source might be needed to confirm the exact trait bound. Assuming Merge is essential for now. The WorkflowCtx struct holds the store and controls: // From: crates/floxide-core/src/context.rs (simplified) use std :: time :: Duration ; use tokio_util :: sync :: CancellationToken ; // For cancellation // The wrapper struct #[derive(Clone, Debug)] pub struct WorkflowCtx < S : Context > { // Generic over the user's Context type 'S' /// The store for the workflow. Holds the user's data. pub store : S , /// The cancellation token for the workflow. cancel : CancellationToken , /// The optional timeout for the workflow. timeout : Option < Duration > , } impl < S : Context > WorkflowCtx < S > { /// Creates a new context with the user's initial store data. pub fn new ( store : S ) -> Self { Self { store , cancel : CancellationToken :: new (), // Create a new token timeout : None , // No timeout initially } } /// Returns true if the workflow has been cancelled. pub fn is_cancelled ( & self ) -> bool { self . cancel . is_cancelled () } /// Cancel the workflow execution. pub fn cancel ( & self ) { self . cancel . cancel (); } // ... other methods for timeouts, running futures with cancellation ... } This shows the core structure: your store is held alongside Floxide's control mechanisms like the CancellationToken .","title":"Under the Hood: Creation and Usage (Updated)"},{"location":"floxide-tutorial/03__workflowctx_____context__trait_/#conclusion","text":"The Context trait and WorkflowCtx struct are essential for managing shared state and control across the Nodes of a workflow run in Floxide. You define your shared data in a struct that implements the Context trait requirements (often via #[derive(...)] ). Floxide wraps your context data in WorkflowCtx , adding control features like cancellation tokens and timeouts. Nodes declare the Context type they expect using context = ... in the node! macro. Inside a Node's logic, the ctx argument provides access to both your shared data ( ctx.store ) and the control methods ( ctx.is_cancelled() ). The requirement for Context to be Serialize / Deserialize is key for enabling distributed execution and checkpointing, allowing state to be saved and loaded across different workers and runs. Now that we understand individual steps ( Node ) and how they share information ( WorkflowCtx ), how do we actually connect these Nodes together to define the sequence and structure of our entire workflow? Next: Chapter 4: Workflow Trait & workflow! Macro","title":"Conclusion"},{"location":"floxide-tutorial/04__workflow__trait____workflow___macro_/","text":"Chapter 4: Workflow Trait & workflow! Macro \u00b6 In the previous chapter , we learned how individual steps ( Node s ) can share information using WorkflowCtx and the Context trait. We now have building blocks (Nodes) and a way to share data (Context). But how do we connect these blocks together to form a complete process? How does Floxide know that after Node A finishes, Node B should start? Especially when these nodes might be running on different computers? What's the Problem? Defining the Assembly Line Layout \u00b6 Imagine you have designed all the individual stations ( Node s) for your assembly line (like \"Download Video\", \"Extract Audio\", \"Upload Result\"). You also have a way for workers at these stations to access shared tools or instructions ( Context ). Now, you need the master blueprint that shows: Which station is the first one? When station A finishes successfully, which station(s) should the item go to next? What if station A produces multiple items ( Transition::NextAll )? Where do they all go? What if station A fails or needs special handling ( Transition::Abort )? Is there a fallback path? How does the whole system know when the entire process is finished? This blueprint is crucial, especially in a distributed system. Without a clear map, how would a worker on computer X know that the output it just produced needs to be picked up by a worker on computer Y running the next step? Floxide uses the Workflow concept to represent this master blueprint. What is a Workflow ? \u00b6 A Workflow in Floxide defines the complete structure and flow of tasks. It's the top-level container that holds all the individual Node s and specifies exactly how they are connected (the \"edges\" between them). Key responsibilities of a Workflow definition: Contains Nodes: Holds instances of all the Node structs that make up the workflow. Specifies Start: Declares which Node is the entry point. Defines Context: Specifies the type of shared Context the workflow uses. Maps Connections (Edges): Explicitly defines the paths between Nodes. This includes: Simple connections: Output of Node A goes to Node B. Fan-out: Output of Node A goes to Node B and Node C. Conditional branching: If Node A returns variant X, go to Node B; if it returns variant Y, go to Node C. Fallback paths: If Node A fails, maybe trigger Node F (e.g., an error handler). Identifies End: Implicitly defines the terminal Node(s) \u2013 those with no further connections. The Workflow Trait: The Contract \u00b6 Like the Node trait, Floxide uses a Workflow trait to define the standard capabilities expected of any workflow structure. This trait ensures that the Floxide engine knows how to interact with your workflow definition, regardless of how complex it is. Some key methods defined by the Workflow trait (simplified concept): // Simplified concept from floxide-core/src/workflow.rs #[async_trait] trait Workflow < C : Context > { type Input ; // Data type the *start* node expects type Output ; // Data type the *terminal* node produces type WorkItem ; // An internal type representing a task (Node + Input) // Run the whole workflow locally async fn run ( & self , ctx : & WorkflowCtx < C > , input : Self :: Input ) -> Result < Self :: Output , FloxideError > ; // Run with saving state after each step async fn run_with_checkpoint < CS > ( .. .) -> Result < Self :: Output , FloxideError > ; // Resume from a saved state async fn resume < CS > ( .. .) -> Result < Self :: Output , FloxideError > ; // --- Distributed Primitives --- // Prepare a distributed run (Orchestrator) async fn start_distributed < CS , Q > ( .. .) -> Result < (), FloxideError > ; // Execute one step in a distributed run (Worker) async fn step_distributed < CS , Q > ( .. .) -> Result < Option < ( String , Self :: Output ) > , StepError < Self :: WorkItem >> ; // Process a single WorkItem (Internal helper for routing) async fn process_work_item ( .. .) -> Result < Option < Self :: Output > , FloxideError > ; // Get a visual representation fn to_dot ( & self ) -> & ' static str ; } This trait provides methods for different execution modes: simple local runs ( run ), resumable runs ( run_with_checkpoint , resume ), and the core primitives for distributed execution ( start_distributed , step_distributed ). The process_work_item method is crucial internally for handling the routing logic based on the defined edges. Manually implementing this trait would be very complex! Thankfully, Floxide provides a macro to do the heavy lifting. The workflow! Macro: Your Workflow Designer \u00b6 The workflow! macro is the high-level tool you'll use to define your workflow's structure. It takes care of generating the necessary struct, the internal WorkItem enum (which tracks which node needs to run with what data), and the implementation of the Workflow trait, including all the complex routing logic. Here's the basic syntax: // General syntax of the workflow! macro workflow ! { // 1. Define the struct holding the Nodes pub struct MyWorkflow { // Fields are instances of your Nodes step_a : NodeAInstance , step_b : NodeBInstance , step_c : NodeCInstance , // You can also include retry policies here if needed // retry_policy: MyRetryPolicy, } // 2. Specify the Context type context = MyWorkflowContext ; // 3. Specify the starting Node field start = step_a ; // 4. Define the connections (edges) edges { // If step_a succeeds, its output goes to step_b step_a => [ step_b ]; // If step_b succeeds, its output goes to step_c // (Example: can also have conditional or fallback edges) step_b => [ step_c ]; // step_c is the last step (terminal node) step_c => []; // Empty list means it's an end point }; } Let's break it down: Struct Definition: pub struct MyWorkflow { ... } defines a struct that will hold instances of your Node s . Give each Node instance a field name (e.g., step_a , step_b ). context = Type; : Specifies the shared Context type this workflow uses (e.g., MyWorkflowData from Chapter 3). start = field_name; : Tells Floxide which field in your struct represents the first Node to execute (e.g., step_a ). edges { ... }; : This is the core mapping section. source_field => [target_field_1, target_field_2, ...]; defines where the output of the source_field Node should go upon successful completion ( Transition::Next or Transition::NextAll ). An empty list [] means this Node is a terminal point. (Advanced: The macro also supports conditional branching based on Node output variants and fallback paths using on_failure , but we'll stick to simple cases here). Example: A Simple Text Processing Workflow \u00b6 Let's combine the Nodes from Chapter 2 into a simple workflow: PrefixAndCapitalizeNode -> CapitalizeTextNode . use floxide ::{ workflow , node , Workflow , Node , Context , Transition , FloxideError }; use serde ::{ Serialize , Deserialize }; use std :: sync :: Arc ; // Needed for callbacks in distributed step // --- Define Context (from Chapter 3) --- #[derive(Clone, Debug, Default, Serialize, Deserialize)] struct SimpleContext {} // No need for `impl Context for SimpleContext {}` if deriving required traits // --- Define Nodes (from Chapter 2, simplified) --- node ! { #[derive(Clone, Debug)] // Add Clone/Debug pub struct PrefixNode { prefix : String } context = SimpleContext ; input = String ; output = String ; | ctx , text | { let result = format! ( \"{}: {}\" , self . prefix , text ); println! ( \"PrefixNode: {} -> {}\" , text , result ); Ok ( Transition :: Next ( result )) } } node ! { #[derive(Clone, Debug)] // Add Clone/Debug pub struct CapitalizeNode {} context = SimpleContext ; input = String ; output = String ; | ctx , text | { let result = text . to_uppercase (); println! ( \"CapitalizeNode: {} -> {}\" , text , result ); Ok ( Transition :: Next ( result )) } } // --- Define the Workflow using the workflow! macro --- workflow ! { // Struct holds instances of our nodes #[derive(Clone, Debug)] // Workflow struct should also be Clone/Debug pub struct TextProcessor { add_prefix : PrefixNode , // Instance of PrefixNode capitalize : CapitalizeNode , // Instance of CapitalizeNode } // Use our simple context context = SimpleContext ; // Start with the 'add_prefix' node start = add_prefix ; // Define the flow: add_prefix -> capitalize -> end edges { add_prefix => [ capitalize ]; // Output of add_prefix goes to capitalize capitalize => []; // capitalize is the terminal node }; } // --- How you might use it (conceptual) --- // #[tokio::main] // async fn main() -> Result<(), FloxideError> { // // 1. Create instances of the Nodes // let prefixer = PrefixNode { prefix: \"INFO\".to_string() }; // let capitalizer = CapitalizeNode {}; // // // 2. Create an instance of the Workflow struct // let my_workflow = TextProcessor { // add_prefix: prefixer, // capitalize: capitalizer, // }; // // // 3. Create the initial context // let initial_context_data = SimpleContext {}; // let wf_ctx = floxide::WorkflowCtx::new(initial_context_data); // // // 4. Run the workflow (using the 'run' method from the Workflow trait) // let input_text = \"hello world\".to_string(); // println!(\"Starting workflow with input: '{}'\", input_text); // let final_result = my_workflow.run(&wf_ctx, input_text).await?; // // println!(\"Workflow finished. Final result: '{}'\", final_result); // // Expected output: // // Starting workflow with input: 'hello world' // // PrefixNode: hello world -> INFO: hello world // // CapitalizeNode: INFO: hello world -> INFO: HELLO WORLD // // Workflow finished. Final result: 'INFO: HELLO WORLD' // // Ok(()) // } Explanation: We defined our SimpleContext and two Nodes ( PrefixNode , CapitalizeNode ). We used workflow! to define TextProcessor . The struct holds fields add_prefix and capitalize . We set context = SimpleContext . We set start = add_prefix . In edges , we defined add_prefix => [capitalize]; (output of add_prefix goes to capitalize ) and capitalize => []; ( capitalize is the end). In the conceptual usage: We create instances of the Nodes and the TextProcessor workflow struct. We create the initial WorkflowCtx . We call the run method (which the workflow! macro generated for us!) with the context and initial input. The macro-generated code handles calling add_prefix , taking its output, calling capitalize with that output, and finally returning the result from capitalize because it's the terminal node. Distributed Emphasis: If we were running this distributed (using start_distributed and step_distributed ), the edges definition is absolutely critical. When add_prefix finishes on Worker 1, the macro-generated process_work_item logic consults the edges . It sees that capitalize is next. It then wraps the output of add_prefix into a WorkItem representing the capitalize task and places it onto the shared WorkQueue . Worker 2 (or maybe even Worker 1 again later) can then pick up this capitalize task from the queue and execute it. The workflow! macro essentially compiles your visual blueprint into executable routing logic. Under the Hood: What Does workflow! Generate? \u00b6 The workflow! macro is a code generator. It takes your high-level definition and expands it into more detailed Rust code, saving you from writing a lot of boilerplate. Workflow Struct: It generates the struct you defined (e.g., pub struct TextProcessor { ... } ). WorkItem Enum: It creates a hidden helper enum (e.g., TextProcessorWorkItem ). This enum has one variant for each Node field in your workflow struct. This enum is how Floxide keeps track of \"what work needs to be done next\". // Simplified generated code (conceptual) #[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)] enum TextProcessorWorkItem { AddPrefix ( String ), // Carries input for PrefixNode Capitalize ( String ), // Carries input for CapitalizeNode } // Also implements Display for unique IDs, needed for WorkItem trait impl std :: fmt :: Display for TextProcessorWorkItem { /* ... */ } impl floxide_core :: workflow :: WorkItem for TextProcessorWorkItem { /* ... */ } impl Workflow : It generates the implementation of the Workflow trait for your struct ( impl Workflow<SimpleContext> for TextProcessor { ... } ). This implementation includes: Definitions for Input , Output , and WorkItem associated types. Implementations for run , run_with_checkpoint , resume , start_distributed , step_distributed , and to_dot . The crucial process_work_item method . This method contains a match statement on the generated WorkItem enum. Each arm corresponds to a Node in your workflow. Inside the arm, it: Calls the actual process method of the corresponding Node instance (e.g., self.add_prefix.process(...) ). Examines the returned Transition . Based on the Transition and your edges definition, it creates the next WorkItem (s) (e.g., TextProcessorWorkItem::Capitalize(...) ) and pushes them onto the work queue ( __q ) provided to it. If a terminal node finishes, it returns Ok(Some(output)) . Otherwise, it returns Ok(None) . Here's a simplified look at the generated process_work_item logic for our example: // Simplified generated code for process_work_item (conceptual) async fn _process_work_item <' a > ( &' a self , ctx : & ' a floxide_core :: WorkflowCtx < SimpleContext > , item : TextProcessorWorkItem , // Input is the generated enum __q : & mut std :: collections :: VecDeque < TextProcessorWorkItem > // Output queue ) -> Result < Option < String > , floxide_core :: error :: FloxideError > { use floxide_core :: transition :: Transition ; match item { TextProcessorWorkItem :: AddPrefix ( input_string ) => { // Call the actual node's process method let transition = self . add_prefix . process ( & ctx . store , input_string ). await ? ; match transition { Transition :: Next ( output_string ) => { // Look up edges for add_prefix: finds [capitalize] // Create the next work item let next_item = TextProcessorWorkItem :: Capitalize ( output_string ); // Add it to the queue __q . push_back ( next_item ); Ok ( None ) // More work to do } Transition :: NextAll ( outputs ) => { /* Handle fan-out similarly */ Ok ( None ) } Transition :: Abort ( e ) => Err ( e ), Transition :: Hold => Ok ( None ), // Do nothing for Hold } } TextProcessorWorkItem :: Capitalize ( input_string ) => { // Call the actual node's process method let transition = self . capitalize . process ( & ctx . store , input_string ). await ? ; match transition { Transition :: Next ( output_string ) => { // Look up edges for capitalize: finds [] (terminal) // Return the final output Ok ( Some ( output_string )) } Transition :: NextAll ( outputs ) => { /* Handle fan-out */ Ok ( None ) } // Might still be terminal Transition :: Abort ( e ) => Err ( e ), Transition :: Hold => Ok ( None ), } } } } This generated code directly translates your edges definition into the runtime logic that connects the Nodes. Here's a sequence diagram showing how the engine uses the generated process_work_item method during a run: sequenceDiagram participant Engine participant WorkflowImpl as TextProcessor Instance participant PrefixNode as self.add_prefix participant CapitalizeNode as self.capitalize participant Queue as Work Queue (__q) Note over Engine: Start run, initial item is AddPrefix(\"hello\") Engine->>WorkflowImpl: process_work_item(ctx, WorkItem::AddPrefix(\"hello\"), queue) WorkflowImpl->>PrefixNode: process(ctx.store, \"hello\") PrefixNode-->>WorkflowImpl: Return Ok(Transition::Next(\"INFO: hello\")) WorkflowImpl->>WorkflowImpl: Check edges for add_prefix -> finds [capitalize] WorkflowImpl->>Queue: Enqueue WorkItem::Capitalize(\"INFO: hello\") WorkflowImpl-->>Engine: Return Ok(None) (more work pending) Note over Engine: Later, process next item from queue... Engine->>WorkflowImpl: process_work_item(ctx, WorkItem::Capitalize(\"INFO: hello\"), queue) WorkflowImpl->>CapitalizeNode: process(ctx.store, \"INFO: hello\") CapitalizeNode-->>WorkflowImpl: Return Ok(Transition::Next(\"INFO: HELLO\")) WorkflowImpl->>WorkflowImpl: Check edges for capitalize -> finds [] (terminal) WorkflowImpl-->>Engine: Return Ok(Some(\"INFO: HELLO\")) (final output) The real implementation of the macro lives in floxide-macros/src/workflow.rs and the Workflow trait definition is in floxide-core/src/workflow.rs . Conclusion \u00b6 The Workflow trait and the workflow! macro are central to defining the overall structure and execution flow in Floxide. A Workflow acts as the master blueprint, holding all Node s and defining their connections (edges). The Workflow trait defines the standard interface for executing workflows in various modes (local, checkpointed, distributed). The workflow! macro provides a convenient way to declare the workflow struct, its Nodes, the starting point, the shared Context , and the crucial edge mappings. The macro generates the necessary Rust code, including the Workflow trait implementation and the internal routing logic ( process_work_item ), which translates your high-level definition into executable steps. This explicit definition of structure and edges is vital for enabling reliable distributed execution , as it provides the map the engine needs to route tasks across different workers via a shared work queue. Now that we have defined our Nodes, Context, and the overall Workflow structure, how does Floxide manage the list of tasks waiting to be executed, especially in a distributed environment? That's the role of the WorkQueue . Next: Chapter 5: WorkQueue Trait","title":"Chapter 4: Workflow Trait & workflow! Macro"},{"location":"floxide-tutorial/04__workflow__trait____workflow___macro_/#chapter-4-workflow-trait-workflow-macro","text":"In the previous chapter , we learned how individual steps ( Node s ) can share information using WorkflowCtx and the Context trait. We now have building blocks (Nodes) and a way to share data (Context). But how do we connect these blocks together to form a complete process? How does Floxide know that after Node A finishes, Node B should start? Especially when these nodes might be running on different computers?","title":"Chapter 4: Workflow Trait &amp; workflow! Macro"},{"location":"floxide-tutorial/04__workflow__trait____workflow___macro_/#whats-the-problem-defining-the-assembly-line-layout","text":"Imagine you have designed all the individual stations ( Node s) for your assembly line (like \"Download Video\", \"Extract Audio\", \"Upload Result\"). You also have a way for workers at these stations to access shared tools or instructions ( Context ). Now, you need the master blueprint that shows: Which station is the first one? When station A finishes successfully, which station(s) should the item go to next? What if station A produces multiple items ( Transition::NextAll )? Where do they all go? What if station A fails or needs special handling ( Transition::Abort )? Is there a fallback path? How does the whole system know when the entire process is finished? This blueprint is crucial, especially in a distributed system. Without a clear map, how would a worker on computer X know that the output it just produced needs to be picked up by a worker on computer Y running the next step? Floxide uses the Workflow concept to represent this master blueprint.","title":"What's the Problem? Defining the Assembly Line Layout"},{"location":"floxide-tutorial/04__workflow__trait____workflow___macro_/#what-is-a-workflow","text":"A Workflow in Floxide defines the complete structure and flow of tasks. It's the top-level container that holds all the individual Node s and specifies exactly how they are connected (the \"edges\" between them). Key responsibilities of a Workflow definition: Contains Nodes: Holds instances of all the Node structs that make up the workflow. Specifies Start: Declares which Node is the entry point. Defines Context: Specifies the type of shared Context the workflow uses. Maps Connections (Edges): Explicitly defines the paths between Nodes. This includes: Simple connections: Output of Node A goes to Node B. Fan-out: Output of Node A goes to Node B and Node C. Conditional branching: If Node A returns variant X, go to Node B; if it returns variant Y, go to Node C. Fallback paths: If Node A fails, maybe trigger Node F (e.g., an error handler). Identifies End: Implicitly defines the terminal Node(s) \u2013 those with no further connections.","title":"What is a Workflow?"},{"location":"floxide-tutorial/04__workflow__trait____workflow___macro_/#the-workflow-trait-the-contract","text":"Like the Node trait, Floxide uses a Workflow trait to define the standard capabilities expected of any workflow structure. This trait ensures that the Floxide engine knows how to interact with your workflow definition, regardless of how complex it is. Some key methods defined by the Workflow trait (simplified concept): // Simplified concept from floxide-core/src/workflow.rs #[async_trait] trait Workflow < C : Context > { type Input ; // Data type the *start* node expects type Output ; // Data type the *terminal* node produces type WorkItem ; // An internal type representing a task (Node + Input) // Run the whole workflow locally async fn run ( & self , ctx : & WorkflowCtx < C > , input : Self :: Input ) -> Result < Self :: Output , FloxideError > ; // Run with saving state after each step async fn run_with_checkpoint < CS > ( .. .) -> Result < Self :: Output , FloxideError > ; // Resume from a saved state async fn resume < CS > ( .. .) -> Result < Self :: Output , FloxideError > ; // --- Distributed Primitives --- // Prepare a distributed run (Orchestrator) async fn start_distributed < CS , Q > ( .. .) -> Result < (), FloxideError > ; // Execute one step in a distributed run (Worker) async fn step_distributed < CS , Q > ( .. .) -> Result < Option < ( String , Self :: Output ) > , StepError < Self :: WorkItem >> ; // Process a single WorkItem (Internal helper for routing) async fn process_work_item ( .. .) -> Result < Option < Self :: Output > , FloxideError > ; // Get a visual representation fn to_dot ( & self ) -> & ' static str ; } This trait provides methods for different execution modes: simple local runs ( run ), resumable runs ( run_with_checkpoint , resume ), and the core primitives for distributed execution ( start_distributed , step_distributed ). The process_work_item method is crucial internally for handling the routing logic based on the defined edges. Manually implementing this trait would be very complex! Thankfully, Floxide provides a macro to do the heavy lifting.","title":"The Workflow Trait: The Contract"},{"location":"floxide-tutorial/04__workflow__trait____workflow___macro_/#the-workflow-macro-your-workflow-designer","text":"The workflow! macro is the high-level tool you'll use to define your workflow's structure. It takes care of generating the necessary struct, the internal WorkItem enum (which tracks which node needs to run with what data), and the implementation of the Workflow trait, including all the complex routing logic. Here's the basic syntax: // General syntax of the workflow! macro workflow ! { // 1. Define the struct holding the Nodes pub struct MyWorkflow { // Fields are instances of your Nodes step_a : NodeAInstance , step_b : NodeBInstance , step_c : NodeCInstance , // You can also include retry policies here if needed // retry_policy: MyRetryPolicy, } // 2. Specify the Context type context = MyWorkflowContext ; // 3. Specify the starting Node field start = step_a ; // 4. Define the connections (edges) edges { // If step_a succeeds, its output goes to step_b step_a => [ step_b ]; // If step_b succeeds, its output goes to step_c // (Example: can also have conditional or fallback edges) step_b => [ step_c ]; // step_c is the last step (terminal node) step_c => []; // Empty list means it's an end point }; } Let's break it down: Struct Definition: pub struct MyWorkflow { ... } defines a struct that will hold instances of your Node s . Give each Node instance a field name (e.g., step_a , step_b ). context = Type; : Specifies the shared Context type this workflow uses (e.g., MyWorkflowData from Chapter 3). start = field_name; : Tells Floxide which field in your struct represents the first Node to execute (e.g., step_a ). edges { ... }; : This is the core mapping section. source_field => [target_field_1, target_field_2, ...]; defines where the output of the source_field Node should go upon successful completion ( Transition::Next or Transition::NextAll ). An empty list [] means this Node is a terminal point. (Advanced: The macro also supports conditional branching based on Node output variants and fallback paths using on_failure , but we'll stick to simple cases here).","title":"The workflow! Macro: Your Workflow Designer"},{"location":"floxide-tutorial/04__workflow__trait____workflow___macro_/#example-a-simple-text-processing-workflow","text":"Let's combine the Nodes from Chapter 2 into a simple workflow: PrefixAndCapitalizeNode -> CapitalizeTextNode . use floxide ::{ workflow , node , Workflow , Node , Context , Transition , FloxideError }; use serde ::{ Serialize , Deserialize }; use std :: sync :: Arc ; // Needed for callbacks in distributed step // --- Define Context (from Chapter 3) --- #[derive(Clone, Debug, Default, Serialize, Deserialize)] struct SimpleContext {} // No need for `impl Context for SimpleContext {}` if deriving required traits // --- Define Nodes (from Chapter 2, simplified) --- node ! { #[derive(Clone, Debug)] // Add Clone/Debug pub struct PrefixNode { prefix : String } context = SimpleContext ; input = String ; output = String ; | ctx , text | { let result = format! ( \"{}: {}\" , self . prefix , text ); println! ( \"PrefixNode: {} -> {}\" , text , result ); Ok ( Transition :: Next ( result )) } } node ! { #[derive(Clone, Debug)] // Add Clone/Debug pub struct CapitalizeNode {} context = SimpleContext ; input = String ; output = String ; | ctx , text | { let result = text . to_uppercase (); println! ( \"CapitalizeNode: {} -> {}\" , text , result ); Ok ( Transition :: Next ( result )) } } // --- Define the Workflow using the workflow! macro --- workflow ! { // Struct holds instances of our nodes #[derive(Clone, Debug)] // Workflow struct should also be Clone/Debug pub struct TextProcessor { add_prefix : PrefixNode , // Instance of PrefixNode capitalize : CapitalizeNode , // Instance of CapitalizeNode } // Use our simple context context = SimpleContext ; // Start with the 'add_prefix' node start = add_prefix ; // Define the flow: add_prefix -> capitalize -> end edges { add_prefix => [ capitalize ]; // Output of add_prefix goes to capitalize capitalize => []; // capitalize is the terminal node }; } // --- How you might use it (conceptual) --- // #[tokio::main] // async fn main() -> Result<(), FloxideError> { // // 1. Create instances of the Nodes // let prefixer = PrefixNode { prefix: \"INFO\".to_string() }; // let capitalizer = CapitalizeNode {}; // // // 2. Create an instance of the Workflow struct // let my_workflow = TextProcessor { // add_prefix: prefixer, // capitalize: capitalizer, // }; // // // 3. Create the initial context // let initial_context_data = SimpleContext {}; // let wf_ctx = floxide::WorkflowCtx::new(initial_context_data); // // // 4. Run the workflow (using the 'run' method from the Workflow trait) // let input_text = \"hello world\".to_string(); // println!(\"Starting workflow with input: '{}'\", input_text); // let final_result = my_workflow.run(&wf_ctx, input_text).await?; // // println!(\"Workflow finished. Final result: '{}'\", final_result); // // Expected output: // // Starting workflow with input: 'hello world' // // PrefixNode: hello world -> INFO: hello world // // CapitalizeNode: INFO: hello world -> INFO: HELLO WORLD // // Workflow finished. Final result: 'INFO: HELLO WORLD' // // Ok(()) // } Explanation: We defined our SimpleContext and two Nodes ( PrefixNode , CapitalizeNode ). We used workflow! to define TextProcessor . The struct holds fields add_prefix and capitalize . We set context = SimpleContext . We set start = add_prefix . In edges , we defined add_prefix => [capitalize]; (output of add_prefix goes to capitalize ) and capitalize => []; ( capitalize is the end). In the conceptual usage: We create instances of the Nodes and the TextProcessor workflow struct. We create the initial WorkflowCtx . We call the run method (which the workflow! macro generated for us!) with the context and initial input. The macro-generated code handles calling add_prefix , taking its output, calling capitalize with that output, and finally returning the result from capitalize because it's the terminal node. Distributed Emphasis: If we were running this distributed (using start_distributed and step_distributed ), the edges definition is absolutely critical. When add_prefix finishes on Worker 1, the macro-generated process_work_item logic consults the edges . It sees that capitalize is next. It then wraps the output of add_prefix into a WorkItem representing the capitalize task and places it onto the shared WorkQueue . Worker 2 (or maybe even Worker 1 again later) can then pick up this capitalize task from the queue and execute it. The workflow! macro essentially compiles your visual blueprint into executable routing logic.","title":"Example: A Simple Text Processing Workflow"},{"location":"floxide-tutorial/04__workflow__trait____workflow___macro_/#under-the-hood-what-does-workflow-generate","text":"The workflow! macro is a code generator. It takes your high-level definition and expands it into more detailed Rust code, saving you from writing a lot of boilerplate. Workflow Struct: It generates the struct you defined (e.g., pub struct TextProcessor { ... } ). WorkItem Enum: It creates a hidden helper enum (e.g., TextProcessorWorkItem ). This enum has one variant for each Node field in your workflow struct. This enum is how Floxide keeps track of \"what work needs to be done next\". // Simplified generated code (conceptual) #[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)] enum TextProcessorWorkItem { AddPrefix ( String ), // Carries input for PrefixNode Capitalize ( String ), // Carries input for CapitalizeNode } // Also implements Display for unique IDs, needed for WorkItem trait impl std :: fmt :: Display for TextProcessorWorkItem { /* ... */ } impl floxide_core :: workflow :: WorkItem for TextProcessorWorkItem { /* ... */ } impl Workflow : It generates the implementation of the Workflow trait for your struct ( impl Workflow<SimpleContext> for TextProcessor { ... } ). This implementation includes: Definitions for Input , Output , and WorkItem associated types. Implementations for run , run_with_checkpoint , resume , start_distributed , step_distributed , and to_dot . The crucial process_work_item method . This method contains a match statement on the generated WorkItem enum. Each arm corresponds to a Node in your workflow. Inside the arm, it: Calls the actual process method of the corresponding Node instance (e.g., self.add_prefix.process(...) ). Examines the returned Transition . Based on the Transition and your edges definition, it creates the next WorkItem (s) (e.g., TextProcessorWorkItem::Capitalize(...) ) and pushes them onto the work queue ( __q ) provided to it. If a terminal node finishes, it returns Ok(Some(output)) . Otherwise, it returns Ok(None) . Here's a simplified look at the generated process_work_item logic for our example: // Simplified generated code for process_work_item (conceptual) async fn _process_work_item <' a > ( &' a self , ctx : & ' a floxide_core :: WorkflowCtx < SimpleContext > , item : TextProcessorWorkItem , // Input is the generated enum __q : & mut std :: collections :: VecDeque < TextProcessorWorkItem > // Output queue ) -> Result < Option < String > , floxide_core :: error :: FloxideError > { use floxide_core :: transition :: Transition ; match item { TextProcessorWorkItem :: AddPrefix ( input_string ) => { // Call the actual node's process method let transition = self . add_prefix . process ( & ctx . store , input_string ). await ? ; match transition { Transition :: Next ( output_string ) => { // Look up edges for add_prefix: finds [capitalize] // Create the next work item let next_item = TextProcessorWorkItem :: Capitalize ( output_string ); // Add it to the queue __q . push_back ( next_item ); Ok ( None ) // More work to do } Transition :: NextAll ( outputs ) => { /* Handle fan-out similarly */ Ok ( None ) } Transition :: Abort ( e ) => Err ( e ), Transition :: Hold => Ok ( None ), // Do nothing for Hold } } TextProcessorWorkItem :: Capitalize ( input_string ) => { // Call the actual node's process method let transition = self . capitalize . process ( & ctx . store , input_string ). await ? ; match transition { Transition :: Next ( output_string ) => { // Look up edges for capitalize: finds [] (terminal) // Return the final output Ok ( Some ( output_string )) } Transition :: NextAll ( outputs ) => { /* Handle fan-out */ Ok ( None ) } // Might still be terminal Transition :: Abort ( e ) => Err ( e ), Transition :: Hold => Ok ( None ), } } } } This generated code directly translates your edges definition into the runtime logic that connects the Nodes. Here's a sequence diagram showing how the engine uses the generated process_work_item method during a run: sequenceDiagram participant Engine participant WorkflowImpl as TextProcessor Instance participant PrefixNode as self.add_prefix participant CapitalizeNode as self.capitalize participant Queue as Work Queue (__q) Note over Engine: Start run, initial item is AddPrefix(\"hello\") Engine->>WorkflowImpl: process_work_item(ctx, WorkItem::AddPrefix(\"hello\"), queue) WorkflowImpl->>PrefixNode: process(ctx.store, \"hello\") PrefixNode-->>WorkflowImpl: Return Ok(Transition::Next(\"INFO: hello\")) WorkflowImpl->>WorkflowImpl: Check edges for add_prefix -> finds [capitalize] WorkflowImpl->>Queue: Enqueue WorkItem::Capitalize(\"INFO: hello\") WorkflowImpl-->>Engine: Return Ok(None) (more work pending) Note over Engine: Later, process next item from queue... Engine->>WorkflowImpl: process_work_item(ctx, WorkItem::Capitalize(\"INFO: hello\"), queue) WorkflowImpl->>CapitalizeNode: process(ctx.store, \"INFO: hello\") CapitalizeNode-->>WorkflowImpl: Return Ok(Transition::Next(\"INFO: HELLO\")) WorkflowImpl->>WorkflowImpl: Check edges for capitalize -> finds [] (terminal) WorkflowImpl-->>Engine: Return Ok(Some(\"INFO: HELLO\")) (final output) The real implementation of the macro lives in floxide-macros/src/workflow.rs and the Workflow trait definition is in floxide-core/src/workflow.rs .","title":"Under the Hood: What Does workflow! Generate?"},{"location":"floxide-tutorial/04__workflow__trait____workflow___macro_/#conclusion","text":"The Workflow trait and the workflow! macro are central to defining the overall structure and execution flow in Floxide. A Workflow acts as the master blueprint, holding all Node s and defining their connections (edges). The Workflow trait defines the standard interface for executing workflows in various modes (local, checkpointed, distributed). The workflow! macro provides a convenient way to declare the workflow struct, its Nodes, the starting point, the shared Context , and the crucial edge mappings. The macro generates the necessary Rust code, including the Workflow trait implementation and the internal routing logic ( process_work_item ), which translates your high-level definition into executable steps. This explicit definition of structure and edges is vital for enabling reliable distributed execution , as it provides the map the engine needs to route tasks across different workers via a shared work queue. Now that we have defined our Nodes, Context, and the overall Workflow structure, how does Floxide manage the list of tasks waiting to be executed, especially in a distributed environment? That's the role of the WorkQueue . Next: Chapter 5: WorkQueue Trait","title":"Conclusion"},{"location":"floxide-tutorial/05__workqueue__trait_/","text":"Chapter 5: WorkQueue Trait \u00b6 In the previous chapter , we learned how to define the entire structure of our workflow using the Workflow trait and the workflow! macro. This blueprint tells Floxide which step ( Node ) follows which, creating the map of our distributed assembly line. But now, think about the items moving on that assembly line. When one station finishes its task, how does the item physically get to the next station? Especially if the next station is handled by a different worker, possibly on a completely different computer? We need a delivery system! What's the Problem? Distributing the Work \u00b6 Imagine our video processing workflow running across several computers (workers). * Worker 1 finishes downloading a video chunk ( DownloadNode ). * The workflow blueprint says the next step is ExtractAudioNode . * Worker 2 is free and ready to extract audio. How does Worker 2 know that Worker 1 just finished a download and needs audio extraction? How does it get the information about which video chunk to process? In a single program on one computer, this is easy \u2013 you just call the next function. But in a distributed system, where workers are independent processes, maybe even on different machines, they need a shared place to coordinate tasks. This is where the WorkQueue comes in. It's the central dispatch system for tasks in Floxide. What is a WorkQueue ? The Digital Bulletin Board \u00b6 Think of the WorkQueue as a shared, digital bulletin board or a job list that all workers can see. Work Item: Each task waiting to be done is represented by a WorkItem . A WorkItem contains information like: Which workflow run does this task belong to? (e.g., \"process_video_123\") Which Node needs to be executed? (e.g., ExtractAudioNode ) What input data does that Node need? (e.g., the path to the downloaded video chunk) (Internally, Floxide uses the WorkItem enum generated by the workflow! macro to represent this). Enqueue: When a Node finishes its processing and the workflow definition indicates the next step(s), the Floxide engine takes the output and creates one or more new WorkItem s. It then \"posts\" these WorkItem s onto the shared bulletin board. This is called enqueuing . Dequeue: Workers are constantly checking this bulletin board for new tasks. When a worker is free, it grabs the next available WorkItem from the board, effectively saying \"I'll take this job!\". This is called dequeuing . Once dequeued, the worker can execute the specified Node with the provided input data. Distributed Emphasis: The WorkQueue is the heart of task distribution in Floxide. It decouples the Node that produces work from the worker that consumes it. This allows different parts of the workflow to run on different machines, coordinated only through this shared queue. graph LR subgraph Worker Machine 1 NodeA[Node A finishes] -- Output --> Engine1[Floxide Engine] end subgraph Shared Infrastructure Queue[(Work Queue)] end subgraph Worker Machine 2 Engine2[Floxide Engine] -- \"Get next task\" --> Worker2[Worker Process] Worker2 -- \"Execute Node B\" --> NodeB[Node B logic] end Engine1 -- \"Enqueue Task (Node B, Output A)\" --> Queue Queue -- \"Dequeue Task (Node B, Output A)\" --> Engine2 style Queue fill:#f9f,stroke:#333,stroke-width:2px The WorkQueue Trait: The Contract for Queues \u00b6 Floxide needs a standard way to interact with any kind of queueing system. Whether you use a simple in-memory list, a powerful Redis database, or a streaming platform like Kafka, Floxide needs to know how to put tasks in ( enqueue ) and take tasks out ( dequeue ). This is defined by the WorkQueue trait. It's a Rust contract specifying the essential operations. Here's a simplified view of the trait: // Simplified concept from floxide-core/src/distributed/work_queue.rs use async_trait :: async_trait ; use crate :: context :: Context ; use crate :: workflow :: WorkItem ; use crate :: distributed :: WorkQueueError ; // Error type for queue operations #[async_trait] pub trait WorkQueue < C : Context , WI : WorkItem > : Clone + Send + Sync + ' static { // Put a work item onto the queue for a specific workflow run. async fn enqueue ( & self , workflow_id : & str , work : WI ) -> Result < (), WorkQueueError > ; // Get the next available work item from the queue (from any run). // Returns None if the queue is empty. async fn dequeue ( & self ) -> Result < Option < ( String , WI ) > , WorkQueueError > ; // Remove all pending work items for a specific workflow run. // Useful for cancellation or cleanup. async fn purge_run ( & self , run_id : & str ) -> Result < (), WorkQueueError > ; // (Other methods like getting pending work might exist) } Explanation: #[async_trait] : These methods are asynchronous ( async ) because interacting with external queue systems (like Redis or Kafka) involves network I/O. WorkQueue<C: Context, WI: WorkItem> : The trait is generic. It works with any Context type C and any WorkItem type WI defined by your Workflow . enqueue(&self, workflow_id: &str, work: WI) : Adds a work item associated with workflow_id to the queue. dequeue(&self) : Attempts to retrieve the next available work item. It returns the workflow_id and the WorkItem itself, or None if no tasks are waiting. purge_run(&self, run_id: &str) : Clears out any waiting tasks specifically for the given run_id . You typically don't implement this trait yourself unless you're integrating a custom queueing system. Floxide (or related crates) will provide implementations for common backends. How the System Uses the WorkQueue \u00b6 You, as the workflow developer using the node! and workflow! macros, usually don't call enqueue or dequeue directly. The Floxide engine and the DistributedWorker handle this behind the scenes. Engine (Processing Step): After a Node returns Transition::Next(output) or Transition::NextAll(outputs) , the engine (specifically, the code generated by workflow! ) determines the next Node(s) based on the edges . For each successor Node, it creates a WorkItem and calls queue.enqueue(...) . Worker (Idle): A DistributedWorker process, when idle, calls queue.dequeue() . Worker (Gets Task): If dequeue returns Some((run_id, work_item)) , the worker gets the run_id and the work_item . Worker (Executes Task): The worker loads the necessary state (using ContextStore for distributed runs or CheckpointStore for local/manual ones) for that run_id , finds the correct Node implementation based on the work_item , and executes its process method. Repeat: The worker finishes, potentially modifies the context (by appending events), and the engine (running within that worker) enqueues new tasks and ensures the context changes are persisted (via ContextStore.merge or CheckpointStore.save ). sequenceDiagram participant NodeLogic as Node Logic participant Engine as Floxide Engine (via workflow! macro) participant Queue as WorkQueue Instance participant Worker as DistributedWorker Note over NodeLogic, Engine: Worker processes WorkItem for Node A NodeLogic->>Engine: Returns Transition::Next(output_data) Engine->>Engine: Determines next step is Node B Engine->>Queue: enqueue(\"run_123\", WorkItem::NodeB(output_data)) Note over Worker, Queue: Later, Worker becomes idle... Worker->>Queue: dequeue() Queue-->>Worker: Returns Some((\"run_123\", WorkItem::NodeB(output_data))) Worker->>Worker: Loads state for \"run_123\" Worker->>Engine: Execute Node B with output_data Note over Worker: Cycle repeats... Different Flavors of Queues (Implementations) \u00b6 The power of using a trait ( WorkQueue ) is that you can swap out the underlying queue implementation without changing your core workflow logic. InMemoryWorkQueue : Floxide provides a simple queue that just uses standard Rust collections (like a HashMap mapping run IDs to VecDeque s) stored in the computer's memory. Pros: Very fast, easy for testing and local development, requires no external services. Cons: Not truly distributed (only works if all workers are threads within the same process), state is lost if the process crashes. Redis Queue: An implementation (like floxide_redis::RedisWorkQueue ) uses Redis LISTs. enqueue uses LPUSH , dequeue uses BRPOP (blocking pop). Pros: Persistent (if Redis persistence is configured), shared across multiple processes/machines, mature technology, readily available in the Floxide ecosystem. Cons: Requires a separate Redis server, slightly higher latency than in-memory. Kafka Queue: An implementation could use Kafka topics. enqueue produces a message, dequeue consumes a message (often using consumer groups for load balancing). Pros: Highly scalable, durable, good for high-throughput scenarios, supports complex streaming patterns. Cons: Requires a Kafka cluster, more complex setup than Redis. Database Queue: You could even implement a queue using a relational database table with locking. Pros: Leverages existing database infrastructure. Cons: Can be less performant than dedicated queues, requires careful handling of locking to avoid contention. The choice of implementation depends on your application's needs for scalability, persistence, and fault tolerance. For distributed execution, you'll need something other than InMemoryWorkQueue . Under the Hood: InMemoryWorkQueue Example \u00b6 Let's peek at how the simple InMemoryWorkQueue might implement the trait. It uses a Mutex (to handle concurrent access from multiple threads/tasks) around a HashMap . The HashMap keys are the run_id strings, and the values are VecDeque s (double-ended queues) holding the WorkItem s for that run. // Simplified from floxide-core/src/distributed/work_queue.rs use std :: collections ::{ HashMap , VecDeque }; use std :: sync :: Arc ; use tokio :: sync :: Mutex ; // ... other imports: WorkItem, Context, WorkQueue, WorkQueueError, async_trait // The struct holds the shared, mutable state protected by a Mutex #[derive(Clone)] pub struct InMemoryWorkQueue < WI : WorkItem > ( Arc < Mutex < HashMap < String , VecDeque < WI >>>> ); impl < WI : WorkItem > InMemoryWorkQueue < WI > { pub fn new () -> Self { Self ( Arc :: new ( Mutex :: new ( HashMap :: new ()))) // Start with empty map } } #[async_trait] impl < C : Context , WI : WorkItem + ' static > WorkQueue < C , WI > for InMemoryWorkQueue < WI > { async fn enqueue ( & self , workflow_id : & str , work : WI ) -> Result < (), WorkQueueError > { // 1. Lock the mutex to get exclusive access to the map let mut map = self . 0. lock (). await ; // 2. Find the queue for this workflow_id, or create it if it doesn't exist // 3. Add the work item to the end of that queue map . entry ( workflow_id . to_string ()) . or_default () . push_back ( work ); // 4. Unlock happens automatically when 'map' goes out of scope Ok (()) } async fn dequeue ( & self ) -> Result < Option < ( String , WI ) > , WorkQueueError > { // 1. Lock the mutex let mut map = self . 0. lock (). await ; // 2. Iterate through all known workflow runs for ( run_id , q ) in map . iter_mut () { // 3. Try to remove an item from the front of the run's queue if let Some ( item ) = q . pop_front () { // 4. If successful, return the run_id and the item return Ok ( Some (( run_id . clone (), item ))); } } // 5. If no items were found in any queue, return None Ok ( None ) } async fn purge_run ( & self , run_id : & str ) -> Result < (), WorkQueueError > { let mut map = self . 0. lock (). await ; // Remove the entry for this run_id, discarding all its items map . remove ( run_id ); Ok (()) } // ... other methods ... } Explanation: Arc<Mutex<...>> : This combination allows safe shared access to the HashMap from multiple asynchronous tasks. Arc allows multiple owners, Mutex ensures only one task accesses the data at a time. lock().await : Acquires the lock. If another task holds the lock, this task waits ( await s). map.entry(...).or_default().push_back(work) : A concise way to get the VecDeque for a workflow_id (creating it if needed) and add the work item. map.iter_mut() : Allows iterating through the runs and modifying their queues. q.pop_front() : Removes and returns the first item from the VecDeque , if any. map.remove(run_id) : Removes the entire queue for the specified run. This simple implementation fulfills the WorkQueue contract using standard Rust tools, making it suitable for single-process scenarios. For true distribution, you'd use a different implementation backed by a service like Redis or Kafka. Conclusion \u00b6 The WorkQueue trait defines the standard interface for the task distribution mechanism in Floxide. It acts as the central coordinator in a distributed workflow, allowing Nodes finishing on one worker to enqueue tasks ( WorkItem s) that can be dequeued and processed by other available workers. It's the core component enabling distributed execution . It decouples task producers (Nodes finishing) from task consumers (Workers starting). The WorkQueue trait provides standard enqueue and dequeue operations. Different implementations (in-memory, Redis, Kafka) can be used depending on requirements, all conforming to the same trait. While the queue manages what tasks need to be run, how does a worker resuming a task, or picking up a task mid-workflow, know the state of the shared Context ? For local/manual runs, we use the CheckpointStore . For distributed runs, we primarily use the ContextStore to manage the shared data, working alongside the WorkQueue and other specialized stores. Next: Chapter 6: Checkpoint & CheckpointStore Trait See Also: Chapter 9: Distributed Stores (Covers ContextStore and other stores used with WorkQueue )","title":"Chapter 5: WorkQueue Trait"},{"location":"floxide-tutorial/05__workqueue__trait_/#chapter-5-workqueue-trait","text":"In the previous chapter , we learned how to define the entire structure of our workflow using the Workflow trait and the workflow! macro. This blueprint tells Floxide which step ( Node ) follows which, creating the map of our distributed assembly line. But now, think about the items moving on that assembly line. When one station finishes its task, how does the item physically get to the next station? Especially if the next station is handled by a different worker, possibly on a completely different computer? We need a delivery system!","title":"Chapter 5: WorkQueue Trait"},{"location":"floxide-tutorial/05__workqueue__trait_/#whats-the-problem-distributing-the-work","text":"Imagine our video processing workflow running across several computers (workers). * Worker 1 finishes downloading a video chunk ( DownloadNode ). * The workflow blueprint says the next step is ExtractAudioNode . * Worker 2 is free and ready to extract audio. How does Worker 2 know that Worker 1 just finished a download and needs audio extraction? How does it get the information about which video chunk to process? In a single program on one computer, this is easy \u2013 you just call the next function. But in a distributed system, where workers are independent processes, maybe even on different machines, they need a shared place to coordinate tasks. This is where the WorkQueue comes in. It's the central dispatch system for tasks in Floxide.","title":"What's the Problem? Distributing the Work"},{"location":"floxide-tutorial/05__workqueue__trait_/#what-is-a-workqueue-the-digital-bulletin-board","text":"Think of the WorkQueue as a shared, digital bulletin board or a job list that all workers can see. Work Item: Each task waiting to be done is represented by a WorkItem . A WorkItem contains information like: Which workflow run does this task belong to? (e.g., \"process_video_123\") Which Node needs to be executed? (e.g., ExtractAudioNode ) What input data does that Node need? (e.g., the path to the downloaded video chunk) (Internally, Floxide uses the WorkItem enum generated by the workflow! macro to represent this). Enqueue: When a Node finishes its processing and the workflow definition indicates the next step(s), the Floxide engine takes the output and creates one or more new WorkItem s. It then \"posts\" these WorkItem s onto the shared bulletin board. This is called enqueuing . Dequeue: Workers are constantly checking this bulletin board for new tasks. When a worker is free, it grabs the next available WorkItem from the board, effectively saying \"I'll take this job!\". This is called dequeuing . Once dequeued, the worker can execute the specified Node with the provided input data. Distributed Emphasis: The WorkQueue is the heart of task distribution in Floxide. It decouples the Node that produces work from the worker that consumes it. This allows different parts of the workflow to run on different machines, coordinated only through this shared queue. graph LR subgraph Worker Machine 1 NodeA[Node A finishes] -- Output --> Engine1[Floxide Engine] end subgraph Shared Infrastructure Queue[(Work Queue)] end subgraph Worker Machine 2 Engine2[Floxide Engine] -- \"Get next task\" --> Worker2[Worker Process] Worker2 -- \"Execute Node B\" --> NodeB[Node B logic] end Engine1 -- \"Enqueue Task (Node B, Output A)\" --> Queue Queue -- \"Dequeue Task (Node B, Output A)\" --> Engine2 style Queue fill:#f9f,stroke:#333,stroke-width:2px","title":"What is a WorkQueue? The Digital Bulletin Board"},{"location":"floxide-tutorial/05__workqueue__trait_/#the-workqueue-trait-the-contract-for-queues","text":"Floxide needs a standard way to interact with any kind of queueing system. Whether you use a simple in-memory list, a powerful Redis database, or a streaming platform like Kafka, Floxide needs to know how to put tasks in ( enqueue ) and take tasks out ( dequeue ). This is defined by the WorkQueue trait. It's a Rust contract specifying the essential operations. Here's a simplified view of the trait: // Simplified concept from floxide-core/src/distributed/work_queue.rs use async_trait :: async_trait ; use crate :: context :: Context ; use crate :: workflow :: WorkItem ; use crate :: distributed :: WorkQueueError ; // Error type for queue operations #[async_trait] pub trait WorkQueue < C : Context , WI : WorkItem > : Clone + Send + Sync + ' static { // Put a work item onto the queue for a specific workflow run. async fn enqueue ( & self , workflow_id : & str , work : WI ) -> Result < (), WorkQueueError > ; // Get the next available work item from the queue (from any run). // Returns None if the queue is empty. async fn dequeue ( & self ) -> Result < Option < ( String , WI ) > , WorkQueueError > ; // Remove all pending work items for a specific workflow run. // Useful for cancellation or cleanup. async fn purge_run ( & self , run_id : & str ) -> Result < (), WorkQueueError > ; // (Other methods like getting pending work might exist) } Explanation: #[async_trait] : These methods are asynchronous ( async ) because interacting with external queue systems (like Redis or Kafka) involves network I/O. WorkQueue<C: Context, WI: WorkItem> : The trait is generic. It works with any Context type C and any WorkItem type WI defined by your Workflow . enqueue(&self, workflow_id: &str, work: WI) : Adds a work item associated with workflow_id to the queue. dequeue(&self) : Attempts to retrieve the next available work item. It returns the workflow_id and the WorkItem itself, or None if no tasks are waiting. purge_run(&self, run_id: &str) : Clears out any waiting tasks specifically for the given run_id . You typically don't implement this trait yourself unless you're integrating a custom queueing system. Floxide (or related crates) will provide implementations for common backends.","title":"The WorkQueue Trait: The Contract for Queues"},{"location":"floxide-tutorial/05__workqueue__trait_/#how-the-system-uses-the-workqueue","text":"You, as the workflow developer using the node! and workflow! macros, usually don't call enqueue or dequeue directly. The Floxide engine and the DistributedWorker handle this behind the scenes. Engine (Processing Step): After a Node returns Transition::Next(output) or Transition::NextAll(outputs) , the engine (specifically, the code generated by workflow! ) determines the next Node(s) based on the edges . For each successor Node, it creates a WorkItem and calls queue.enqueue(...) . Worker (Idle): A DistributedWorker process, when idle, calls queue.dequeue() . Worker (Gets Task): If dequeue returns Some((run_id, work_item)) , the worker gets the run_id and the work_item . Worker (Executes Task): The worker loads the necessary state (using ContextStore for distributed runs or CheckpointStore for local/manual ones) for that run_id , finds the correct Node implementation based on the work_item , and executes its process method. Repeat: The worker finishes, potentially modifies the context (by appending events), and the engine (running within that worker) enqueues new tasks and ensures the context changes are persisted (via ContextStore.merge or CheckpointStore.save ). sequenceDiagram participant NodeLogic as Node Logic participant Engine as Floxide Engine (via workflow! macro) participant Queue as WorkQueue Instance participant Worker as DistributedWorker Note over NodeLogic, Engine: Worker processes WorkItem for Node A NodeLogic->>Engine: Returns Transition::Next(output_data) Engine->>Engine: Determines next step is Node B Engine->>Queue: enqueue(\"run_123\", WorkItem::NodeB(output_data)) Note over Worker, Queue: Later, Worker becomes idle... Worker->>Queue: dequeue() Queue-->>Worker: Returns Some((\"run_123\", WorkItem::NodeB(output_data))) Worker->>Worker: Loads state for \"run_123\" Worker->>Engine: Execute Node B with output_data Note over Worker: Cycle repeats...","title":"How the System Uses the WorkQueue"},{"location":"floxide-tutorial/05__workqueue__trait_/#different-flavors-of-queues-implementations","text":"The power of using a trait ( WorkQueue ) is that you can swap out the underlying queue implementation without changing your core workflow logic. InMemoryWorkQueue : Floxide provides a simple queue that just uses standard Rust collections (like a HashMap mapping run IDs to VecDeque s) stored in the computer's memory. Pros: Very fast, easy for testing and local development, requires no external services. Cons: Not truly distributed (only works if all workers are threads within the same process), state is lost if the process crashes. Redis Queue: An implementation (like floxide_redis::RedisWorkQueue ) uses Redis LISTs. enqueue uses LPUSH , dequeue uses BRPOP (blocking pop). Pros: Persistent (if Redis persistence is configured), shared across multiple processes/machines, mature technology, readily available in the Floxide ecosystem. Cons: Requires a separate Redis server, slightly higher latency than in-memory. Kafka Queue: An implementation could use Kafka topics. enqueue produces a message, dequeue consumes a message (often using consumer groups for load balancing). Pros: Highly scalable, durable, good for high-throughput scenarios, supports complex streaming patterns. Cons: Requires a Kafka cluster, more complex setup than Redis. Database Queue: You could even implement a queue using a relational database table with locking. Pros: Leverages existing database infrastructure. Cons: Can be less performant than dedicated queues, requires careful handling of locking to avoid contention. The choice of implementation depends on your application's needs for scalability, persistence, and fault tolerance. For distributed execution, you'll need something other than InMemoryWorkQueue .","title":"Different Flavors of Queues (Implementations)"},{"location":"floxide-tutorial/05__workqueue__trait_/#under-the-hood-inmemoryworkqueue-example","text":"Let's peek at how the simple InMemoryWorkQueue might implement the trait. It uses a Mutex (to handle concurrent access from multiple threads/tasks) around a HashMap . The HashMap keys are the run_id strings, and the values are VecDeque s (double-ended queues) holding the WorkItem s for that run. // Simplified from floxide-core/src/distributed/work_queue.rs use std :: collections ::{ HashMap , VecDeque }; use std :: sync :: Arc ; use tokio :: sync :: Mutex ; // ... other imports: WorkItem, Context, WorkQueue, WorkQueueError, async_trait // The struct holds the shared, mutable state protected by a Mutex #[derive(Clone)] pub struct InMemoryWorkQueue < WI : WorkItem > ( Arc < Mutex < HashMap < String , VecDeque < WI >>>> ); impl < WI : WorkItem > InMemoryWorkQueue < WI > { pub fn new () -> Self { Self ( Arc :: new ( Mutex :: new ( HashMap :: new ()))) // Start with empty map } } #[async_trait] impl < C : Context , WI : WorkItem + ' static > WorkQueue < C , WI > for InMemoryWorkQueue < WI > { async fn enqueue ( & self , workflow_id : & str , work : WI ) -> Result < (), WorkQueueError > { // 1. Lock the mutex to get exclusive access to the map let mut map = self . 0. lock (). await ; // 2. Find the queue for this workflow_id, or create it if it doesn't exist // 3. Add the work item to the end of that queue map . entry ( workflow_id . to_string ()) . or_default () . push_back ( work ); // 4. Unlock happens automatically when 'map' goes out of scope Ok (()) } async fn dequeue ( & self ) -> Result < Option < ( String , WI ) > , WorkQueueError > { // 1. Lock the mutex let mut map = self . 0. lock (). await ; // 2. Iterate through all known workflow runs for ( run_id , q ) in map . iter_mut () { // 3. Try to remove an item from the front of the run's queue if let Some ( item ) = q . pop_front () { // 4. If successful, return the run_id and the item return Ok ( Some (( run_id . clone (), item ))); } } // 5. If no items were found in any queue, return None Ok ( None ) } async fn purge_run ( & self , run_id : & str ) -> Result < (), WorkQueueError > { let mut map = self . 0. lock (). await ; // Remove the entry for this run_id, discarding all its items map . remove ( run_id ); Ok (()) } // ... other methods ... } Explanation: Arc<Mutex<...>> : This combination allows safe shared access to the HashMap from multiple asynchronous tasks. Arc allows multiple owners, Mutex ensures only one task accesses the data at a time. lock().await : Acquires the lock. If another task holds the lock, this task waits ( await s). map.entry(...).or_default().push_back(work) : A concise way to get the VecDeque for a workflow_id (creating it if needed) and add the work item. map.iter_mut() : Allows iterating through the runs and modifying their queues. q.pop_front() : Removes and returns the first item from the VecDeque , if any. map.remove(run_id) : Removes the entire queue for the specified run. This simple implementation fulfills the WorkQueue contract using standard Rust tools, making it suitable for single-process scenarios. For true distribution, you'd use a different implementation backed by a service like Redis or Kafka.","title":"Under the Hood: InMemoryWorkQueue Example"},{"location":"floxide-tutorial/05__workqueue__trait_/#conclusion","text":"The WorkQueue trait defines the standard interface for the task distribution mechanism in Floxide. It acts as the central coordinator in a distributed workflow, allowing Nodes finishing on one worker to enqueue tasks ( WorkItem s) that can be dequeued and processed by other available workers. It's the core component enabling distributed execution . It decouples task producers (Nodes finishing) from task consumers (Workers starting). The WorkQueue trait provides standard enqueue and dequeue operations. Different implementations (in-memory, Redis, Kafka) can be used depending on requirements, all conforming to the same trait. While the queue manages what tasks need to be run, how does a worker resuming a task, or picking up a task mid-workflow, know the state of the shared Context ? For local/manual runs, we use the CheckpointStore . For distributed runs, we primarily use the ContextStore to manage the shared data, working alongside the WorkQueue and other specialized stores. Next: Chapter 6: Checkpoint & CheckpointStore Trait See Also: Chapter 9: Distributed Stores (Covers ContextStore and other stores used with WorkQueue )","title":"Conclusion"},{"location":"floxide-tutorial/06__checkpoint_____checkpointstore__trait_/","text":"Chapter 6: Checkpoint & CheckpointStore Trait \u00b6 In the previous chapter , we saw how the WorkQueue acts like a central bulletin board, allowing different workers to pick up tasks in a distributed workflow. This is great for distributing the work , but what happens if something goes wrong? Or how does a worker know the current state of the shared information ( WorkflowCtx ) when it picks up a task mid-way through a long workflow? What's the Problem? Remembering Where We Left Off \u00b6 Imagine playing a long video game. You wouldn't want to start from the very beginning every time you turn the game off or if the power suddenly goes out! You need a way to save your progress . Similarly, workflows, especially distributed ones, can be long-running and complex. * What if a worker processing a crucial video segment crashes? Do we lose all the work done so far? * If Worker A finishes processing step 1 and puts a task for step 2 onto the WorkQueue , how does Worker B (which might be on a different computer) know the latest value of the shared api_key or processed_items_count from the workflow's Context when it picks up the step 2 task? We need a mechanism to: 1. Save the current state of a running workflow at specific points. 2. Load that state back to resume the workflow later, either after a crash or when a different worker takes over. This mechanism is called checkpointing , and it's absolutely essential for fault tolerance and effective state management in distributed systems like Floxide. Important Scope Note: The Checkpoint struct and CheckpointStore trait described in this chapter are the primary mechanisms for saving and resuming the complete state (shared data and pending tasks) of a workflow run. This is particularly useful for single-process workflows where you want to be able to stop and resume later, or for scenarios where you are manually managing the execution flow (like the checkpoint_example.rs ). For fully distributed workflows managed by Floxide's Orchestrator and WorkerPool , a different set of more granular storage traits ( ContextStore , WorkQueue , etc., discussed in Chapter 9 ) are typically used for better efficiency and control in a multi-worker environment. We will focus on the core Checkpoint / CheckpointStore mechanism here. Checkpoint : The Workflow Snapshot \u00b6 A Checkpoint in Floxide is like taking a snapshot of your workflow's current progress. It captures two critical pieces of information at a particular moment in time for a specific workflow run: The Shared Context: The current state of your custom data stored inside the WorkflowCtx::store . This ensures that when the workflow resumes, it has the correct shared information (like API keys, counters, configuration loaded mid-way, etc.). The Pending Work Queue: The list of WorkItem s that are still waiting to be processed for this specific workflow run . This ensures the workflow knows which steps are next. Think of it like saving your game: the save file contains your character's stats, inventory (like the context), and the list of quests you still need to do (like the pending work queue). Floxide defines a simple struct to hold this snapshot: // Simplified from crates/floxide-core/src/checkpoint.rs use std :: collections :: VecDeque ; // A type of queue use crate :: context :: Context ; use crate :: workflow :: WorkItem ; /// A snapshot of a workflow's pending work and its context. #[derive(Debug, Clone)] // Allows copying and printing pub struct Checkpoint < C : Context , WI : WorkItem > { /// The user-provided context for the workflow pub context : C , /// The queue of pending work items for this run pub queue : VecDeque < WI > , } context: C : Holds the actual instance of your shared Context data (e.g., MyWorkflowData from Chapter 3). queue: VecDeque<WI> : Holds the list of WorkItem s generated by the workflow! macro that haven't been processed yet for this run. By using a trait, Floxide allows you to choose the storage backend that fits your needs: * InMemoryCheckpointStore : Stores checkpoints in memory (simple, good for tests, but not persistent or distributed). * Database Store : Could store checkpoints as rows in a database table. * File System Store : Could save checkpoints as files on disk. * Cloud Storage Store : Could save checkpoints to services like AWS S3 or Google Cloud Storage. * (Distributed Alternative) : As mentioned, dedicated stores like RedisContextStore , RedisWorkQueue (using different traits) are used for scalable distributed setups. Checkpointing vs. Distributed Stores ( ContextStore , WorkQueue ) \u00b6 It's important to understand the distinction between the Checkpoint / CheckpointStore mechanism described here and the storage approach used in fully managed distributed Floxide applications (using Orchestrator and WorkerPool , covered later). Checkpoint : Bundles the shared Context and the pending WorkItem queue for a specific run into a single snapshot. CheckpointStore : Saves and loads this combined snapshot. This approach is simple and effective for saving the entire state of a run to resume it later in the same process or under manual control . However, in a large distributed system with many workers: * Workers often only need the latest shared Context to perform their task. Loading the potentially large list of all pending WorkItem s for the entire run along with the context can be inefficient. * Task distribution is typically handled globally by a central WorkQueue , not by each worker loading a run-specific queue from a checkpoint. Therefore, for distributed execution, Floxide uses a set of more specialized store traits (detailed in Chapter 9 ): ContextStore : Manages only the saving, loading, and merging of the shared Context data for a run. WorkQueue : Handles the global enqueueing and dequeueing of WorkItem s across all runs and workers. WorkItemStateStore : Tracks the status (e.g., pending, in-progress, completed, failed) and retry attempts for individual WorkItem instances within a run. Other Stores: RunInfoStore , MetricsStore , ErrorStore , LivenessStore handle other aspects of monitoring and managing distributed runs. This separation allows for more efficient state management in a distributed environment, as workers fetch only the specific state they need (e.g., context via ContextStore , next task via WorkQueue ). In summary: Use CheckpointStore for local persistence/resumption. Use the distributed store collection ( ContextStore , WorkQueue , etc.) when building scalable, multi-worker applications with Orchestrator and WorkerPool . How Checkpointing Enables Fault-Tolerant Workflows (Local/Manual Context) \u00b6 Let's see how the Checkpoint and CheckpointStore work together, especially in a distributed setup. Start: A workflow run starts (e.g., run ID \"video_abc\"). An initial checkpoint might be saved with the starting context and the first task in the queue. Worker A Takes Task 1: Worker A loads the checkpoint for \"video_abc\". It gets the context and sees Task 1 is next. It processes Task 1. Worker A Finishes Task 1: Let's say Task 1 updated the context (e.g., incremented a counter) and produced Task 2 and Task 3. Worker A Saves Checkpoint: The Floxide engine (running within Worker A) creates a new Checkpoint containing: The updated context. The remaining work queue (now containing Task 2 and Task 3). It calls store.save(\"video_abc\", new_checkpoint) . The old checkpoint is overwritten. Crash! (Optional): Worker A crashes immediately after saving. No work is lost because the progress is saved! Worker B Takes Task 2: Some time later, Worker B (maybe on a different machine) dequeues Task 2 from the main WorkQueue . Worker B Loads Checkpoint: Before processing Task 2, Worker B calls store.load(\"video_abc\") . It receives the checkpoint saved by Worker A in step 4. Worker B Resumes: Worker B now has the correct, updated context (with the incremented counter) and knows that Task 3 is also pending (from the checkpoint's queue). It proceeds to execute Task 2. sequenceDiagram participant WorkerA as Worker A participant Store as Checkpoint Store participant WorkerB as Worker B participant Engine as Floxide Engine Note over WorkerA, Store: Run \"video_abc\", Worker A processes Task 1 WorkerA->>Engine: Finish Task 1 (Context updated, Task 2 & 3 next) Engine->>Engine: Create Checkpoint { updated_context, queue:[Task2, Task3] } Engine->>Store: save(\"video_abc\", checkpoint) Store-->>Engine: Ok Note over WorkerA, WorkerB: Worker A might crash here - state is safe! Note over WorkerB, Store: Later, Worker B needs to process Task 2 for \"video_abc\" WorkerB->>Store: load(\"video_abc\") Store-->>WorkerB: Return Checkpoint { updated_context, queue:[Task2, Task3] } WorkerB->>Engine: Restore context, know Task 3 is pending Engine->>WorkerB: Execute Task 2 with updated_context This save/load cycle ensures that state is consistently passed between steps, even across different workers or process restarts. Example: InMemoryCheckpointStore \u00b6 Floxide provides a basic InMemoryCheckpointStore for testing and simple cases. It uses a standard Rust HashMap protected by a RwLock (allowing multiple readers or one writer) to store checkpoints in memory. // Simplified from crates/floxide-core/src/checkpoint.rs use std :: collections :: HashMap ; use std :: sync ::{ Arc , RwLock }; // For thread-safe sharing // ... other imports #[derive(Clone)] pub struct InMemoryCheckpointStore < C : Context , WI : WorkItem > { // Arc allows sharing ownership across threads/tasks // RwLock allows multiple readers or one writer at a time inner : Arc < RwLock < HashMap < String , Checkpoint < C , WI >>>> , // PhantomData might be needed depending on generic usage _phantom : std :: marker :: PhantomData < ( C , WI ) > , } // --- Implementation of the CheckpointStore trait --- #[async_trait] impl < C : Context , WI : WorkItem > CheckpointStore < C , WI > for InMemoryCheckpointStore < C , WI > { async fn save ( & self , workflow_id : & str , checkpoint : & Checkpoint < C , WI > , ) -> Result < (), CheckpointError > { // Get write access to the map (blocks other writers/readers) let mut map = self . inner . write (). unwrap (); // .unwrap() simplifies error handling here // Insert a clone of the checkpoint into the map map . insert ( workflow_id . to_string (), checkpoint . clone ()); Ok (()) // Lock is released automatically when 'map' goes out of scope } async fn load ( & self , workflow_id : & str ) -> Result < Option < Checkpoint < C , WI >> , CheckpointError > { // Get read access to the map (blocks writers, allows other readers) let map = self . inner . read (). unwrap (); // Get the checkpoint, clone it if found let maybe_checkpoint = map . get ( workflow_id ). map ( | ck | ck . clone ()); Ok ( maybe_checkpoint ) // Lock is released automatically when 'map' goes out of scope } } Arc<RwLock<HashMap<...>>> : The standard Rust way to share mutable data safely across asynchronous tasks. inner.write().unwrap() : Acquires a write lock on the map. inner.read().unwrap() : Acquires a read lock. map.insert(...) / map.get(...) : Standard HashMap operations. .clone() : We clone the Checkpoint when saving and loading to avoid ownership issues with the map. This requires C and WI to be Clone . This implementation is simple but effective for single-process scenarios. For real distributed systems, you'd use an implementation backed by Redis, a database, or another shared storage mechanism, likely implementing the more granular distributed store traits ( ContextStore , etc.). Under the Hood: Serialization is Key \u00b6 How does the CheckpointStore actually save your custom Context data ( C ) and the WorkItem ( WI )? It relies on serialization . Serialization: Converting the in-memory Rust structs ( Checkpoint , your Context , your WorkItem ) into a format that can be stored or transmitted (like JSON, MessagePack, Protobuf, etc.). Deserialization: Converting the stored format back into the Rust structs. Floxide leverages the powerful serde library for this. Remember in Chapter 3 , we required your Context struct to derive Serialize and Deserialize ? And the workflow! macro automatically derives them for the WorkItem enum it generates. The CheckpointStore implementation (like InMemoryCheckpointStore , or a hypothetical RedisCheckpointStore ) is responsible for: 1. During save : Taking the Checkpoint<C, WI> struct and using serde (e.g., serde_json::to_string or serde_json::to_vec ) to turn it into a byte array or string. Then, storing those bytes/string in its backend (e.g., the in-memory HashMap , a Redis key, a database BLOB). 2. During load : Retrieving the stored bytes/string from the backend. Then, using serde (e.g., serde_json::from_slice or serde_json::from_str ) to parse it back into a Checkpoint<C, WI> struct. This serialization is what allows complex Rust data structures representing your workflow's state to be saved, shared across machines, and loaded back reliably. Conclusion \u00b6 Checkpoints are the \"save game\" mechanism for Floxide workflows. A Checkpoint is a snapshot containing the current shared Context and the list of pending WorkItem s for a specific run, primarily used for local/manual persistence. The CheckpointStore trait defines the standard interface ( save , load ) for persisting and retrieving these snapshots. Implementations of CheckpointStore (like InMemoryCheckpointStore ) handle the actual storage and rely on serde for serialization. Checkpointing is crucial for fault tolerance in single-process or manually managed runs. For distributed execution , Floxide uses a different set of stores ( ContextStore , WorkQueue , etc.) for better scalability and separation of concerns. Now that we have the core pieces for basic execution and local persistence, let's explore how Floxide manages execution in a fully distributed environment. Next: Chapter 7: DistributedWorker (Focuses on the worker component which uses the distributed stores) See Also: Chapter 9: Distributed Stores (Details the stores like ContextStore used by workers/orchestrators)","title":"Chapter 6: Checkpoint & CheckpointStore Trait"},{"location":"floxide-tutorial/06__checkpoint_____checkpointstore__trait_/#chapter-6-checkpoint-checkpointstore-trait","text":"In the previous chapter , we saw how the WorkQueue acts like a central bulletin board, allowing different workers to pick up tasks in a distributed workflow. This is great for distributing the work , but what happens if something goes wrong? Or how does a worker know the current state of the shared information ( WorkflowCtx ) when it picks up a task mid-way through a long workflow?","title":"Chapter 6: Checkpoint &amp; CheckpointStore Trait"},{"location":"floxide-tutorial/06__checkpoint_____checkpointstore__trait_/#whats-the-problem-remembering-where-we-left-off","text":"Imagine playing a long video game. You wouldn't want to start from the very beginning every time you turn the game off or if the power suddenly goes out! You need a way to save your progress . Similarly, workflows, especially distributed ones, can be long-running and complex. * What if a worker processing a crucial video segment crashes? Do we lose all the work done so far? * If Worker A finishes processing step 1 and puts a task for step 2 onto the WorkQueue , how does Worker B (which might be on a different computer) know the latest value of the shared api_key or processed_items_count from the workflow's Context when it picks up the step 2 task? We need a mechanism to: 1. Save the current state of a running workflow at specific points. 2. Load that state back to resume the workflow later, either after a crash or when a different worker takes over. This mechanism is called checkpointing , and it's absolutely essential for fault tolerance and effective state management in distributed systems like Floxide. Important Scope Note: The Checkpoint struct and CheckpointStore trait described in this chapter are the primary mechanisms for saving and resuming the complete state (shared data and pending tasks) of a workflow run. This is particularly useful for single-process workflows where you want to be able to stop and resume later, or for scenarios where you are manually managing the execution flow (like the checkpoint_example.rs ). For fully distributed workflows managed by Floxide's Orchestrator and WorkerPool , a different set of more granular storage traits ( ContextStore , WorkQueue , etc., discussed in Chapter 9 ) are typically used for better efficiency and control in a multi-worker environment. We will focus on the core Checkpoint / CheckpointStore mechanism here.","title":"What's the Problem? Remembering Where We Left Off"},{"location":"floxide-tutorial/06__checkpoint_____checkpointstore__trait_/#checkpoint-the-workflow-snapshot","text":"A Checkpoint in Floxide is like taking a snapshot of your workflow's current progress. It captures two critical pieces of information at a particular moment in time for a specific workflow run: The Shared Context: The current state of your custom data stored inside the WorkflowCtx::store . This ensures that when the workflow resumes, it has the correct shared information (like API keys, counters, configuration loaded mid-way, etc.). The Pending Work Queue: The list of WorkItem s that are still waiting to be processed for this specific workflow run . This ensures the workflow knows which steps are next. Think of it like saving your game: the save file contains your character's stats, inventory (like the context), and the list of quests you still need to do (like the pending work queue). Floxide defines a simple struct to hold this snapshot: // Simplified from crates/floxide-core/src/checkpoint.rs use std :: collections :: VecDeque ; // A type of queue use crate :: context :: Context ; use crate :: workflow :: WorkItem ; /// A snapshot of a workflow's pending work and its context. #[derive(Debug, Clone)] // Allows copying and printing pub struct Checkpoint < C : Context , WI : WorkItem > { /// The user-provided context for the workflow pub context : C , /// The queue of pending work items for this run pub queue : VecDeque < WI > , } context: C : Holds the actual instance of your shared Context data (e.g., MyWorkflowData from Chapter 3). queue: VecDeque<WI> : Holds the list of WorkItem s generated by the workflow! macro that haven't been processed yet for this run. By using a trait, Floxide allows you to choose the storage backend that fits your needs: * InMemoryCheckpointStore : Stores checkpoints in memory (simple, good for tests, but not persistent or distributed). * Database Store : Could store checkpoints as rows in a database table. * File System Store : Could save checkpoints as files on disk. * Cloud Storage Store : Could save checkpoints to services like AWS S3 or Google Cloud Storage. * (Distributed Alternative) : As mentioned, dedicated stores like RedisContextStore , RedisWorkQueue (using different traits) are used for scalable distributed setups.","title":"Checkpoint: The Workflow Snapshot"},{"location":"floxide-tutorial/06__checkpoint_____checkpointstore__trait_/#checkpointing-vs-distributed-stores-contextstore-workqueue","text":"It's important to understand the distinction between the Checkpoint / CheckpointStore mechanism described here and the storage approach used in fully managed distributed Floxide applications (using Orchestrator and WorkerPool , covered later). Checkpoint : Bundles the shared Context and the pending WorkItem queue for a specific run into a single snapshot. CheckpointStore : Saves and loads this combined snapshot. This approach is simple and effective for saving the entire state of a run to resume it later in the same process or under manual control . However, in a large distributed system with many workers: * Workers often only need the latest shared Context to perform their task. Loading the potentially large list of all pending WorkItem s for the entire run along with the context can be inefficient. * Task distribution is typically handled globally by a central WorkQueue , not by each worker loading a run-specific queue from a checkpoint. Therefore, for distributed execution, Floxide uses a set of more specialized store traits (detailed in Chapter 9 ): ContextStore : Manages only the saving, loading, and merging of the shared Context data for a run. WorkQueue : Handles the global enqueueing and dequeueing of WorkItem s across all runs and workers. WorkItemStateStore : Tracks the status (e.g., pending, in-progress, completed, failed) and retry attempts for individual WorkItem instances within a run. Other Stores: RunInfoStore , MetricsStore , ErrorStore , LivenessStore handle other aspects of monitoring and managing distributed runs. This separation allows for more efficient state management in a distributed environment, as workers fetch only the specific state they need (e.g., context via ContextStore , next task via WorkQueue ). In summary: Use CheckpointStore for local persistence/resumption. Use the distributed store collection ( ContextStore , WorkQueue , etc.) when building scalable, multi-worker applications with Orchestrator and WorkerPool .","title":"Checkpointing vs. Distributed Stores (ContextStore, WorkQueue)"},{"location":"floxide-tutorial/06__checkpoint_____checkpointstore__trait_/#how-checkpointing-enables-fault-tolerant-workflows-localmanual-context","text":"Let's see how the Checkpoint and CheckpointStore work together, especially in a distributed setup. Start: A workflow run starts (e.g., run ID \"video_abc\"). An initial checkpoint might be saved with the starting context and the first task in the queue. Worker A Takes Task 1: Worker A loads the checkpoint for \"video_abc\". It gets the context and sees Task 1 is next. It processes Task 1. Worker A Finishes Task 1: Let's say Task 1 updated the context (e.g., incremented a counter) and produced Task 2 and Task 3. Worker A Saves Checkpoint: The Floxide engine (running within Worker A) creates a new Checkpoint containing: The updated context. The remaining work queue (now containing Task 2 and Task 3). It calls store.save(\"video_abc\", new_checkpoint) . The old checkpoint is overwritten. Crash! (Optional): Worker A crashes immediately after saving. No work is lost because the progress is saved! Worker B Takes Task 2: Some time later, Worker B (maybe on a different machine) dequeues Task 2 from the main WorkQueue . Worker B Loads Checkpoint: Before processing Task 2, Worker B calls store.load(\"video_abc\") . It receives the checkpoint saved by Worker A in step 4. Worker B Resumes: Worker B now has the correct, updated context (with the incremented counter) and knows that Task 3 is also pending (from the checkpoint's queue). It proceeds to execute Task 2. sequenceDiagram participant WorkerA as Worker A participant Store as Checkpoint Store participant WorkerB as Worker B participant Engine as Floxide Engine Note over WorkerA, Store: Run \"video_abc\", Worker A processes Task 1 WorkerA->>Engine: Finish Task 1 (Context updated, Task 2 & 3 next) Engine->>Engine: Create Checkpoint { updated_context, queue:[Task2, Task3] } Engine->>Store: save(\"video_abc\", checkpoint) Store-->>Engine: Ok Note over WorkerA, WorkerB: Worker A might crash here - state is safe! Note over WorkerB, Store: Later, Worker B needs to process Task 2 for \"video_abc\" WorkerB->>Store: load(\"video_abc\") Store-->>WorkerB: Return Checkpoint { updated_context, queue:[Task2, Task3] } WorkerB->>Engine: Restore context, know Task 3 is pending Engine->>WorkerB: Execute Task 2 with updated_context This save/load cycle ensures that state is consistently passed between steps, even across different workers or process restarts.","title":"How Checkpointing Enables Fault-Tolerant Workflows (Local/Manual Context)"},{"location":"floxide-tutorial/06__checkpoint_____checkpointstore__trait_/#example-inmemorycheckpointstore","text":"Floxide provides a basic InMemoryCheckpointStore for testing and simple cases. It uses a standard Rust HashMap protected by a RwLock (allowing multiple readers or one writer) to store checkpoints in memory. // Simplified from crates/floxide-core/src/checkpoint.rs use std :: collections :: HashMap ; use std :: sync ::{ Arc , RwLock }; // For thread-safe sharing // ... other imports #[derive(Clone)] pub struct InMemoryCheckpointStore < C : Context , WI : WorkItem > { // Arc allows sharing ownership across threads/tasks // RwLock allows multiple readers or one writer at a time inner : Arc < RwLock < HashMap < String , Checkpoint < C , WI >>>> , // PhantomData might be needed depending on generic usage _phantom : std :: marker :: PhantomData < ( C , WI ) > , } // --- Implementation of the CheckpointStore trait --- #[async_trait] impl < C : Context , WI : WorkItem > CheckpointStore < C , WI > for InMemoryCheckpointStore < C , WI > { async fn save ( & self , workflow_id : & str , checkpoint : & Checkpoint < C , WI > , ) -> Result < (), CheckpointError > { // Get write access to the map (blocks other writers/readers) let mut map = self . inner . write (). unwrap (); // .unwrap() simplifies error handling here // Insert a clone of the checkpoint into the map map . insert ( workflow_id . to_string (), checkpoint . clone ()); Ok (()) // Lock is released automatically when 'map' goes out of scope } async fn load ( & self , workflow_id : & str ) -> Result < Option < Checkpoint < C , WI >> , CheckpointError > { // Get read access to the map (blocks writers, allows other readers) let map = self . inner . read (). unwrap (); // Get the checkpoint, clone it if found let maybe_checkpoint = map . get ( workflow_id ). map ( | ck | ck . clone ()); Ok ( maybe_checkpoint ) // Lock is released automatically when 'map' goes out of scope } } Arc<RwLock<HashMap<...>>> : The standard Rust way to share mutable data safely across asynchronous tasks. inner.write().unwrap() : Acquires a write lock on the map. inner.read().unwrap() : Acquires a read lock. map.insert(...) / map.get(...) : Standard HashMap operations. .clone() : We clone the Checkpoint when saving and loading to avoid ownership issues with the map. This requires C and WI to be Clone . This implementation is simple but effective for single-process scenarios. For real distributed systems, you'd use an implementation backed by Redis, a database, or another shared storage mechanism, likely implementing the more granular distributed store traits ( ContextStore , etc.).","title":"Example: InMemoryCheckpointStore"},{"location":"floxide-tutorial/06__checkpoint_____checkpointstore__trait_/#under-the-hood-serialization-is-key","text":"How does the CheckpointStore actually save your custom Context data ( C ) and the WorkItem ( WI )? It relies on serialization . Serialization: Converting the in-memory Rust structs ( Checkpoint , your Context , your WorkItem ) into a format that can be stored or transmitted (like JSON, MessagePack, Protobuf, etc.). Deserialization: Converting the stored format back into the Rust structs. Floxide leverages the powerful serde library for this. Remember in Chapter 3 , we required your Context struct to derive Serialize and Deserialize ? And the workflow! macro automatically derives them for the WorkItem enum it generates. The CheckpointStore implementation (like InMemoryCheckpointStore , or a hypothetical RedisCheckpointStore ) is responsible for: 1. During save : Taking the Checkpoint<C, WI> struct and using serde (e.g., serde_json::to_string or serde_json::to_vec ) to turn it into a byte array or string. Then, storing those bytes/string in its backend (e.g., the in-memory HashMap , a Redis key, a database BLOB). 2. During load : Retrieving the stored bytes/string from the backend. Then, using serde (e.g., serde_json::from_slice or serde_json::from_str ) to parse it back into a Checkpoint<C, WI> struct. This serialization is what allows complex Rust data structures representing your workflow's state to be saved, shared across machines, and loaded back reliably.","title":"Under the Hood: Serialization is Key"},{"location":"floxide-tutorial/06__checkpoint_____checkpointstore__trait_/#conclusion","text":"Checkpoints are the \"save game\" mechanism for Floxide workflows. A Checkpoint is a snapshot containing the current shared Context and the list of pending WorkItem s for a specific run, primarily used for local/manual persistence. The CheckpointStore trait defines the standard interface ( save , load ) for persisting and retrieving these snapshots. Implementations of CheckpointStore (like InMemoryCheckpointStore ) handle the actual storage and rely on serde for serialization. Checkpointing is crucial for fault tolerance in single-process or manually managed runs. For distributed execution , Floxide uses a different set of stores ( ContextStore , WorkQueue , etc.) for better scalability and separation of concerns. Now that we have the core pieces for basic execution and local persistence, let's explore how Floxide manages execution in a fully distributed environment. Next: Chapter 7: DistributedWorker (Focuses on the worker component which uses the distributed stores) See Also: Chapter 9: Distributed Stores (Details the stores like ContextStore used by workers/orchestrators)","title":"Conclusion"},{"location":"floxide-tutorial/07__distributedworker__/","text":"Chapter 7: DistributedWorker \u00b6 In the previous chapter , we learned about Checkpoint and CheckpointStore , the \"save game\" system for Floxide workflows. This allows us to save the state of a workflow run (its shared Context and pending tasks) so we can resume later or recover from crashes. But who actually does the work in a distributed setup? We have the blueprint ( Workflow ), the task list ( WorkQueue , and the save system ( CheckpointStore ). Now we need the actual employees who run the assembly line stations. What's the Problem? Actively Processing Tasks \u00b6 Imagine our distributed video processing factory. We have: * A list of jobs (the WorkQueue ). * Instructions for each job (the Node logic within the Workflow blueprint). * A way to save shared project state (the ContextStore ). * Other ledgers for tracking progress, errors, etc. (the other Distributed Stores ). But nothing happens on its own! We need programs \u2013 workers \u2013 that actively: 1. Look at the job list. 2. Pick up the next available job. 3. Figure out the current state of the project (load the latest shared Context using the ContextStore ). 4. Do the work for that job step. 5. Save the updated project state. 6. Potentially add new follow-up jobs to the list. 7. Go back to step 1 and repeat. This tireless entity is what Floxide calls a DistributedWorker . What is a DistributedWorker ? The Engine's Employee \u00b6 A DistributedWorker is an independent process or task whose main job is to execute workflow steps in a distributed Floxide setup. It's like an employee who performs the core work cycle: Check Queue: Repeatedly asks the shared WorkQueue : \"Any jobs available?\" Get Task: If a task ( WorkItem ) is available, the worker takes it. Load State: Reads the latest shared Context for the specific workflow run this task belongs to using the ContextStore . Execute Step: Runs the processing logic defined by the corresponding Node (using the Workflow definition to find the right code). The node might append events to the context it received. Handle Outcome: Based on the Transition returned by the Node: Enqueues new tasks onto the WorkQueue for subsequent steps. Updates the status of the current work item in the WorkItemStateStore . Save State: Merges the changes made to the context (the appended events) with the previously loaded state and saves the result back to the ContextStore using its merge capability. Also updates metrics ( MetricsStore ) and potentially logs errors ( ErrorStore ). Repeat: Goes back to checking the queue for the next job. Distributed Emphasis: The magic happens when you run multiple DistributedWorker instances. They can run on the same machine or, more importantly, on different machines . They all connect to the same shared WorkQueue and distributed stores ( ContextStore , ErrorStore , etc.). This allows them to work together in parallel, processing different tasks for different workflow runs (or even different tasks for the same run if a step used Transition::NextAll ). This parallel, distributed processing is Floxide's core strength. graph TD subgraph Shared Infrastructure WQ[(Work Queue)] CS[(Context Store)] ES[(Error Store)] MS[(Metrics Store)] LS[(Liveness Store)] WIS[(WorkItemState Store)] RIS[(RunInfo Store)] end subgraph Machine 1 W1[Worker 1] end subgraph Machine 2 W2[Worker 2] W3[Worker 3] end %% Worker Interactions (Same pattern for all) W1 -- \"Get/Put Task\" --> WQ W1 -- \"Load/Save State\" --> CS W1 -- \"Process Step\" --> W1 W2 -- \"Get/Put Task\" --> WQ W2 -- \"Load/Save State\" --> CS W2 -- \"Process Step\" --> W2 W3 -- \"Get/Put Task\" --> WQ W3 -- \"Load/Save State\" --> CS W3 -- \"Process Step\" --> W3 How a DistributedWorker Uses Floxide Components \u00b6 The DistributedWorker doesn't do everything from scratch. It relies heavily on the other abstractions we've learned about: Workflow : The worker needs the workflow definition to know which Node logic to execute for a given WorkItem and how to handle transitions based on the defined edges . It primarily uses the step_distributed method provided by the Workflow trait implementation (generated by the workflow! macro). WorkQueue : The worker constantly interacts with the queue to dequeue tasks to process and enqueue subsequent tasks. ContextStore : Before executing a step, the worker get s the current context state. After successful execution, it merge s the changes (appended events) back into the store. Other Distributed Stores (Chapter 9) : Workers also interact with these stores to report their status ( LivenessStore ), record errors ( ErrorStore ), update metrics ( MetricsStore ), track run status ( RunInfoStore ), and manage the state of individual work items ( WorkItemStateStore ). This provides observability and control. RetryPolicy (Chapter 10) : The worker can be configured with a retry policy to automatically handle transient errors during step execution. The Worker Loop in Action (Simplified) \u00b6 Let's walk through a single cycle for one worker (Worker #42): Dequeue: Worker #42 calls queue.dequeue() . Result: It receives Some((\"video_abc\", WorkItem::ExtractAudio(\"chunk_3.mp4\"))) . (It got a job!) Load Context: Worker #42 calls context_store.get(\"video_abc\") . It gets back the latest MyWorkflowData context (perhaps with an event log indicating processed_chunks: 2 ). Execute Step: The worker looks at WorkItem::ExtractAudio . Using the Workflow definition, it finds the ExtractAudioNode logic. It calls the process method of that Node, passing the loaded Context (via WorkflowCtx ) and the input \"chunk_3.mp4\" . Node Returns & Appends Event: The ExtractAudioNode finishes and returns Ok(Transition::Next(\"chunk_3.aac\")) . Internally, it also called ctx.store.event_log.append(MyWorkflowEvent::AudioExtracted(...)) . Handle Transition: The worker sees Transition::Next . It checks the Workflow 's edges for ExtractAudioNode . Let's say the edge points to GenerateSubtitlesNode . Enqueue Next: The worker creates WorkItem::GenerateSubtitles(\"chunk_3.aac\") (using the output from step 5) and calls queue.enqueue(\"video_abc\", new_work_item) . Merge & Save Context: The worker takes the context returned from the node execution (containing the new AudioExtracted event) and calls context_store.merge(\"video_abc\", context_with_new_event) . The ContextStore implementation uses the Merge trait on MyWorkflowData (and EventLog ) to combine this correctly with any other concurrent changes. Update Other Stores: The worker might also call metrics_store.update_metrics(...) , work_item_state_store.set_status(...) , etc. Loop: Worker #42 goes back to step 1, ready for the next job. If in step 2, queue.dequeue() returned None , the worker would typically wait for a short period and then try again. Using DistributedWorker \u00b6 You typically don't interact with the DistributedWorker struct directly in your Node logic. Instead, you configure and run it as a separate process or task. Floxide provides helpers to build and run workers. First, you need instances of your workflow definition and all the required store implementations (using appropriate distributed backends like Redis or Kafka, not just in-memory ones for real distribution). use floxide ::{ Workflow , DistributedWorker , WorkQueue , ContextStore , Context , // Import other store traits: RunInfoStore, MetricsStore, ErrorStore, LivenessStore, WorkItemStateStore // Import your specific workflow, context, and store implementations // e.g., TextProcessor, SimpleContext, RedisWorkQueue, RedisContextStore etc. }; use std :: sync :: Arc ; // Assume these are properly configured instances for distributed use // let my_workflow: TextProcessor = // ... initialized workflow struct // let my_queue: RedisWorkQueue<...> = // ... connected queue // let my_context_store: RedisContextStore<...> = // ... connected context store // let my_run_info_store: RedisRunInfoStore = // ... // ... etc for all stores ... // Create a worker instance using the builder pattern let worker = DistributedWorker :: builder () . workflow ( my_workflow ) . queue ( my_queue ) . context_store ( my_context_store ) . run_info_store ( my_run_info_store ) . metrics_store ( my_metrics_store ) . error_store ( my_error_store ) . liveness_store ( my_liveness_store ) . work_item_state_store ( my_work_item_state_store ) . build (); // Optional: Configure retry policy // worker.set_retry_policy(RetryPolicy::default()); // Define a unique ID for this worker instance (e.g., from hostname/PID) let worker_id = 42 ; // Or generate dynamically This code sets up a DistributedWorker instance, providing it with everything it needs to operate: the workflow logic, the queue, and various state stores. Then, you typically run the worker's main loop in an async task: // This would usually be run within an async runtime like Tokio // Clone the worker if needed (it's designed to be Clone-able) let worker_clone = worker . clone (); // Spawn an async task to run the worker loop forever tokio :: spawn ( async move { // run_forever loops indefinitely, processing tasks as they appear worker_clone . run_forever ( worker_id ). await ; // Note: run_forever technically never returns Ok, it runs until cancelled or panics. }); println! ( \"Worker {} started and polling for tasks...\" , worker_id ); // Keep the main program alive, or manage worker tasks (e.g., using WorkerPool) // ... This code starts the worker. The run_forever method contains the core loop described earlier (dequeue, load, process, save, enqueue). You would typically launch many such worker tasks, potentially across multiple machines, all configured with the same shared stores and workflow definition. Under the Hood: The step_distributed Method \u00b6 The DistributedWorker itself doesn't contain the complex logic for loading, executing, saving, and enqueuing based on transitions. It delegates most of this to the Workflow trait's step_distributed method (which is implemented by the code generated by the workflow! macro ). The worker.run_once(worker_id) method essentially does this: Calls queue.dequeue() . If a work_item and run_id are received: Updates worker status (liveness, work item state) using the provided stores. Calls self.workflow.step_distributed(...) , passing the context_store , queue , run_id , work_item , and potentially other stores needed for state updates within the step execution logic (like metrics/error stores). The step_distributed implementation (generated by workflow! ) handles: Loading the context ( context_store.get(run_id) ). Calling the correct Node's process method (using process_work_item ). Node appends events. Handling the Transition and enqueuing next items ( queue.enqueue(...) ). Merging and saving the context ( context_store.merge(run_id, ...) ). Updates worker status based on the result of step_distributed . If no item is dequeued, it indicates idleness. Here's a simplified sequence diagram of run_once : sequenceDiagram participant Worker as DistributedWorker (run_once) participant WQ as WorkQueue participant WorkflowImpl as Workflow (step_distributed) participant CtxStore as ContextStore participant Stores as Other Stores (Liveness, etc.) Worker->>WQ: dequeue() alt Task Found (run_id, item) WQ-->>Worker: Return Some((run_id, item)) Worker->>Stores: Update status (starting item) Worker->>WorkflowImpl: step_distributed(ctx_store, queue, run_id, item, ...) Note right of WorkflowImpl: Loads Context (CtxStore), <br/>Executes Node (appends events),<br/>Merges/Saves Context (CtxStore),<br/>Enqueues Next Tasks (WQ) WorkflowImpl-->>Worker: Return Result (Ok(None) or Ok(Some(output)) or Err) Worker->>Stores: Update status (item finished/failed) Worker-->>Worker: Return result to caller else No Task Found WQ-->>Worker: Return None Worker->>Stores: Update status (idle) Worker-->>Worker: Return Ok(None) end Let's look at simplified code from floxide-core/src/distributed/worker.rs : The DistributedWorker struct holds all the necessary components: // Simplified from crates/floxide-core/src/distributed/worker.rs use crate :: distributed ::{ ContextStore , ErrorStore , LivenessStore , MetricsStore , RunInfoStore , WorkItemStateStore , WorkQueue }; pub struct DistributedWorker < W , C , Q , CS , RIS , MS , ES , LS , WIS > where W : Workflow < C , WorkItem : ' static > , C : Context , Q : WorkQueue < C , W :: WorkItem > , CS : ContextStore < C > , RIS : RunInfoStore , MS : MetricsStore , ES : ErrorStore , LS : LivenessStore , WIS : WorkItemStateStore < W :: WorkItem > , { workflow : W , queue : Q , context_store : CS , run_info_store : RIS , metrics_store : MS , error_store : ES , liveness_store : LS , work_item_state_store : WIS , retry_policy : Option < RetryPolicy > , phantom : std :: marker :: PhantomData < C > , } The run_forever method loops, calling run_once : // Simplified from crates/floxide-core/src/distributed/worker.rs impl <.. . > DistributedWorker <.. . > { pub async fn run_forever ( & self , worker_id : usize ) -> std :: convert :: Infallible { loop { match self . run_once ( worker_id ). await { Ok ( Some (( _run_id , _output ))) => { // Work was done, loop immediately for more } Ok ( None ) => { // No work found, sleep briefly sleep ( Duration :: from_millis ( 100 )). await ; } Err ( e ) => { // Error occurred, log and sleep error ! ( worker_id , error = ? e , \"Worker error\" ); sleep ( Duration :: from_millis ( 100 )). await ; } } } } } And run_once orchestrates the call to the workflow's core distributed step logic: // Simplified CONCEPT from crates/floxide-core/src/distributed/worker.rs // The actual implementation uses callbacks for state updates. impl <.. . > DistributedWorker <.. . > { pub async fn run_once ( & self , worker_id : usize ) -> Result < Option < ( String , W :: Output ) > , FloxideError > { // Update liveness/heartbeat (simplified) self . heartbeat ( worker_id ). await ; // *** Core Logic: Delegate to Workflow::step_distributed *** // This method encapsulates: dequeue, load, process, save, enqueue match self . workflow . step_distributed ( & self . context_store , & self . queue , worker_id , self . build_callbacks ( worker_id ) // Provides hooks for state updates ). await { Ok ( Some (( run_id , output ))) => { // Workflow run completed! // Update status to idle self . on_idle_state_updates ( worker_id ). await ? ; Ok ( Some (( run_id , output ))) } Ok ( None ) => { // Step processed, but workflow not finished OR queue was empty // Update status to idle self . on_idle_state_updates ( worker_id ). await ? ; Ok ( None ) } Err ( step_error ) => { // Handle step error (potentially using retry policy logic) // Update status to idle or failed self . on_idle_state_updates ( worker_id ). await ? ; Err ( step_error . error ) // Return the underlying FloxideError } } } // Helper to create callback object fn build_callbacks ( & self , worker_id : usize ) -> Arc < dyn StepCallbacks < C , W >> { // ... creates an object that updates stores on events ... } // Helpers for state updates via callbacks (simplified) async fn on_idle_state_updates ( & self , worker_id : usize ) -> Result < (), FloxideError > ; async fn heartbeat ( & self , worker_id : usize ); // ... other state update helpers for start, success, error ... } The key takeaway is that the DistributedWorker acts as the runner or host process, performing the loop and calling the appropriate methods on the Workflow , WorkQueue , and ContextStore to execute the distributed steps defined by your application. Worker Pools \u00b6 Running and managing individual worker tasks can be tedious. Floxide often provides a WorkerPool utility (shown in the provided context code) that simplifies starting, stopping, and monitoring multiple DistributedWorker instances concurrently. Conclusion \u00b6 The DistributedWorker is the active entity in a Floxide distributed workflow. It's the \"employee\" that continuously: * Pulls tasks from the shared WorkQueue . * Loads the necessary state from the ContextStore . * Executes the Node logic defined in the Workflow (via step_distributed ). * Saves the updated state back to the ContextStore . * Enqueues follow-up tasks. By running multiple workers, potentially across many machines, all interacting with the same shared queue and stores, Floxide achieves parallel and distributed workflow execution. But how do we start a new workflow run in this distributed environment? We can't just call my_workflow.run() anymore. We need a way to kick things off, create the initial checkpoint, and put the very first task onto the queue for the workers to find. That's the job of the DistributedOrchestrator . Next: Chapter 8: DistributedOrchestrator","title":"Chapter 7: DistributedWorker"},{"location":"floxide-tutorial/07__distributedworker__/#chapter-7-distributedworker","text":"In the previous chapter , we learned about Checkpoint and CheckpointStore , the \"save game\" system for Floxide workflows. This allows us to save the state of a workflow run (its shared Context and pending tasks) so we can resume later or recover from crashes. But who actually does the work in a distributed setup? We have the blueprint ( Workflow ), the task list ( WorkQueue , and the save system ( CheckpointStore ). Now we need the actual employees who run the assembly line stations.","title":"Chapter 7: DistributedWorker"},{"location":"floxide-tutorial/07__distributedworker__/#whats-the-problem-actively-processing-tasks","text":"Imagine our distributed video processing factory. We have: * A list of jobs (the WorkQueue ). * Instructions for each job (the Node logic within the Workflow blueprint). * A way to save shared project state (the ContextStore ). * Other ledgers for tracking progress, errors, etc. (the other Distributed Stores ). But nothing happens on its own! We need programs \u2013 workers \u2013 that actively: 1. Look at the job list. 2. Pick up the next available job. 3. Figure out the current state of the project (load the latest shared Context using the ContextStore ). 4. Do the work for that job step. 5. Save the updated project state. 6. Potentially add new follow-up jobs to the list. 7. Go back to step 1 and repeat. This tireless entity is what Floxide calls a DistributedWorker .","title":"What's the Problem? Actively Processing Tasks"},{"location":"floxide-tutorial/07__distributedworker__/#what-is-a-distributedworker-the-engines-employee","text":"A DistributedWorker is an independent process or task whose main job is to execute workflow steps in a distributed Floxide setup. It's like an employee who performs the core work cycle: Check Queue: Repeatedly asks the shared WorkQueue : \"Any jobs available?\" Get Task: If a task ( WorkItem ) is available, the worker takes it. Load State: Reads the latest shared Context for the specific workflow run this task belongs to using the ContextStore . Execute Step: Runs the processing logic defined by the corresponding Node (using the Workflow definition to find the right code). The node might append events to the context it received. Handle Outcome: Based on the Transition returned by the Node: Enqueues new tasks onto the WorkQueue for subsequent steps. Updates the status of the current work item in the WorkItemStateStore . Save State: Merges the changes made to the context (the appended events) with the previously loaded state and saves the result back to the ContextStore using its merge capability. Also updates metrics ( MetricsStore ) and potentially logs errors ( ErrorStore ). Repeat: Goes back to checking the queue for the next job. Distributed Emphasis: The magic happens when you run multiple DistributedWorker instances. They can run on the same machine or, more importantly, on different machines . They all connect to the same shared WorkQueue and distributed stores ( ContextStore , ErrorStore , etc.). This allows them to work together in parallel, processing different tasks for different workflow runs (or even different tasks for the same run if a step used Transition::NextAll ). This parallel, distributed processing is Floxide's core strength. graph TD subgraph Shared Infrastructure WQ[(Work Queue)] CS[(Context Store)] ES[(Error Store)] MS[(Metrics Store)] LS[(Liveness Store)] WIS[(WorkItemState Store)] RIS[(RunInfo Store)] end subgraph Machine 1 W1[Worker 1] end subgraph Machine 2 W2[Worker 2] W3[Worker 3] end %% Worker Interactions (Same pattern for all) W1 -- \"Get/Put Task\" --> WQ W1 -- \"Load/Save State\" --> CS W1 -- \"Process Step\" --> W1 W2 -- \"Get/Put Task\" --> WQ W2 -- \"Load/Save State\" --> CS W2 -- \"Process Step\" --> W2 W3 -- \"Get/Put Task\" --> WQ W3 -- \"Load/Save State\" --> CS W3 -- \"Process Step\" --> W3","title":"What is a DistributedWorker? The Engine's Employee"},{"location":"floxide-tutorial/07__distributedworker__/#how-a-distributedworker-uses-floxide-components","text":"The DistributedWorker doesn't do everything from scratch. It relies heavily on the other abstractions we've learned about: Workflow : The worker needs the workflow definition to know which Node logic to execute for a given WorkItem and how to handle transitions based on the defined edges . It primarily uses the step_distributed method provided by the Workflow trait implementation (generated by the workflow! macro). WorkQueue : The worker constantly interacts with the queue to dequeue tasks to process and enqueue subsequent tasks. ContextStore : Before executing a step, the worker get s the current context state. After successful execution, it merge s the changes (appended events) back into the store. Other Distributed Stores (Chapter 9) : Workers also interact with these stores to report their status ( LivenessStore ), record errors ( ErrorStore ), update metrics ( MetricsStore ), track run status ( RunInfoStore ), and manage the state of individual work items ( WorkItemStateStore ). This provides observability and control. RetryPolicy (Chapter 10) : The worker can be configured with a retry policy to automatically handle transient errors during step execution.","title":"How a DistributedWorker Uses Floxide Components"},{"location":"floxide-tutorial/07__distributedworker__/#the-worker-loop-in-action-simplified","text":"Let's walk through a single cycle for one worker (Worker #42): Dequeue: Worker #42 calls queue.dequeue() . Result: It receives Some((\"video_abc\", WorkItem::ExtractAudio(\"chunk_3.mp4\"))) . (It got a job!) Load Context: Worker #42 calls context_store.get(\"video_abc\") . It gets back the latest MyWorkflowData context (perhaps with an event log indicating processed_chunks: 2 ). Execute Step: The worker looks at WorkItem::ExtractAudio . Using the Workflow definition, it finds the ExtractAudioNode logic. It calls the process method of that Node, passing the loaded Context (via WorkflowCtx ) and the input \"chunk_3.mp4\" . Node Returns & Appends Event: The ExtractAudioNode finishes and returns Ok(Transition::Next(\"chunk_3.aac\")) . Internally, it also called ctx.store.event_log.append(MyWorkflowEvent::AudioExtracted(...)) . Handle Transition: The worker sees Transition::Next . It checks the Workflow 's edges for ExtractAudioNode . Let's say the edge points to GenerateSubtitlesNode . Enqueue Next: The worker creates WorkItem::GenerateSubtitles(\"chunk_3.aac\") (using the output from step 5) and calls queue.enqueue(\"video_abc\", new_work_item) . Merge & Save Context: The worker takes the context returned from the node execution (containing the new AudioExtracted event) and calls context_store.merge(\"video_abc\", context_with_new_event) . The ContextStore implementation uses the Merge trait on MyWorkflowData (and EventLog ) to combine this correctly with any other concurrent changes. Update Other Stores: The worker might also call metrics_store.update_metrics(...) , work_item_state_store.set_status(...) , etc. Loop: Worker #42 goes back to step 1, ready for the next job. If in step 2, queue.dequeue() returned None , the worker would typically wait for a short period and then try again.","title":"The Worker Loop in Action (Simplified)"},{"location":"floxide-tutorial/07__distributedworker__/#using-distributedworker","text":"You typically don't interact with the DistributedWorker struct directly in your Node logic. Instead, you configure and run it as a separate process or task. Floxide provides helpers to build and run workers. First, you need instances of your workflow definition and all the required store implementations (using appropriate distributed backends like Redis or Kafka, not just in-memory ones for real distribution). use floxide ::{ Workflow , DistributedWorker , WorkQueue , ContextStore , Context , // Import other store traits: RunInfoStore, MetricsStore, ErrorStore, LivenessStore, WorkItemStateStore // Import your specific workflow, context, and store implementations // e.g., TextProcessor, SimpleContext, RedisWorkQueue, RedisContextStore etc. }; use std :: sync :: Arc ; // Assume these are properly configured instances for distributed use // let my_workflow: TextProcessor = // ... initialized workflow struct // let my_queue: RedisWorkQueue<...> = // ... connected queue // let my_context_store: RedisContextStore<...> = // ... connected context store // let my_run_info_store: RedisRunInfoStore = // ... // ... etc for all stores ... // Create a worker instance using the builder pattern let worker = DistributedWorker :: builder () . workflow ( my_workflow ) . queue ( my_queue ) . context_store ( my_context_store ) . run_info_store ( my_run_info_store ) . metrics_store ( my_metrics_store ) . error_store ( my_error_store ) . liveness_store ( my_liveness_store ) . work_item_state_store ( my_work_item_state_store ) . build (); // Optional: Configure retry policy // worker.set_retry_policy(RetryPolicy::default()); // Define a unique ID for this worker instance (e.g., from hostname/PID) let worker_id = 42 ; // Or generate dynamically This code sets up a DistributedWorker instance, providing it with everything it needs to operate: the workflow logic, the queue, and various state stores. Then, you typically run the worker's main loop in an async task: // This would usually be run within an async runtime like Tokio // Clone the worker if needed (it's designed to be Clone-able) let worker_clone = worker . clone (); // Spawn an async task to run the worker loop forever tokio :: spawn ( async move { // run_forever loops indefinitely, processing tasks as they appear worker_clone . run_forever ( worker_id ). await ; // Note: run_forever technically never returns Ok, it runs until cancelled or panics. }); println! ( \"Worker {} started and polling for tasks...\" , worker_id ); // Keep the main program alive, or manage worker tasks (e.g., using WorkerPool) // ... This code starts the worker. The run_forever method contains the core loop described earlier (dequeue, load, process, save, enqueue). You would typically launch many such worker tasks, potentially across multiple machines, all configured with the same shared stores and workflow definition.","title":"Using DistributedWorker"},{"location":"floxide-tutorial/07__distributedworker__/#under-the-hood-the-step_distributed-method","text":"The DistributedWorker itself doesn't contain the complex logic for loading, executing, saving, and enqueuing based on transitions. It delegates most of this to the Workflow trait's step_distributed method (which is implemented by the code generated by the workflow! macro ). The worker.run_once(worker_id) method essentially does this: Calls queue.dequeue() . If a work_item and run_id are received: Updates worker status (liveness, work item state) using the provided stores. Calls self.workflow.step_distributed(...) , passing the context_store , queue , run_id , work_item , and potentially other stores needed for state updates within the step execution logic (like metrics/error stores). The step_distributed implementation (generated by workflow! ) handles: Loading the context ( context_store.get(run_id) ). Calling the correct Node's process method (using process_work_item ). Node appends events. Handling the Transition and enqueuing next items ( queue.enqueue(...) ). Merging and saving the context ( context_store.merge(run_id, ...) ). Updates worker status based on the result of step_distributed . If no item is dequeued, it indicates idleness. Here's a simplified sequence diagram of run_once : sequenceDiagram participant Worker as DistributedWorker (run_once) participant WQ as WorkQueue participant WorkflowImpl as Workflow (step_distributed) participant CtxStore as ContextStore participant Stores as Other Stores (Liveness, etc.) Worker->>WQ: dequeue() alt Task Found (run_id, item) WQ-->>Worker: Return Some((run_id, item)) Worker->>Stores: Update status (starting item) Worker->>WorkflowImpl: step_distributed(ctx_store, queue, run_id, item, ...) Note right of WorkflowImpl: Loads Context (CtxStore), <br/>Executes Node (appends events),<br/>Merges/Saves Context (CtxStore),<br/>Enqueues Next Tasks (WQ) WorkflowImpl-->>Worker: Return Result (Ok(None) or Ok(Some(output)) or Err) Worker->>Stores: Update status (item finished/failed) Worker-->>Worker: Return result to caller else No Task Found WQ-->>Worker: Return None Worker->>Stores: Update status (idle) Worker-->>Worker: Return Ok(None) end Let's look at simplified code from floxide-core/src/distributed/worker.rs : The DistributedWorker struct holds all the necessary components: // Simplified from crates/floxide-core/src/distributed/worker.rs use crate :: distributed ::{ ContextStore , ErrorStore , LivenessStore , MetricsStore , RunInfoStore , WorkItemStateStore , WorkQueue }; pub struct DistributedWorker < W , C , Q , CS , RIS , MS , ES , LS , WIS > where W : Workflow < C , WorkItem : ' static > , C : Context , Q : WorkQueue < C , W :: WorkItem > , CS : ContextStore < C > , RIS : RunInfoStore , MS : MetricsStore , ES : ErrorStore , LS : LivenessStore , WIS : WorkItemStateStore < W :: WorkItem > , { workflow : W , queue : Q , context_store : CS , run_info_store : RIS , metrics_store : MS , error_store : ES , liveness_store : LS , work_item_state_store : WIS , retry_policy : Option < RetryPolicy > , phantom : std :: marker :: PhantomData < C > , } The run_forever method loops, calling run_once : // Simplified from crates/floxide-core/src/distributed/worker.rs impl <.. . > DistributedWorker <.. . > { pub async fn run_forever ( & self , worker_id : usize ) -> std :: convert :: Infallible { loop { match self . run_once ( worker_id ). await { Ok ( Some (( _run_id , _output ))) => { // Work was done, loop immediately for more } Ok ( None ) => { // No work found, sleep briefly sleep ( Duration :: from_millis ( 100 )). await ; } Err ( e ) => { // Error occurred, log and sleep error ! ( worker_id , error = ? e , \"Worker error\" ); sleep ( Duration :: from_millis ( 100 )). await ; } } } } } And run_once orchestrates the call to the workflow's core distributed step logic: // Simplified CONCEPT from crates/floxide-core/src/distributed/worker.rs // The actual implementation uses callbacks for state updates. impl <.. . > DistributedWorker <.. . > { pub async fn run_once ( & self , worker_id : usize ) -> Result < Option < ( String , W :: Output ) > , FloxideError > { // Update liveness/heartbeat (simplified) self . heartbeat ( worker_id ). await ; // *** Core Logic: Delegate to Workflow::step_distributed *** // This method encapsulates: dequeue, load, process, save, enqueue match self . workflow . step_distributed ( & self . context_store , & self . queue , worker_id , self . build_callbacks ( worker_id ) // Provides hooks for state updates ). await { Ok ( Some (( run_id , output ))) => { // Workflow run completed! // Update status to idle self . on_idle_state_updates ( worker_id ). await ? ; Ok ( Some (( run_id , output ))) } Ok ( None ) => { // Step processed, but workflow not finished OR queue was empty // Update status to idle self . on_idle_state_updates ( worker_id ). await ? ; Ok ( None ) } Err ( step_error ) => { // Handle step error (potentially using retry policy logic) // Update status to idle or failed self . on_idle_state_updates ( worker_id ). await ? ; Err ( step_error . error ) // Return the underlying FloxideError } } } // Helper to create callback object fn build_callbacks ( & self , worker_id : usize ) -> Arc < dyn StepCallbacks < C , W >> { // ... creates an object that updates stores on events ... } // Helpers for state updates via callbacks (simplified) async fn on_idle_state_updates ( & self , worker_id : usize ) -> Result < (), FloxideError > ; async fn heartbeat ( & self , worker_id : usize ); // ... other state update helpers for start, success, error ... } The key takeaway is that the DistributedWorker acts as the runner or host process, performing the loop and calling the appropriate methods on the Workflow , WorkQueue , and ContextStore to execute the distributed steps defined by your application.","title":"Under the Hood: The step_distributed Method"},{"location":"floxide-tutorial/07__distributedworker__/#worker-pools","text":"Running and managing individual worker tasks can be tedious. Floxide often provides a WorkerPool utility (shown in the provided context code) that simplifies starting, stopping, and monitoring multiple DistributedWorker instances concurrently.","title":"Worker Pools"},{"location":"floxide-tutorial/07__distributedworker__/#conclusion","text":"The DistributedWorker is the active entity in a Floxide distributed workflow. It's the \"employee\" that continuously: * Pulls tasks from the shared WorkQueue . * Loads the necessary state from the ContextStore . * Executes the Node logic defined in the Workflow (via step_distributed ). * Saves the updated state back to the ContextStore . * Enqueues follow-up tasks. By running multiple workers, potentially across many machines, all interacting with the same shared queue and stores, Floxide achieves parallel and distributed workflow execution. But how do we start a new workflow run in this distributed environment? We can't just call my_workflow.run() anymore. We need a way to kick things off, create the initial checkpoint, and put the very first task onto the queue for the workers to find. That's the job of the DistributedOrchestrator . Next: Chapter 8: DistributedOrchestrator","title":"Conclusion"},{"location":"floxide-tutorial/08__distributedorchestrator__/","text":"Chapter 8: DistributedOrchestrator \u00b6 In the previous chapter , we met the DistributedWorker . These are the diligent employees in our distributed workflow factory, constantly picking up tasks from the WorkQueue , processing them using the Workflow blueprint, and saving progress using the CheckpointStore . But wait... if workers just grab tasks from a queue, who puts the first task on the queue? How do we initiate a whole new workflow run? And how do we check on the progress of all these ongoing runs, or maybe stop one if needed? What's the Problem? Managing the Factory Floor \u00b6 Imagine our video processing factory again. We have workers ( DistributedWorker ) ready at their stations, a job board ( WorkQueue ) to list tasks, and a way to save progress ( CheckpointStore ). But we need someone in charge, a Project Manager , to: Start New Projects: Say \"Okay, let's start processing this new video, 'intro.mp4'. Here's the initial configuration.\" This involves setting up the initial shared Context in the ContextStore and putting the very first task (like \"Download Video\") on the job board ( WorkQueue ). Monitor Progress: Walk around the factory floor (figuratively!) and ask \"How's the processing for 'intro.mp4' going? Is it done yet? Did any problems occur?\" using the RunInfoStore , MetricsStore , ErrorStore , etc. Control Execution: Maybe decide \"Hold on, stop working on 'outro.mp4' for now,\" or \"Cancel the 'test_video.avi' project entirely.\" Gather Reports: Collect information about how many projects are running, which workers are active, and any errors that have happened. Without this central management figure, our distributed factory would be chaotic. Workers wouldn't know when new projects start, and we wouldn't know how they're progressing. In Floxide, this project manager role is filled by the DistributedOrchestrator . What is a DistributedOrchestrator ? Your Workflow Manager \u00b6 The DistributedOrchestrator is the central control panel for managing your distributed Floxide workflow runs. It doesn't execute the individual steps itself (that's the DistributedWorker 's job), but it provides the interface to interact with the entire distributed system at a higher level. Think of it as the dashboard and control levers for your workflow factory. Its main responsibilities include: Starting new workflow runs: This involves providing the initial Context , saving it to the ContextStore , and putting the very first WorkItem onto the WorkQueue . This \"seeds\" the workflow run, making it available for workers to pick up. Querying the status of runs: Asking \"Is run 'video_abc' still running, completed, failed, or paused?\" (using RunInfoStore ). Cancelling or pausing runs: Sending signals to stop or temporarily halt processing for a specific run (updating RunInfoStore and purging WorkQueue ). Retrieving run context: Fetching the latest shared context data for a run from the ContextStore . Retrieving run history and metrics: Getting information about past and present runs from RunInfoStore and MetricsStore . Checking for errors: Listing any errors that occurred during a run via ErrorStore . Monitoring worker liveness: Checking which workers are active and responsive. Distributed Emphasis: The DistributedOrchestrator achieves this by interacting with all the shared, distributed stores we've been learning about (and will learn more about in the next chapter ). It talks to the WorkQueue , ContextStore , RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore , etc., to get a unified view of the system and exert control. graph TD Orchestrator[Distributed Orchestrator] -- Manages --> Run1[Workflow Run 'video_abc'] Orchestrator -- Manages --> Run2[Workflow Run 'audio_xyz'] subgraph Shared Infrastructure WQ[(Work Queue)] CtxS[(Context Store)] RIS[(RunInfo Store)] MS[(Metrics Store)] ES[(Error Store)] LS[(Liveness Store)] WIS[(WorkItemState Store)] end Orchestrator -- \"start_run()\" --> WQ & CtxS & RIS Orchestrator -- \"status()\" --> RIS Orchestrator -- \"cancel()\" --> RIS & WQ Orchestrator -- \"errors()\" --> ES Orchestrator -- \"liveness()\" --> LS Orchestrator -- \"metrics()\" --> MS Orchestrator -- \"context()\" --> CtxS How to Use the DistributedOrchestrator \u00b6 You typically use the DistributedOrchestrator from your main application code, perhaps in response to a user request, an incoming API call, or a scheduled event. 1. Creating an Orchestrator Instance \u00b6 First, you need to create an instance of DistributedOrchestrator . This requires providing instances of your Workflow definition and all the necessary distributed store implementations. These stores must be implementations designed for distributed use (like Redis-backed stores, database stores, etc.) for the system to work across multiple machines. use floxide ::{ DistributedOrchestrator , WorkflowCtx , distributed ::{ OrchestratorBuilder , ContextStore /* other stores */ }, // --- Assume these are your types/implementations --- workflow :: TextProcessor , // Your workflow struct from workflow! macro context :: SimpleContext , // Your context struct // Concrete store implementations (using Redis as an example) RedisWorkQueue , RedisContextStore , RedisRunInfoStore , RedisMetricsStore , RedisErrorStore , RedisLivenessStore , RedisWorkItemStateStore , }; use std :: sync :: Arc ; // Assume these are fully configured clients connected to your backend services let my_workflow = TextProcessor { /* ... node instances ... */ }; let redis_queue = RedisWorkQueue :: new ( /* redis config */ ). await ? ; let redis_context = RedisContextStore :: new ( /* redis config */ ). await ? ; let redis_run_info = RedisRunInfoStore :: new ( /* redis config */ ). await ? ; let redis_metrics = RedisMetricsStore :: new ( /* redis config */ ). await ? ; let redis_errors = RedisErrorStore :: new ( /* redis config */ ). await ? ; let redis_liveness = RedisLivenessStore :: new ( /* redis config */ ). await ? ; let redis_work_item = RedisWorkItemStateStore :: new ( /* redis config */ ). await ? ; // Create the orchestrator instance using the builder let orchestrator = OrchestratorBuilder :: new () . workflow ( my_workflow ) . queue ( redis_queue ) . context_store ( redis_context ) . run_info_store ( redis_run_info ) . metrics_store ( redis_metrics ) . error_store ( redis_errors ) . liveness_store ( redis_liveness ) . work_item_state_store ( redis_work_item ) . build (); println! ( \"Orchestrator created successfully!\" ); This code gathers all the necessary components (the workflow logic and connections to all the shared backend stores) and bundles them into an orchestrator object using a convenient builder pattern. 2. Starting a Workflow Run \u00b6 The most common action is starting a new run. You provide the initial Context and the input data for the first Node . // Define the initial shared context data for this specific run let initial_context_data = SimpleContext {}; // Or MyWorkflowData { api_key: \"...\", ... } let wf_ctx = WorkflowCtx :: new ( initial_context_data ); // Define the input for the very first node in the workflow let initial_input = \"start processing this text\" . to_string (); // Call start_run on the orchestrator let run_id = orchestrator . start_run ( & wf_ctx , initial_input ). await ? ; println! ( \"Successfully started workflow run with ID: {}\" , run_id ); // Example Output: // Successfully started workflow run with ID: 1e7a3c4f-8b0a-4b1e-8c7f-9d2e1b0a3d5e What start_run Does: * Generates a unique ID (like 1e7a... ) for this specific run. * Calls the underlying workflow.start_distributed(...) method. This method: * Initializes the context state in the ContextStore using context_store.set(run_id, initial_context_data) . * Determines the first WorkItem (s) based on the start node and the initial_input . * Pushes the first WorkItem (s) onto the WorkQueue . * Records the new run's information (ID, status= Running , start time) in the RunInfoStore . * Returns the unique run_id . Crucially, start_run does not execute any workflow logic itself. It just sets everything up and puts the first task on the queue. The DistributedWorker s will eventually see this task on the queue and start processing it. 3. Checking Run Status \u00b6 You can easily check the status of a run using its ID. let run_id_to_check = \"1e7a3c4f-8b0a-4b1e-8c7f-9d2e1b0a3d5e\" ; // From previous step let status = orchestrator . status ( run_id_to_check ). await ? ; println! ( \"Status for run {}: {:?}\" , run_id_to_check , status ); // Example Output (could be Running, Completed, Failed, Paused, Cancelled): // Status for run 1e7a...: Running This queries the RunInfoStore to get the latest recorded status for that run_id . 4. Cancelling a Run \u00b6 If you need to stop a run prematurely: let run_id_to_cancel = \"1e7a3c4f-8b0a-4b1e-8c7f-9d2e1b0a3d5e\" ; orchestrator . cancel ( run_id_to_cancel ). await ? ; println! ( \"Requested cancellation for run {}\" , run_id_to_cancel ); // Later, checking status might show: // Status for run 1e7a...: Cancelled What cancel Does: * Updates the status in the RunInfoStore to Cancelled . * Calls queue.purge_run(run_id) to remove any pending tasks for this run from the WorkQueue . * (Note: Workers currently processing a task for this run might finish that task, but they won't pick up new ones. Nodes can also check ctx.is_cancelled() to stop early). 5. Other Management Functions \u00b6 The DistributedOrchestrator provides other useful methods: list_runs(filter) : Get a list of all runs, optionally filtering by status. pause(run_id) / resume(run_id) : Temporarily halt or continue a run. errors(run_id) : Retrieve a list of errors recorded for a run from the ErrorStore . metrics(run_id) : Get performance metrics (like step timings) from the MetricsStore . liveness() / list_worker_health() : Check the status of connected workers via the LivenessStore . pending_work(run_id) : See what tasks are currently waiting in the queue for a specific run. context(run_id) : Retrieve the latest saved context data for the run from the ContextStore . wait_for_completion(run_id, poll_interval) : Wait until a run finishes (Completed, Failed, or Cancelled). These methods allow comprehensive monitoring and control over your distributed workflows. Under the Hood: How start_run Works \u00b6 Let's trace the main steps when you call orchestrator.start_run(ctx, input) : Generate ID: The start_run method creates a unique run_id (e.g., using UUIDs). Delegate to Workflow: It calls self.workflow.start_distributed(ctx, input, &self.context_store, &self.queue, &run_id) . start_distributed (Inside workflow! Macro Code): Calls context_store.set(run_id, ctx.store) to save the initial context state. Determines the first WorkItem based on the workflow's start node and the provided input . Calls self.queue.enqueue(run_id, first_work_item) to put the first task on the main queue. Record Run Info: Back in start_run , it creates a RunInfo struct (status=Running, started_at=now) and calls self.run_info_store.insert_run(run_info) . Return ID: start_run returns the generated run_id . Here's a sequence diagram illustrating this: sequenceDiagram participant UserApp as Your Application participant Orch as DistributedOrchestrator participant WF as Workflow (start_distributed) participant CtxS as ContextStore participant WQ as WorkQueue participant RIS as RunInfoStore UserApp->>Orch: start_run(ctx, input) Orch->>Orch: Generate run_id Orch->>WF: start_distributed(ctx, input, context_store, queue, run_id) WF->>CtxS: set(run_id, ctx.store) CtxS-->>WF: Ok WF->>WF: Determine first_item WF->>WQ: enqueue(run_id, first_item) WQ-->>WF: Ok WF-->>Orch: Ok Orch->>RIS: insert_run(run_id, status=Running, ...) RIS-->>Orch: Ok Orch-->>UserApp: Return run_id The DistributedOrchestrator struct itself (defined in floxide-core/src/distributed/orchestrator.rs ) mainly holds references to the workflow and all the store implementations. // Simplified from crates/floxide-core/src/distributed/orchestrator.rs use crate :: distributed ::{ ContextStore , ErrorStore , LivenessStore , MetricsStore , RunInfoStore , WorkItemStateStore , WorkQueue }; // Generic struct holding all components pub struct DistributedOrchestrator < W , C , Q , CS , RIS , MS , ES , LS , WIS > where /* type bounds for Workflow, Context, and all Stores */ { workflow : W , queue : Q , context_store : CS , run_info_store : RIS , metrics_store : MS , error_store : ES , liveness_store : LS , work_item_state_store : WIS , // PhantomData used for generic type C phantom : std :: marker :: PhantomData < C > , } impl <.. . > DistributedOrchestrator <.. . > /* type bounds */ { // Constructor takes all components pub fn new ( workflow : W , queue : Q , context_store : CS , run_info_store : RIS , /* etc */ ) -> Self { /* ... store components ... */ } // start_run implementation pub async fn start_run ( & self , ctx : & WorkflowCtx < C > , input : W :: Input , ) -> Result < String , FloxideError > { // 1. Generate unique ID let run_id = uuid :: Uuid :: new_v4 (). to_string (); // 2. Delegate to workflow's seeding primitive self . workflow . start_distributed ( ctx , input , & self . context_store , & self . queue , & run_id ) . await ? ; // Handles context save + queue enqueue // 3. Record initial run info let run_info = RunInfo { /* run_id, status=Running, started_at */ }; self . run_info_store . insert_run ( run_info ). await ? ; // 4. Return the ID Ok ( run_id ) } // ... implementations for status(), cancel(), errors(), etc. ... // These mostly delegate calls to the corresponding store interfaces. } The code clearly shows start_run coordinating between generating an ID, calling the workflow's specific seeding logic ( start_distributed ), and updating the RunInfoStore . Conclusion \u00b6 The DistributedOrchestrator is the essential management interface for your distributed Floxide workflows. It acts as the central point of control, allowing you to: Initiate new workflow runs in a distributed environment using start_run . Monitor their progress ( status , metrics , errors , liveness ). Control their execution ( cancel , pause , resume ). It achieves this by interacting with the underlying Workflow definition and, crucially, the various shared distributed stores that maintain the state and coordination information for the entire system. Speaking of these stores, while we've mentioned them ( RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore ), let's take a closer look at what each one does in the next chapter. Next: Chapter 9: Distributed Stores ( RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore )","title":"Chapter 8: DistributedOrchestrator"},{"location":"floxide-tutorial/08__distributedorchestrator__/#chapter-8-distributedorchestrator","text":"In the previous chapter , we met the DistributedWorker . These are the diligent employees in our distributed workflow factory, constantly picking up tasks from the WorkQueue , processing them using the Workflow blueprint, and saving progress using the CheckpointStore . But wait... if workers just grab tasks from a queue, who puts the first task on the queue? How do we initiate a whole new workflow run? And how do we check on the progress of all these ongoing runs, or maybe stop one if needed?","title":"Chapter 8: DistributedOrchestrator"},{"location":"floxide-tutorial/08__distributedorchestrator__/#whats-the-problem-managing-the-factory-floor","text":"Imagine our video processing factory again. We have workers ( DistributedWorker ) ready at their stations, a job board ( WorkQueue ) to list tasks, and a way to save progress ( CheckpointStore ). But we need someone in charge, a Project Manager , to: Start New Projects: Say \"Okay, let's start processing this new video, 'intro.mp4'. Here's the initial configuration.\" This involves setting up the initial shared Context in the ContextStore and putting the very first task (like \"Download Video\") on the job board ( WorkQueue ). Monitor Progress: Walk around the factory floor (figuratively!) and ask \"How's the processing for 'intro.mp4' going? Is it done yet? Did any problems occur?\" using the RunInfoStore , MetricsStore , ErrorStore , etc. Control Execution: Maybe decide \"Hold on, stop working on 'outro.mp4' for now,\" or \"Cancel the 'test_video.avi' project entirely.\" Gather Reports: Collect information about how many projects are running, which workers are active, and any errors that have happened. Without this central management figure, our distributed factory would be chaotic. Workers wouldn't know when new projects start, and we wouldn't know how they're progressing. In Floxide, this project manager role is filled by the DistributedOrchestrator .","title":"What's the Problem? Managing the Factory Floor"},{"location":"floxide-tutorial/08__distributedorchestrator__/#what-is-a-distributedorchestrator-your-workflow-manager","text":"The DistributedOrchestrator is the central control panel for managing your distributed Floxide workflow runs. It doesn't execute the individual steps itself (that's the DistributedWorker 's job), but it provides the interface to interact with the entire distributed system at a higher level. Think of it as the dashboard and control levers for your workflow factory. Its main responsibilities include: Starting new workflow runs: This involves providing the initial Context , saving it to the ContextStore , and putting the very first WorkItem onto the WorkQueue . This \"seeds\" the workflow run, making it available for workers to pick up. Querying the status of runs: Asking \"Is run 'video_abc' still running, completed, failed, or paused?\" (using RunInfoStore ). Cancelling or pausing runs: Sending signals to stop or temporarily halt processing for a specific run (updating RunInfoStore and purging WorkQueue ). Retrieving run context: Fetching the latest shared context data for a run from the ContextStore . Retrieving run history and metrics: Getting information about past and present runs from RunInfoStore and MetricsStore . Checking for errors: Listing any errors that occurred during a run via ErrorStore . Monitoring worker liveness: Checking which workers are active and responsive. Distributed Emphasis: The DistributedOrchestrator achieves this by interacting with all the shared, distributed stores we've been learning about (and will learn more about in the next chapter ). It talks to the WorkQueue , ContextStore , RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore , etc., to get a unified view of the system and exert control. graph TD Orchestrator[Distributed Orchestrator] -- Manages --> Run1[Workflow Run 'video_abc'] Orchestrator -- Manages --> Run2[Workflow Run 'audio_xyz'] subgraph Shared Infrastructure WQ[(Work Queue)] CtxS[(Context Store)] RIS[(RunInfo Store)] MS[(Metrics Store)] ES[(Error Store)] LS[(Liveness Store)] WIS[(WorkItemState Store)] end Orchestrator -- \"start_run()\" --> WQ & CtxS & RIS Orchestrator -- \"status()\" --> RIS Orchestrator -- \"cancel()\" --> RIS & WQ Orchestrator -- \"errors()\" --> ES Orchestrator -- \"liveness()\" --> LS Orchestrator -- \"metrics()\" --> MS Orchestrator -- \"context()\" --> CtxS","title":"What is a DistributedOrchestrator? Your Workflow Manager"},{"location":"floxide-tutorial/08__distributedorchestrator__/#how-to-use-the-distributedorchestrator","text":"You typically use the DistributedOrchestrator from your main application code, perhaps in response to a user request, an incoming API call, or a scheduled event.","title":"How to Use the DistributedOrchestrator"},{"location":"floxide-tutorial/08__distributedorchestrator__/#1-creating-an-orchestrator-instance","text":"First, you need to create an instance of DistributedOrchestrator . This requires providing instances of your Workflow definition and all the necessary distributed store implementations. These stores must be implementations designed for distributed use (like Redis-backed stores, database stores, etc.) for the system to work across multiple machines. use floxide ::{ DistributedOrchestrator , WorkflowCtx , distributed ::{ OrchestratorBuilder , ContextStore /* other stores */ }, // --- Assume these are your types/implementations --- workflow :: TextProcessor , // Your workflow struct from workflow! macro context :: SimpleContext , // Your context struct // Concrete store implementations (using Redis as an example) RedisWorkQueue , RedisContextStore , RedisRunInfoStore , RedisMetricsStore , RedisErrorStore , RedisLivenessStore , RedisWorkItemStateStore , }; use std :: sync :: Arc ; // Assume these are fully configured clients connected to your backend services let my_workflow = TextProcessor { /* ... node instances ... */ }; let redis_queue = RedisWorkQueue :: new ( /* redis config */ ). await ? ; let redis_context = RedisContextStore :: new ( /* redis config */ ). await ? ; let redis_run_info = RedisRunInfoStore :: new ( /* redis config */ ). await ? ; let redis_metrics = RedisMetricsStore :: new ( /* redis config */ ). await ? ; let redis_errors = RedisErrorStore :: new ( /* redis config */ ). await ? ; let redis_liveness = RedisLivenessStore :: new ( /* redis config */ ). await ? ; let redis_work_item = RedisWorkItemStateStore :: new ( /* redis config */ ). await ? ; // Create the orchestrator instance using the builder let orchestrator = OrchestratorBuilder :: new () . workflow ( my_workflow ) . queue ( redis_queue ) . context_store ( redis_context ) . run_info_store ( redis_run_info ) . metrics_store ( redis_metrics ) . error_store ( redis_errors ) . liveness_store ( redis_liveness ) . work_item_state_store ( redis_work_item ) . build (); println! ( \"Orchestrator created successfully!\" ); This code gathers all the necessary components (the workflow logic and connections to all the shared backend stores) and bundles them into an orchestrator object using a convenient builder pattern.","title":"1. Creating an Orchestrator Instance"},{"location":"floxide-tutorial/08__distributedorchestrator__/#2-starting-a-workflow-run","text":"The most common action is starting a new run. You provide the initial Context and the input data for the first Node . // Define the initial shared context data for this specific run let initial_context_data = SimpleContext {}; // Or MyWorkflowData { api_key: \"...\", ... } let wf_ctx = WorkflowCtx :: new ( initial_context_data ); // Define the input for the very first node in the workflow let initial_input = \"start processing this text\" . to_string (); // Call start_run on the orchestrator let run_id = orchestrator . start_run ( & wf_ctx , initial_input ). await ? ; println! ( \"Successfully started workflow run with ID: {}\" , run_id ); // Example Output: // Successfully started workflow run with ID: 1e7a3c4f-8b0a-4b1e-8c7f-9d2e1b0a3d5e What start_run Does: * Generates a unique ID (like 1e7a... ) for this specific run. * Calls the underlying workflow.start_distributed(...) method. This method: * Initializes the context state in the ContextStore using context_store.set(run_id, initial_context_data) . * Determines the first WorkItem (s) based on the start node and the initial_input . * Pushes the first WorkItem (s) onto the WorkQueue . * Records the new run's information (ID, status= Running , start time) in the RunInfoStore . * Returns the unique run_id . Crucially, start_run does not execute any workflow logic itself. It just sets everything up and puts the first task on the queue. The DistributedWorker s will eventually see this task on the queue and start processing it.","title":"2. Starting a Workflow Run"},{"location":"floxide-tutorial/08__distributedorchestrator__/#3-checking-run-status","text":"You can easily check the status of a run using its ID. let run_id_to_check = \"1e7a3c4f-8b0a-4b1e-8c7f-9d2e1b0a3d5e\" ; // From previous step let status = orchestrator . status ( run_id_to_check ). await ? ; println! ( \"Status for run {}: {:?}\" , run_id_to_check , status ); // Example Output (could be Running, Completed, Failed, Paused, Cancelled): // Status for run 1e7a...: Running This queries the RunInfoStore to get the latest recorded status for that run_id .","title":"3. Checking Run Status"},{"location":"floxide-tutorial/08__distributedorchestrator__/#4-cancelling-a-run","text":"If you need to stop a run prematurely: let run_id_to_cancel = \"1e7a3c4f-8b0a-4b1e-8c7f-9d2e1b0a3d5e\" ; orchestrator . cancel ( run_id_to_cancel ). await ? ; println! ( \"Requested cancellation for run {}\" , run_id_to_cancel ); // Later, checking status might show: // Status for run 1e7a...: Cancelled What cancel Does: * Updates the status in the RunInfoStore to Cancelled . * Calls queue.purge_run(run_id) to remove any pending tasks for this run from the WorkQueue . * (Note: Workers currently processing a task for this run might finish that task, but they won't pick up new ones. Nodes can also check ctx.is_cancelled() to stop early).","title":"4. Cancelling a Run"},{"location":"floxide-tutorial/08__distributedorchestrator__/#5-other-management-functions","text":"The DistributedOrchestrator provides other useful methods: list_runs(filter) : Get a list of all runs, optionally filtering by status. pause(run_id) / resume(run_id) : Temporarily halt or continue a run. errors(run_id) : Retrieve a list of errors recorded for a run from the ErrorStore . metrics(run_id) : Get performance metrics (like step timings) from the MetricsStore . liveness() / list_worker_health() : Check the status of connected workers via the LivenessStore . pending_work(run_id) : See what tasks are currently waiting in the queue for a specific run. context(run_id) : Retrieve the latest saved context data for the run from the ContextStore . wait_for_completion(run_id, poll_interval) : Wait until a run finishes (Completed, Failed, or Cancelled). These methods allow comprehensive monitoring and control over your distributed workflows.","title":"5. Other Management Functions"},{"location":"floxide-tutorial/08__distributedorchestrator__/#under-the-hood-how-start_run-works","text":"Let's trace the main steps when you call orchestrator.start_run(ctx, input) : Generate ID: The start_run method creates a unique run_id (e.g., using UUIDs). Delegate to Workflow: It calls self.workflow.start_distributed(ctx, input, &self.context_store, &self.queue, &run_id) . start_distributed (Inside workflow! Macro Code): Calls context_store.set(run_id, ctx.store) to save the initial context state. Determines the first WorkItem based on the workflow's start node and the provided input . Calls self.queue.enqueue(run_id, first_work_item) to put the first task on the main queue. Record Run Info: Back in start_run , it creates a RunInfo struct (status=Running, started_at=now) and calls self.run_info_store.insert_run(run_info) . Return ID: start_run returns the generated run_id . Here's a sequence diagram illustrating this: sequenceDiagram participant UserApp as Your Application participant Orch as DistributedOrchestrator participant WF as Workflow (start_distributed) participant CtxS as ContextStore participant WQ as WorkQueue participant RIS as RunInfoStore UserApp->>Orch: start_run(ctx, input) Orch->>Orch: Generate run_id Orch->>WF: start_distributed(ctx, input, context_store, queue, run_id) WF->>CtxS: set(run_id, ctx.store) CtxS-->>WF: Ok WF->>WF: Determine first_item WF->>WQ: enqueue(run_id, first_item) WQ-->>WF: Ok WF-->>Orch: Ok Orch->>RIS: insert_run(run_id, status=Running, ...) RIS-->>Orch: Ok Orch-->>UserApp: Return run_id The DistributedOrchestrator struct itself (defined in floxide-core/src/distributed/orchestrator.rs ) mainly holds references to the workflow and all the store implementations. // Simplified from crates/floxide-core/src/distributed/orchestrator.rs use crate :: distributed ::{ ContextStore , ErrorStore , LivenessStore , MetricsStore , RunInfoStore , WorkItemStateStore , WorkQueue }; // Generic struct holding all components pub struct DistributedOrchestrator < W , C , Q , CS , RIS , MS , ES , LS , WIS > where /* type bounds for Workflow, Context, and all Stores */ { workflow : W , queue : Q , context_store : CS , run_info_store : RIS , metrics_store : MS , error_store : ES , liveness_store : LS , work_item_state_store : WIS , // PhantomData used for generic type C phantom : std :: marker :: PhantomData < C > , } impl <.. . > DistributedOrchestrator <.. . > /* type bounds */ { // Constructor takes all components pub fn new ( workflow : W , queue : Q , context_store : CS , run_info_store : RIS , /* etc */ ) -> Self { /* ... store components ... */ } // start_run implementation pub async fn start_run ( & self , ctx : & WorkflowCtx < C > , input : W :: Input , ) -> Result < String , FloxideError > { // 1. Generate unique ID let run_id = uuid :: Uuid :: new_v4 (). to_string (); // 2. Delegate to workflow's seeding primitive self . workflow . start_distributed ( ctx , input , & self . context_store , & self . queue , & run_id ) . await ? ; // Handles context save + queue enqueue // 3. Record initial run info let run_info = RunInfo { /* run_id, status=Running, started_at */ }; self . run_info_store . insert_run ( run_info ). await ? ; // 4. Return the ID Ok ( run_id ) } // ... implementations for status(), cancel(), errors(), etc. ... // These mostly delegate calls to the corresponding store interfaces. } The code clearly shows start_run coordinating between generating an ID, calling the workflow's specific seeding logic ( start_distributed ), and updating the RunInfoStore .","title":"Under the Hood: How start_run Works"},{"location":"floxide-tutorial/08__distributedorchestrator__/#conclusion","text":"The DistributedOrchestrator is the essential management interface for your distributed Floxide workflows. It acts as the central point of control, allowing you to: Initiate new workflow runs in a distributed environment using start_run . Monitor their progress ( status , metrics , errors , liveness ). Control their execution ( cancel , pause , resume ). It achieves this by interacting with the underlying Workflow definition and, crucially, the various shared distributed stores that maintain the state and coordination information for the entire system. Speaking of these stores, while we've mentioned them ( RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore ), let's take a closer look at what each one does in the next chapter. Next: Chapter 9: Distributed Stores ( RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore )","title":"Conclusion"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/","text":"Chapter 9: Distributed Stores ( RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore ) \u00b6 In the previous chapter , we met the DistributedOrchestrator , the project manager for our distributed workflow factory. It starts new runs, checks their status, and manages them. We also have our DistributedWorker s diligently processing tasks from the WorkQueue and saving progress using the CheckpointStore . But how does the orchestrator know the status of a run? How does it track errors? How do workers report they are still alive? How do we count how many tasks succeeded or failed? Simply saving the workflow's core data ( Checkpoint ) isn't enough for monitoring and management. What's the Problem? Keeping Track of Everything \u00b6 Imagine running that video processing factory. Besides the main blueprints ( Workflow ) and the task list ( WorkQueue ), the factory manager ( DistributedOrchestrator ) and the floor supervisors need several specialized logbooks or ledgers to keep things running smoothly: Shared Project Data: Where is the latest version of the project's shared information (like configuration, progress counters, aggregated results) stored? Project Status Board: Which videos are currently being processed? Which are finished? Which failed? Quality Control Log: How many steps completed successfully? How many failed? How many needed retries? Incident Report Book: What errors occurred, when, and during which step? Employee Attendance Sheet: Which workers are currently active? When did they last check in? Detailed Task Checklist: For each video, what's the exact status of each specific step (Download, Extract Audio, etc.) \u2013 is it waiting, running, done, or failed? In a distributed system, these logbooks need to be accessible and updatable by everyone involved (the orchestrator and all workers), even if they are on different computers. Floxide provides a set of specialized \"Distributed Stores\" to act as these shared, digital logbooks. What are Distributed Stores? Specialized Ledgers \u00b6 In a distributed Floxide system, managing state involves several specialized storage traits. Unlike the CheckpointStore (used primarily for local persistence), these stores separate concerns for better scalability and observability in a multi-worker environment: ContextStore : Manages the shared workflow Context data. RunInfoStore : Tracks overall run status. MetricsStore : Counts completed/failed items. ErrorStore : Logs errors encountered. LivenessStore : Tracks worker heartbeats and health. WorkItemStateStore : Tracks the status of individual tasks within a run. These stores work together with the WorkQueue to provide a complete picture and enable coordination between the orchestrator and workers. The Different Stores Explained \u00b6 Let's look at what each store is responsible for: 1. ContextStore : The Shared Project Data Store \u00b6 Purpose: To reliably save, load, and merge the shared Context data associated with each workflow run. Analogy: A central, version-controlled document repository where each project's shared notes and configuration are stored. When multiple people edit concurrently, the system knows how to merge their changes. Key Info Stored: The complete Context object for each run_id . Key Operations: set(run_id, context) : Saves the initial or overwrites the context. get(run_id) : Retrieves the latest saved context. merge(run_id, context_fragment) : Crucially , merges a partial update (e.g., new events appended by a worker) into the existing stored context using the Merge trait defined on the context type (see Chapter 3 ). Used By: Orchestrator: To set the initial context when starting a run, and potentially get the final context. Worker: To get the context before processing a step, and to merge the updated context (with appended events) after successfully completing a step. 2. RunInfoStore : The Project Status Board \u00b6 Purpose: Keeps track of the high-level status of each workflow run. Analogy: A whiteboard listing all ongoing projects (video runs) and their current status (Running, Completed, Failed, Paused, Cancelled). Key Info Stored: For each run_id : status, start time, end time. Used By: Orchestrator: To start_run (sets status to Running), status() , list_runs() , cancel() , pause() , resume() . Worker: Potentially updates status to Completed or Failed upon run completion (often via callbacks). 3. MetricsStore : The Quality Control Log \u00b6 Purpose: Collects numerical metrics about each workflow run's execution. Analogy: A tally sheet tracking how many items passed quality control, how many failed, and how many needed rework (retries). Key Info Stored: For each run_id : count of completed items, failed items, retried items. Used By: Worker: Updates counts after processing each step (e.g., increment completed or failed count). Orchestrator: To query metrics() for a run. 4. ErrorStore : The Incident Report Book \u00b6 Purpose: Records detailed information about errors that occur during workflow execution. Analogy: A logbook where workers report any machine malfunctions or defects found, noting the time, machine, and details. Key Info Stored: For each run_id : a list of errors, including the step ( WorkItem ), the error details, the attempt number, and the timestamp. Used By: Worker: Records an error when a Node 's process method returns an Err or Transition::Abort . Orchestrator: To query errors() for a run to diagnose problems. 5. LivenessStore : The Employee Attendance & Health Monitor \u00b6 Purpose: Tracks whether workers are active and responsive. Analogy: An automated system where employees clock in/out and report their status (Idle, Working, On Break). The manager can see who is currently active. Key Info Stored: For each worker_id : the timestamp of the last heartbeat, current status (Idle, InProgress, Retrying), current task being worked on, error count. Used By: Worker: Periodically sends heartbeat() updates and updates its status ( update_health() ) when starting/finishing/retrying tasks. Orchestrator: To check liveness() or list_worker_health() to monitor the workforce. 6. WorkItemStateStore : The Detailed Task Checklist \u00b6 Purpose: Tracks the specific status of each individual WorkItem (task) within a workflow run. This is more granular than RunInfoStore . Analogy: A detailed checklist attached to each product on the assembly line, where each station worker marks their specific task as Pending, InProgress, Completed, or Failed. Key Info Stored: For each run_id and each unique WorkItem instance within that run: status (Pending, InProgress, Failed, WaitingRetry, PermanentlyFailed, Completed), number of attempts. Used By: Worker: Updates the status before starting a task (Pending -> InProgress), after success (-> Completed), or after failure (-> Failed/WaitingRetry/PermanentlyFailed). Increments attempt count. Orchestrator: Can query list_work_items() to see the detailed state of all steps in a run. Useful for debugging or resuming complex workflows. How They Enable Distribution \u00b6 These stores are defined as traits (interfaces). The key idea is that for a real distributed system, you provide implementations of these traits that use a shared, persistent backend accessible by all orchestrators and workers. Common backends include: Redis: A fast in-memory data store, suitable for all stores. The floxide-redis crate provides implementations like RedisContextStore , RedisRunInfoStore , etc. Databases (SQL/NoSQL): Can be used, especially for stores requiring more structured data like RunInfoStore or ErrorStore . Cloud Services: Specific services like AWS DynamoDB, Google Cloud Datastore, or Azure Cosmos DB can back these stores. When Worker 1 on Machine A successfully processes a step and calls context_store.merge(...) and metrics_store.update_metrics(...) , the underlying Redis or database implementation updates the shared state. Subsequently, the Orchestrator or another worker querying context_store.get(...) or metrics_store.get_metrics(...) will see the results of Worker 1's actions. Using the Stores (Conceptual) \u00b6 You generally don't interact with these stores directly in your Node logic. The DistributedWorker and DistributedOrchestrator use them internally as part of their operations. When the orchestrator calls start_run , it uses RunInfoStore.insert_run(...) . When the orchestrator calls status() , it uses RunInfoStore.get_run(...) . When a worker starts a task, it calls context_store.get(...) , LivenessStore.update_health(...) and WorkItemStateStore.set_status(..., InProgress) . When a worker finishes a task successfully, it calls context_store.merge(...) , MetricsStore.update_metrics(...) and WorkItemStateStore.set_status(..., Completed) . When a worker encounters an error, it calls ErrorStore.record_error(...) , MetricsStore.update_metrics(...) , LivenessStore.update_health(...) and WorkItemStateStore.set_status(..., Failed/...) . Note: It typically does not merge context changes on error. Periodically, the worker calls LivenessStore.update_heartbeat(...) . The Store Traits \u00b6 Each store has a corresponding trait defining its methods. We saw RunInfoStore earlier. Here's a conceptual look at ContextStore : // Simplified concept from floxide-core/src/distributed/context_store.rs use async_trait :: async_trait ; use crate :: context :: Context ; use crate :: distributed :: ContextStoreError ; /// Store for workflow run context data. #[async_trait] pub trait ContextStore < C : Context > : Clone + Send + Sync + ' static { /// Set (or overwrite) the entire context for a run. async fn set ( & self , run_id : & str , context : C ) -> Result < (), ContextStoreError > ; /// Get the current context for a run. async fn get ( & self , run_id : & str ) -> Result < Option < C > , ContextStoreError > ; /// Merge a context fragment into the existing context for a run. /// This relies on the `Merge` trait implemented by type `C`. async fn merge ( & self , run_id : & str , context_fragment : C ) -> Result < (), ContextStoreError > ; // Potentially other methods like `delete` } This defines the standard operations. The key method here is merge , which enables concurrent updates using the event sourcing pattern described in Chapter 3 . Implementations: In-Memory vs. Distributed \u00b6 Floxide provides default InMemory...Store implementations for all these traits (e.g., InMemoryContextStore , InMemoryRunInfoStore ). These are useful for testing and single-process use. The floxide-redis crate provides Redis...Store implementations (e.g., RedisContextStore ) for use with a Redis backend in truly distributed scenarios. Here's a conceptual look at InMemoryContextStore : // Simplified concept use std :: collections :: HashMap ; use std :: sync :: Arc ; use tokio :: sync :: Mutex ; use crate :: merge :: Merge ; // <-- Requires Context to implement Merge /// In-memory context store. #[derive(Clone, Default)] pub struct InMemoryContextStore < C : Context > { inner : Arc < Mutex < HashMap < String , C >>> , } #[async_trait] impl < C : Context > ContextStore < C > for InMemoryContextStore < C > { async fn set ( & self , run_id : & str , context : C ) -> Result < (), ContextStoreError > { let mut map = self . inner . lock (). await ; map . insert ( run_id . to_string (), context ); Ok (()) } async fn get ( & self , run_id : & str ) -> Result < Option < C > , ContextStoreError > { let map = self . inner . lock (). await ; Ok ( map . get ( run_id ). cloned ()) } async fn merge ( & self , run_id : & str , context_fragment : C ) -> Result < (), ContextStoreError > { let mut map = self . inner . lock (). await ; match map . entry ( run_id . to_string ()) { std :: collections :: hash_map :: Entry :: Occupied ( mut entry ) => { // Use the Merge trait to combine the fragment entry . get_mut (). merge ( context_fragment ); } std :: collections :: hash_map :: Entry :: Vacant ( entry ) => { // If no existing context, just insert the fragment entry . insert ( context_fragment ); } } Ok (()) } } Important: For true distribution across multiple machines, you must use implementations backed by external, shared services. The floxide-redis crate provides these for Redis. Under the Hood: Orchestrator and Worker Interaction \u00b6 The orchestrator and workers rely heavily on these stores to coordinate. sequenceDiagram participant Orch as Orchestrator participant Stores as Distributed Stores participant Worker as Distributed Worker Note over Orch, Worker: Managing Run \"video_xyz\" Orch->>Stores: RunInfoStore.insert_run(\"video_xyz\", Running) Orch->>Stores: ContextStore.set(\"video_xyz\", initial_context) Note over Orch, Worker: (Workflow started, first task queued) Worker->>Stores: LivenessStore.update_health(worker_1, InProgress) Worker->>Stores: WorkItemStateStore.set_status(\"video_xyz\", task_A, InProgress) Worker->>Stores: ContextStore.get(\"video_xyz\") Stores-->>Worker: Return current_context Note over Worker: Processing Task A... Node appends events to context Worker->>Stores: ContextStore.merge(\"video_xyz\", context_with_events) Worker->>Stores: MetricsStore.update_metrics(\"video_xyz\", completed_count++) Worker->>Stores: LivenessStore.update_health(worker_1, Idle) Worker->>Stores: WorkItemStateStore.set_status(\"video_xyz\", task_A, Completed) Orch->>Stores: RunInfoStore.get_run(\"video_xyz\") Stores-->>Orch: Return RunInfo { status: Running } Orch->>Stores: ContextStore.get(\"video_xyz\") Stores-->>Orch: Return latest_merged_context Worker->>Stores: LivenessStore.update_heartbeat(worker_1, now) This diagram shows how different actions trigger updates or queries to the various specialized stores, using ContextStore specifically for the shared workflow data. Conclusion \u00b6 The distributed stores ( ContextStore , RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore ) are essential components for managing and monitoring Floxide workflows in a distributed environment. They act as specialized, shared ledgers, providing: State Management ( ContextStore ): Reliably persists and merges shared workflow data using the Merge trait. Visibility: Allows the orchestrator and users to see the status of runs, worker health, errors, and performance metrics. Coordination: Enables workers and the orchestrator to access consistent state information, even when running on different machines. Control: Provides the foundation for actions like cancelling runs or checking worker liveness. While in-memory versions are provided, real distributed applications require implementations backed by shared, persistent storage. The floxide-redis crate provides implementations for Redis. These stores work together with the WorkQueue to make distributed workflow execution manageable and robust. Now that we understand how errors are tracked ( ErrorStore ) and how individual task states are managed ( WorkItemStateStore ), how can we build workflows that automatically handle transient failures? Let's look at how Floxide supports retries. Next: Chapter 10: RetryPolicy & RetryNode","title":"Chapter 9: Distributed Stores"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#chapter-9-distributed-stores-runinfostore-metricsstore-errorstore-livenessstore-workitemstatestore","text":"In the previous chapter , we met the DistributedOrchestrator , the project manager for our distributed workflow factory. It starts new runs, checks their status, and manages them. We also have our DistributedWorker s diligently processing tasks from the WorkQueue and saving progress using the CheckpointStore . But how does the orchestrator know the status of a run? How does it track errors? How do workers report they are still alive? How do we count how many tasks succeeded or failed? Simply saving the workflow's core data ( Checkpoint ) isn't enough for monitoring and management.","title":"Chapter 9: Distributed Stores (RunInfoStore, MetricsStore, ErrorStore, LivenessStore, WorkItemStateStore)"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#whats-the-problem-keeping-track-of-everything","text":"Imagine running that video processing factory. Besides the main blueprints ( Workflow ) and the task list ( WorkQueue ), the factory manager ( DistributedOrchestrator ) and the floor supervisors need several specialized logbooks or ledgers to keep things running smoothly: Shared Project Data: Where is the latest version of the project's shared information (like configuration, progress counters, aggregated results) stored? Project Status Board: Which videos are currently being processed? Which are finished? Which failed? Quality Control Log: How many steps completed successfully? How many failed? How many needed retries? Incident Report Book: What errors occurred, when, and during which step? Employee Attendance Sheet: Which workers are currently active? When did they last check in? Detailed Task Checklist: For each video, what's the exact status of each specific step (Download, Extract Audio, etc.) \u2013 is it waiting, running, done, or failed? In a distributed system, these logbooks need to be accessible and updatable by everyone involved (the orchestrator and all workers), even if they are on different computers. Floxide provides a set of specialized \"Distributed Stores\" to act as these shared, digital logbooks.","title":"What's the Problem? Keeping Track of Everything"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#what-are-distributed-stores-specialized-ledgers","text":"In a distributed Floxide system, managing state involves several specialized storage traits. Unlike the CheckpointStore (used primarily for local persistence), these stores separate concerns for better scalability and observability in a multi-worker environment: ContextStore : Manages the shared workflow Context data. RunInfoStore : Tracks overall run status. MetricsStore : Counts completed/failed items. ErrorStore : Logs errors encountered. LivenessStore : Tracks worker heartbeats and health. WorkItemStateStore : Tracks the status of individual tasks within a run. These stores work together with the WorkQueue to provide a complete picture and enable coordination between the orchestrator and workers.","title":"What are Distributed Stores? Specialized Ledgers"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#the-different-stores-explained","text":"Let's look at what each store is responsible for:","title":"The Different Stores Explained"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#1-contextstore-the-shared-project-data-store","text":"Purpose: To reliably save, load, and merge the shared Context data associated with each workflow run. Analogy: A central, version-controlled document repository where each project's shared notes and configuration are stored. When multiple people edit concurrently, the system knows how to merge their changes. Key Info Stored: The complete Context object for each run_id . Key Operations: set(run_id, context) : Saves the initial or overwrites the context. get(run_id) : Retrieves the latest saved context. merge(run_id, context_fragment) : Crucially , merges a partial update (e.g., new events appended by a worker) into the existing stored context using the Merge trait defined on the context type (see Chapter 3 ). Used By: Orchestrator: To set the initial context when starting a run, and potentially get the final context. Worker: To get the context before processing a step, and to merge the updated context (with appended events) after successfully completing a step.","title":"1. ContextStore: The Shared Project Data Store"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#2-runinfostore-the-project-status-board","text":"Purpose: Keeps track of the high-level status of each workflow run. Analogy: A whiteboard listing all ongoing projects (video runs) and their current status (Running, Completed, Failed, Paused, Cancelled). Key Info Stored: For each run_id : status, start time, end time. Used By: Orchestrator: To start_run (sets status to Running), status() , list_runs() , cancel() , pause() , resume() . Worker: Potentially updates status to Completed or Failed upon run completion (often via callbacks).","title":"2. RunInfoStore: The Project Status Board"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#3-metricsstore-the-quality-control-log","text":"Purpose: Collects numerical metrics about each workflow run's execution. Analogy: A tally sheet tracking how many items passed quality control, how many failed, and how many needed rework (retries). Key Info Stored: For each run_id : count of completed items, failed items, retried items. Used By: Worker: Updates counts after processing each step (e.g., increment completed or failed count). Orchestrator: To query metrics() for a run.","title":"3. MetricsStore: The Quality Control Log"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#4-errorstore-the-incident-report-book","text":"Purpose: Records detailed information about errors that occur during workflow execution. Analogy: A logbook where workers report any machine malfunctions or defects found, noting the time, machine, and details. Key Info Stored: For each run_id : a list of errors, including the step ( WorkItem ), the error details, the attempt number, and the timestamp. Used By: Worker: Records an error when a Node 's process method returns an Err or Transition::Abort . Orchestrator: To query errors() for a run to diagnose problems.","title":"4. ErrorStore: The Incident Report Book"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#5-livenessstore-the-employee-attendance-health-monitor","text":"Purpose: Tracks whether workers are active and responsive. Analogy: An automated system where employees clock in/out and report their status (Idle, Working, On Break). The manager can see who is currently active. Key Info Stored: For each worker_id : the timestamp of the last heartbeat, current status (Idle, InProgress, Retrying), current task being worked on, error count. Used By: Worker: Periodically sends heartbeat() updates and updates its status ( update_health() ) when starting/finishing/retrying tasks. Orchestrator: To check liveness() or list_worker_health() to monitor the workforce.","title":"5. LivenessStore: The Employee Attendance &amp; Health Monitor"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#6-workitemstatestore-the-detailed-task-checklist","text":"Purpose: Tracks the specific status of each individual WorkItem (task) within a workflow run. This is more granular than RunInfoStore . Analogy: A detailed checklist attached to each product on the assembly line, where each station worker marks their specific task as Pending, InProgress, Completed, or Failed. Key Info Stored: For each run_id and each unique WorkItem instance within that run: status (Pending, InProgress, Failed, WaitingRetry, PermanentlyFailed, Completed), number of attempts. Used By: Worker: Updates the status before starting a task (Pending -> InProgress), after success (-> Completed), or after failure (-> Failed/WaitingRetry/PermanentlyFailed). Increments attempt count. Orchestrator: Can query list_work_items() to see the detailed state of all steps in a run. Useful for debugging or resuming complex workflows.","title":"6. WorkItemStateStore: The Detailed Task Checklist"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#how-they-enable-distribution","text":"These stores are defined as traits (interfaces). The key idea is that for a real distributed system, you provide implementations of these traits that use a shared, persistent backend accessible by all orchestrators and workers. Common backends include: Redis: A fast in-memory data store, suitable for all stores. The floxide-redis crate provides implementations like RedisContextStore , RedisRunInfoStore , etc. Databases (SQL/NoSQL): Can be used, especially for stores requiring more structured data like RunInfoStore or ErrorStore . Cloud Services: Specific services like AWS DynamoDB, Google Cloud Datastore, or Azure Cosmos DB can back these stores. When Worker 1 on Machine A successfully processes a step and calls context_store.merge(...) and metrics_store.update_metrics(...) , the underlying Redis or database implementation updates the shared state. Subsequently, the Orchestrator or another worker querying context_store.get(...) or metrics_store.get_metrics(...) will see the results of Worker 1's actions.","title":"How They Enable Distribution"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#using-the-stores-conceptual","text":"You generally don't interact with these stores directly in your Node logic. The DistributedWorker and DistributedOrchestrator use them internally as part of their operations. When the orchestrator calls start_run , it uses RunInfoStore.insert_run(...) . When the orchestrator calls status() , it uses RunInfoStore.get_run(...) . When a worker starts a task, it calls context_store.get(...) , LivenessStore.update_health(...) and WorkItemStateStore.set_status(..., InProgress) . When a worker finishes a task successfully, it calls context_store.merge(...) , MetricsStore.update_metrics(...) and WorkItemStateStore.set_status(..., Completed) . When a worker encounters an error, it calls ErrorStore.record_error(...) , MetricsStore.update_metrics(...) , LivenessStore.update_health(...) and WorkItemStateStore.set_status(..., Failed/...) . Note: It typically does not merge context changes on error. Periodically, the worker calls LivenessStore.update_heartbeat(...) .","title":"Using the Stores (Conceptual)"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#the-store-traits","text":"Each store has a corresponding trait defining its methods. We saw RunInfoStore earlier. Here's a conceptual look at ContextStore : // Simplified concept from floxide-core/src/distributed/context_store.rs use async_trait :: async_trait ; use crate :: context :: Context ; use crate :: distributed :: ContextStoreError ; /// Store for workflow run context data. #[async_trait] pub trait ContextStore < C : Context > : Clone + Send + Sync + ' static { /// Set (or overwrite) the entire context for a run. async fn set ( & self , run_id : & str , context : C ) -> Result < (), ContextStoreError > ; /// Get the current context for a run. async fn get ( & self , run_id : & str ) -> Result < Option < C > , ContextStoreError > ; /// Merge a context fragment into the existing context for a run. /// This relies on the `Merge` trait implemented by type `C`. async fn merge ( & self , run_id : & str , context_fragment : C ) -> Result < (), ContextStoreError > ; // Potentially other methods like `delete` } This defines the standard operations. The key method here is merge , which enables concurrent updates using the event sourcing pattern described in Chapter 3 .","title":"The Store Traits"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#implementations-in-memory-vs-distributed","text":"Floxide provides default InMemory...Store implementations for all these traits (e.g., InMemoryContextStore , InMemoryRunInfoStore ). These are useful for testing and single-process use. The floxide-redis crate provides Redis...Store implementations (e.g., RedisContextStore ) for use with a Redis backend in truly distributed scenarios. Here's a conceptual look at InMemoryContextStore : // Simplified concept use std :: collections :: HashMap ; use std :: sync :: Arc ; use tokio :: sync :: Mutex ; use crate :: merge :: Merge ; // <-- Requires Context to implement Merge /// In-memory context store. #[derive(Clone, Default)] pub struct InMemoryContextStore < C : Context > { inner : Arc < Mutex < HashMap < String , C >>> , } #[async_trait] impl < C : Context > ContextStore < C > for InMemoryContextStore < C > { async fn set ( & self , run_id : & str , context : C ) -> Result < (), ContextStoreError > { let mut map = self . inner . lock (). await ; map . insert ( run_id . to_string (), context ); Ok (()) } async fn get ( & self , run_id : & str ) -> Result < Option < C > , ContextStoreError > { let map = self . inner . lock (). await ; Ok ( map . get ( run_id ). cloned ()) } async fn merge ( & self , run_id : & str , context_fragment : C ) -> Result < (), ContextStoreError > { let mut map = self . inner . lock (). await ; match map . entry ( run_id . to_string ()) { std :: collections :: hash_map :: Entry :: Occupied ( mut entry ) => { // Use the Merge trait to combine the fragment entry . get_mut (). merge ( context_fragment ); } std :: collections :: hash_map :: Entry :: Vacant ( entry ) => { // If no existing context, just insert the fragment entry . insert ( context_fragment ); } } Ok (()) } } Important: For true distribution across multiple machines, you must use implementations backed by external, shared services. The floxide-redis crate provides these for Redis.","title":"Implementations: In-Memory vs. Distributed"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#under-the-hood-orchestrator-and-worker-interaction","text":"The orchestrator and workers rely heavily on these stores to coordinate. sequenceDiagram participant Orch as Orchestrator participant Stores as Distributed Stores participant Worker as Distributed Worker Note over Orch, Worker: Managing Run \"video_xyz\" Orch->>Stores: RunInfoStore.insert_run(\"video_xyz\", Running) Orch->>Stores: ContextStore.set(\"video_xyz\", initial_context) Note over Orch, Worker: (Workflow started, first task queued) Worker->>Stores: LivenessStore.update_health(worker_1, InProgress) Worker->>Stores: WorkItemStateStore.set_status(\"video_xyz\", task_A, InProgress) Worker->>Stores: ContextStore.get(\"video_xyz\") Stores-->>Worker: Return current_context Note over Worker: Processing Task A... Node appends events to context Worker->>Stores: ContextStore.merge(\"video_xyz\", context_with_events) Worker->>Stores: MetricsStore.update_metrics(\"video_xyz\", completed_count++) Worker->>Stores: LivenessStore.update_health(worker_1, Idle) Worker->>Stores: WorkItemStateStore.set_status(\"video_xyz\", task_A, Completed) Orch->>Stores: RunInfoStore.get_run(\"video_xyz\") Stores-->>Orch: Return RunInfo { status: Running } Orch->>Stores: ContextStore.get(\"video_xyz\") Stores-->>Orch: Return latest_merged_context Worker->>Stores: LivenessStore.update_heartbeat(worker_1, now) This diagram shows how different actions trigger updates or queries to the various specialized stores, using ContextStore specifically for the shared workflow data.","title":"Under the Hood: Orchestrator and Worker Interaction"},{"location":"floxide-tutorial/09_distributed_stores___runinfostore____metricsstore____errorstore____livenessstore____workitemstatestore___/#conclusion","text":"The distributed stores ( ContextStore , RunInfoStore , MetricsStore , ErrorStore , LivenessStore , WorkItemStateStore ) are essential components for managing and monitoring Floxide workflows in a distributed environment. They act as specialized, shared ledgers, providing: State Management ( ContextStore ): Reliably persists and merges shared workflow data using the Merge trait. Visibility: Allows the orchestrator and users to see the status of runs, worker health, errors, and performance metrics. Coordination: Enables workers and the orchestrator to access consistent state information, even when running on different machines. Control: Provides the foundation for actions like cancelling runs or checking worker liveness. While in-memory versions are provided, real distributed applications require implementations backed by shared, persistent storage. The floxide-redis crate provides implementations for Redis. These stores work together with the WorkQueue to make distributed workflow execution manageable and robust. Now that we understand how errors are tracked ( ErrorStore ) and how individual task states are managed ( WorkItemStateStore ), how can we build workflows that automatically handle transient failures? Let's look at how Floxide supports retries. Next: Chapter 10: RetryPolicy & RetryNode","title":"Conclusion"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/","text":"Chapter 10: RetryPolicy & RetryNode \u00b6 Welcome to the final chapter in our introductory tour of Floxide! In the previous chapter , we explored the various distributed stores that Floxide uses to keep track of run status, errors, metrics, and worker health, enabling monitoring and management across our distributed system. We saw how errors are recorded in the ErrorStore . But what if an error is just temporary? What's the Problem? Dealing with Temporary Glitches \u00b6 Imagine one step in your workflow involves calling an external web service (an API) to get some data. What happens if the network connection briefly drops just as your Node makes the call? Or what if the external service is momentarily overloaded and returns a temporary error? If the Node simply fails and returns Transition::Abort , the whole workflow run might stop unnecessarily. The problem might fix itself if we just waited a moment and tried again! This is especially common in distributed systems. Tasks running on different machines communicate over networks, which aren't always perfectly reliable. Services might become temporarily unavailable. We need a way to automatically handle these transient failures without stopping the entire workflow. Floxide provides built-in support for automatic retries using the RetryPolicy and RetryNode concepts. The Concepts: Rules and the Enforcer \u00b6 Floxide splits the retry logic into two parts: RetryPolicy : This defines the rules for how retries should happen. Think of it as the instruction manual for retrying a failed task: How many times should we retry? (Maximum attempts) How long should we wait between tries? (Initial backoff duration) Should the wait time increase after each failure? (Backoff strategy: Linear or Exponential) What's the longest we should ever wait? (Maximum backoff duration) Which kinds of errors should we even bother retrying? (Maybe only retry network errors, not configuration errors). RetryNode : This is the enforcer that applies the rules. It's a special kind of Node that wraps your original Node. You give it your potentially fallible Node (like the one calling the API) and a RetryPolicy . When the RetryNode is executed, it tries running the inner Node. If the inner Node succeeds, the RetryNode passes the result along. If the inner Node fails with an error that the RetryPolicy says is retryable: The RetryNode catches the error. It waits for the duration specified by the policy's backoff strategy. It tries running the inner Node again . It repeats this process until the inner Node succeeds, or the maximum number of attempts is reached, or a non-retryable error occurs. This wrapper approach makes it easy to add resilience to existing Nodes without modifying their core logic. How to Use Retries \u00b6 Let's add retries to a hypothetical CallApiNode . 1. Define a RetryPolicy \u00b6 First, you create an instance of RetryPolicy specifying your desired rules. use floxide_core :: retry ::{ RetryPolicy , BackoffStrategy , RetryError }; use std :: time :: Duration ; // Example policy: // - Try up to 5 times in total (1 initial + 4 retries). // - Start with a 100ms wait. // - Double the wait time after each failure (exponential backoff). // - Don't wait longer than 1 second between tries. // - Retry only on \"Generic\" errors (often used for I/O or temporary issues). let my_retry_policy = RetryPolicy :: new ( 5 , // max_attempts Duration :: from_millis ( 100 ), // initial_backoff Duration :: from_secs ( 1 ), // max_backoff BackoffStrategy :: Exponential , // strategy RetryError :: Generic , // which errors to retry ); // You can also add jitter (random variation) to backoff times // let my_retry_policy = my_retry_policy.with_jitter(Duration::from_millis(50)); println! ( \"Created retry policy: {:?}\" , my_retry_policy ); Explanation: * We use RetryPolicy::new(...) to create the policy object. * max_attempts : The total number of tries. * initial_backoff : The wait time before the first retry (after the initial failure). * max_backoff : The ceiling for the wait time, even if the strategy calculates a longer duration. * BackoffStrategy::Exponential : Means the wait time roughly doubles each time (100ms, 200ms, 400ms, 800ms, then capped at 1000ms). Linear would add the initial amount each time (100ms, 200ms, 300ms...). * RetryError::Generic : Specifies that we should only retry if the inner Node fails with FloxideError::Generic . Other errors like FloxideError::Cancelled or FloxideError::Timeout would not be retried with this setting. RetryError::All would retry any error. 2. Wrap Your Node with RetryNode \u00b6 Now, take your original Node instance and wrap it using the floxide_core::retry::with_retry helper function (or directly creating RetryNode ). use floxide_core :: retry :: with_retry ; // Assume CallApiNode is a Node struct we defined elsewhere using `node!` // use crate::nodes::CallApiNode; // Create an instance of the potentially fallible node let original_api_node = CallApiNode { /* ... configuration ... */ }; // Wrap it with our retry policy let resilient_api_node = with_retry ( original_api_node , my_retry_policy . clone ()); println! ( \"Created a RetryNode wrapping CallApiNode.\" ); Explanation: * with_retry takes the original Node instance ( original_api_node ) and the RetryPolicy ( my_retry_policy ). * It returns a new RetryNode instance ( resilient_api_node ). This RetryNode now contains both the original node and the policy. * Crucially, RetryNode itself also implements the Node trait ! This means you can use resilient_api_node anywhere you would use a regular Node. 3. Use the RetryNode in Your Workflow \u00b6 You can now use the wrapped node ( resilient_api_node ) in your workflow! macro definition just like any other node. use floxide ::{ workflow , node , Workflow , Node , Context , Transition , FloxideError }; use serde ::{ Serialize , Deserialize }; use std :: sync :: Arc ; // --- Assume definitions for SimpleContext, StartNode, EndNode, CallApiNode --- // --- Assume `my_retry_policy` is defined as above --- // --- Workflow Definition --- workflow ! { #[derive(Clone, Debug)] pub struct MyApiWorkflow { // Node instances for the workflow step1_start : StartNode , // *** Use the wrapped RetryNode here! *** step2_call_api : floxide_core :: retry :: RetryNode < CallApiNode > , step3_end : EndNode , } context = SimpleContext ; start = step1_start ; edges { step1_start => [ step2_call_api ]; step2_call_api => [ step3_end ]; step3_end => []; }; } // --- How you might create the workflow instance --- // let start_node = StartNode {}; // let original_api_node = CallApiNode { /* ... */ }; // let resilient_api_node = with_retry(original_api_node, my_retry_policy); // Wrap it! // let end_node = EndNode {}; // // let my_workflow = MyApiWorkflow { // step1_start: start_node, // step2_call_api: resilient_api_node, // Assign the wrapped node // step3_end: end_node, // }; Explanation: * In the MyApiWorkflow struct definition inside workflow! , the type for step2_call_api is floxide_core::retry::RetryNode<CallApiNode> . * When you create an instance of MyApiWorkflow , you provide the resilient_api_node (the wrapped one) for the step2_call_api field. * When the Floxide engine runs this workflow and reaches step2_call_api , it will execute the RetryNode 's logic, which internally handles the retries for the CallApiNode according to my_retry_policy . (Alternative) Using #[retry] Attribute: The workflow! macro also provides a convenient #[retry = ...] attribute as a shortcut. You can define a RetryPolicy as a field in your workflow struct and apply it directly to a node field: use floxide_core :: retry :: RetryPolicy ; // ... other imports ... workflow ! { #[derive(Clone, Debug)] pub struct MyApiWorkflowWithAttr { // Define the policy as a field api_retry_policy : RetryPolicy , // Node instances step1_start : StartNode , // Apply the policy directly using the attribute #[retry = api_retry_policy] step2_call_api : CallApiNode , // Note: type is the original Node here! step3_end : EndNode , } context = SimpleContext ; start = step1_start ; edges { /* ... same edges ... */ }; } // --- How you might create this workflow instance --- // let my_policy = RetryPolicy::new(/* ... */); // let start_node = StartNode {}; // let original_api_node = CallApiNode { /* ... */ }; // let end_node = EndNode {}; // // let my_workflow_attr = MyApiWorkflowWithAttr { // api_retry_policy: my_policy, // step1_start: start_node, // step2_call_api: original_api_node, // Provide the original node // step3_end: end_node, // }; // The macro automatically wraps step2_call_api with the policy at compile time. This attribute simplifies the workflow definition when using retries. How it Works Under the Hood \u00b6 What happens when the engine calls process on a RetryNode ? Call Inner: The RetryNode calls the process method of the inner Node it contains, passing along the context and input. Check Result: It examines the Result<Transition, FloxideError> returned by the inner Node. Success? If the result is Ok(Transition::Next(..)) , Ok(Transition::NextAll(..)) , or Ok(Transition::Hold) , the RetryNode simply returns that result immediately. The job is done. Failure? If the result is Ok(Transition::Abort(e)) or Err(e) , the RetryNode catches the error e . Check Policy: It calls self.policy.should_retry(&e, current_attempt) . This checks two things: Have we exceeded max_attempts ? Is this e the kind of error the policy wants to retry (based on RetryError setting)? Retry? If should_retry returns true : Calculate wait time: let backoff = self.policy.backoff_duration(current_attempt); Wait: It calls ctx.wait(backoff).await . Using the WorkflowCtx for waiting is important because it respects overall workflow cancellation or timeouts. Increment current_attempt . Loop back to step 1 (Call Inner again). Give Up? If should_retry returns false : The RetryNode stops trying. It returns the original error e (as Err(e) ). The workflow will then likely handle this as a permanent failure (perhaps using fallback edges defined in workflow! , or ultimately failing the run). Here's a simplified sequence diagram: sequenceDiagram participant Engine as Floxide Engine participant RetryNode as RetryNode.process participant InnerNode as InnerNode.process participant Policy as RetryPolicy participant Ctx as WorkflowCtx Engine->>RetryNode: process(ctx, input) loop Until Success or Max Attempts RetryNode->>InnerNode: process(ctx, input) InnerNode-->>RetryNode: Return Result (e.g., Err(e)) alt Success (Ok(Transition)) RetryNode-->>Engine: Return Ok(Transition) Note over Engine: Processing continues else Failure (Err(e) or Abort(e)) RetryNode->>Policy: should_retry(e, attempt)? Policy-->>RetryNode: Return decision (true/false) alt Retry (decision is true) RetryNode->>Policy: backoff_duration(attempt) Policy-->>RetryNode: Return duration RetryNode->>Ctx: wait(duration) Ctx-->>RetryNode: Wait completes (or Cancelled) RetryNode->>RetryNode: Increment attempt count Note over RetryNode: Loop back to call InnerNode else Give Up (decision is false) RetryNode-->>Engine: Return Err(e) Note over Engine: Error handled (e.g., Abort) end end end Deeper Dive into Code \u00b6 The core logic resides in floxide-core/src/retry.rs . RetryPolicy Struct: // Simplified from crates/floxide-core/src/retry.rs #[derive(Clone, Debug)] pub struct RetryPolicy { pub max_attempts : usize , pub initial_backoff : Duration , pub max_backoff : Duration , pub strategy : BackoffStrategy , // Enum: Linear, Exponential pub jitter : Option < Duration > , pub retry_error : RetryError , // Enum: All, Cancelled, Timeout, Generic } impl RetryPolicy { // Determines if an error should be retried based on policy rules and attempt count. pub fn should_retry ( & self , error : & FloxideError , attempt : usize ) -> bool { // Check max_attempts and error type based on retry_error setting // ... logic ... } // Calculates the duration to wait before the next attempt. pub fn backoff_duration ( & self , attempt : usize ) -> Duration { // Calculate base duration based on strategy (Linear/Exponential) // Cap at max_backoff // Add jitter if configured // ... logic ... } } This struct holds the configuration, and the methods implement the rules for deciding whether to retry and how long to wait. RetryNode Struct: // Simplified from crates/floxide-core/src/retry.rs #[derive(Clone, Debug)] pub struct RetryNode < N > { // Generic over the inner Node type N /// Inner node to invoke. pub inner : N , /// Policy controlling retry attempts and backoff. pub policy : RetryPolicy , } This simple struct just holds the inner node and the policy to apply to it. RetryNode::process Implementation: // Simplified from crates/floxide-core/src/retry.rs use crate :: node :: Node ; use crate :: context ::{ Context , WorkflowCtx }; // Context needed for wait use crate :: retry :: RetryDelay ; // Trait providing ctx.wait() #[async_trait::async_trait] impl < C , N > Node < C > for RetryNode < N > where C : Context + RetryDelay , // Context must support waiting N : Node < C > + Clone + Send + Sync + ' static , // Inner node requirements N :: Input : Clone + Send + ' static , // ... other bounds ... { type Input = N :: Input ; type Output = N :: Output ; async fn process ( & self , ctx : & C , // The workflow context input : Self :: Input , ) -> Result < Transition < Self :: Output > , FloxideError > { let mut attempt = 1 ; // Start with attempt 1 loop { // Keep trying until success or give up // Call the inner node's process method let result = self . inner . process ( ctx , input . clone ()). await ; match result { // Pass through successful transitions immediately Ok ( Transition :: Next ( out )) => return Ok ( Transition :: Next ( out )), Ok ( Transition :: NextAll ( vs )) => return Ok ( Transition :: NextAll ( vs )), Ok ( Transition :: Hold ) => return Ok ( Transition :: Hold ), // Handle failures (Abort or Err) Ok ( Transition :: Abort ( e )) | Err ( e ) => { tracing :: debug ! ( attempt , error =% e , \"RetryNode: caught error\" ); // Check if policy allows retrying this error at this attempt count if self . policy . should_retry ( & e , attempt ) { let backoff = self . policy . backoff_duration ( attempt ); tracing :: info ! ( attempt , backoff =? backoff , \"RetryNode: retrying\" ); // Wait for the backoff period, respecting context cancellation ctx . wait ( backoff ). await ? ; // Returns Err if cancelled/timeout attempt += 1 ; // Increment attempt count continue ; // Go to the next iteration of the loop } else { // Max attempts reached or non-retryable error tracing :: warn ! ( attempt , error =% e , \"RetryNode: giving up\" ); return Err ( e ); // Return the final error } } } } // End of loop } } This implementation clearly shows the loop structure, the call to the inner node, the error handling, the policy checks ( should_retry , backoff_duration ), the crucial ctx.wait(backoff).await call, and the final return of either success or the persistent error. Conclusion \u00b6 RetryPolicy and RetryNode provide a powerful and flexible mechanism for adding automatic retries to individual steps in your Floxide workflows. RetryPolicy defines how to retry (attempts, backoff, error types). RetryNode acts as a wrapper that applies the policy to an existing Node. They work seamlessly with the rest of Floxide, including context-aware waiting ( ctx.wait ) that respects cancellation and timeouts. This significantly improves the fault tolerance of your workflows, especially in distributed environments where transient errors are common. By making individual steps more resilient, you make the entire distributed workflow more robust and reliable. This concludes our introduction to the core concepts of Floxide! You've learned about transitions, nodes, context, workflows, queues, checkpoints, workers, orchestrators, stores, and retries \u2013 everything you need to start building your own easy, distributed workflows.","title":"Chapter 10: RetryPolicy & RetryNode"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/#chapter-10-retrypolicy-retrynode","text":"Welcome to the final chapter in our introductory tour of Floxide! In the previous chapter , we explored the various distributed stores that Floxide uses to keep track of run status, errors, metrics, and worker health, enabling monitoring and management across our distributed system. We saw how errors are recorded in the ErrorStore . But what if an error is just temporary?","title":"Chapter 10: RetryPolicy &amp; RetryNode"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/#whats-the-problem-dealing-with-temporary-glitches","text":"Imagine one step in your workflow involves calling an external web service (an API) to get some data. What happens if the network connection briefly drops just as your Node makes the call? Or what if the external service is momentarily overloaded and returns a temporary error? If the Node simply fails and returns Transition::Abort , the whole workflow run might stop unnecessarily. The problem might fix itself if we just waited a moment and tried again! This is especially common in distributed systems. Tasks running on different machines communicate over networks, which aren't always perfectly reliable. Services might become temporarily unavailable. We need a way to automatically handle these transient failures without stopping the entire workflow. Floxide provides built-in support for automatic retries using the RetryPolicy and RetryNode concepts.","title":"What's the Problem? Dealing with Temporary Glitches"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/#the-concepts-rules-and-the-enforcer","text":"Floxide splits the retry logic into two parts: RetryPolicy : This defines the rules for how retries should happen. Think of it as the instruction manual for retrying a failed task: How many times should we retry? (Maximum attempts) How long should we wait between tries? (Initial backoff duration) Should the wait time increase after each failure? (Backoff strategy: Linear or Exponential) What's the longest we should ever wait? (Maximum backoff duration) Which kinds of errors should we even bother retrying? (Maybe only retry network errors, not configuration errors). RetryNode : This is the enforcer that applies the rules. It's a special kind of Node that wraps your original Node. You give it your potentially fallible Node (like the one calling the API) and a RetryPolicy . When the RetryNode is executed, it tries running the inner Node. If the inner Node succeeds, the RetryNode passes the result along. If the inner Node fails with an error that the RetryPolicy says is retryable: The RetryNode catches the error. It waits for the duration specified by the policy's backoff strategy. It tries running the inner Node again . It repeats this process until the inner Node succeeds, or the maximum number of attempts is reached, or a non-retryable error occurs. This wrapper approach makes it easy to add resilience to existing Nodes without modifying their core logic.","title":"The Concepts: Rules and the Enforcer"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/#how-to-use-retries","text":"Let's add retries to a hypothetical CallApiNode .","title":"How to Use Retries"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/#1-define-a-retrypolicy","text":"First, you create an instance of RetryPolicy specifying your desired rules. use floxide_core :: retry ::{ RetryPolicy , BackoffStrategy , RetryError }; use std :: time :: Duration ; // Example policy: // - Try up to 5 times in total (1 initial + 4 retries). // - Start with a 100ms wait. // - Double the wait time after each failure (exponential backoff). // - Don't wait longer than 1 second between tries. // - Retry only on \"Generic\" errors (often used for I/O or temporary issues). let my_retry_policy = RetryPolicy :: new ( 5 , // max_attempts Duration :: from_millis ( 100 ), // initial_backoff Duration :: from_secs ( 1 ), // max_backoff BackoffStrategy :: Exponential , // strategy RetryError :: Generic , // which errors to retry ); // You can also add jitter (random variation) to backoff times // let my_retry_policy = my_retry_policy.with_jitter(Duration::from_millis(50)); println! ( \"Created retry policy: {:?}\" , my_retry_policy ); Explanation: * We use RetryPolicy::new(...) to create the policy object. * max_attempts : The total number of tries. * initial_backoff : The wait time before the first retry (after the initial failure). * max_backoff : The ceiling for the wait time, even if the strategy calculates a longer duration. * BackoffStrategy::Exponential : Means the wait time roughly doubles each time (100ms, 200ms, 400ms, 800ms, then capped at 1000ms). Linear would add the initial amount each time (100ms, 200ms, 300ms...). * RetryError::Generic : Specifies that we should only retry if the inner Node fails with FloxideError::Generic . Other errors like FloxideError::Cancelled or FloxideError::Timeout would not be retried with this setting. RetryError::All would retry any error.","title":"1. Define a RetryPolicy"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/#2-wrap-your-node-with-retrynode","text":"Now, take your original Node instance and wrap it using the floxide_core::retry::with_retry helper function (or directly creating RetryNode ). use floxide_core :: retry :: with_retry ; // Assume CallApiNode is a Node struct we defined elsewhere using `node!` // use crate::nodes::CallApiNode; // Create an instance of the potentially fallible node let original_api_node = CallApiNode { /* ... configuration ... */ }; // Wrap it with our retry policy let resilient_api_node = with_retry ( original_api_node , my_retry_policy . clone ()); println! ( \"Created a RetryNode wrapping CallApiNode.\" ); Explanation: * with_retry takes the original Node instance ( original_api_node ) and the RetryPolicy ( my_retry_policy ). * It returns a new RetryNode instance ( resilient_api_node ). This RetryNode now contains both the original node and the policy. * Crucially, RetryNode itself also implements the Node trait ! This means you can use resilient_api_node anywhere you would use a regular Node.","title":"2. Wrap Your Node with RetryNode"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/#3-use-the-retrynode-in-your-workflow","text":"You can now use the wrapped node ( resilient_api_node ) in your workflow! macro definition just like any other node. use floxide ::{ workflow , node , Workflow , Node , Context , Transition , FloxideError }; use serde ::{ Serialize , Deserialize }; use std :: sync :: Arc ; // --- Assume definitions for SimpleContext, StartNode, EndNode, CallApiNode --- // --- Assume `my_retry_policy` is defined as above --- // --- Workflow Definition --- workflow ! { #[derive(Clone, Debug)] pub struct MyApiWorkflow { // Node instances for the workflow step1_start : StartNode , // *** Use the wrapped RetryNode here! *** step2_call_api : floxide_core :: retry :: RetryNode < CallApiNode > , step3_end : EndNode , } context = SimpleContext ; start = step1_start ; edges { step1_start => [ step2_call_api ]; step2_call_api => [ step3_end ]; step3_end => []; }; } // --- How you might create the workflow instance --- // let start_node = StartNode {}; // let original_api_node = CallApiNode { /* ... */ }; // let resilient_api_node = with_retry(original_api_node, my_retry_policy); // Wrap it! // let end_node = EndNode {}; // // let my_workflow = MyApiWorkflow { // step1_start: start_node, // step2_call_api: resilient_api_node, // Assign the wrapped node // step3_end: end_node, // }; Explanation: * In the MyApiWorkflow struct definition inside workflow! , the type for step2_call_api is floxide_core::retry::RetryNode<CallApiNode> . * When you create an instance of MyApiWorkflow , you provide the resilient_api_node (the wrapped one) for the step2_call_api field. * When the Floxide engine runs this workflow and reaches step2_call_api , it will execute the RetryNode 's logic, which internally handles the retries for the CallApiNode according to my_retry_policy . (Alternative) Using #[retry] Attribute: The workflow! macro also provides a convenient #[retry = ...] attribute as a shortcut. You can define a RetryPolicy as a field in your workflow struct and apply it directly to a node field: use floxide_core :: retry :: RetryPolicy ; // ... other imports ... workflow ! { #[derive(Clone, Debug)] pub struct MyApiWorkflowWithAttr { // Define the policy as a field api_retry_policy : RetryPolicy , // Node instances step1_start : StartNode , // Apply the policy directly using the attribute #[retry = api_retry_policy] step2_call_api : CallApiNode , // Note: type is the original Node here! step3_end : EndNode , } context = SimpleContext ; start = step1_start ; edges { /* ... same edges ... */ }; } // --- How you might create this workflow instance --- // let my_policy = RetryPolicy::new(/* ... */); // let start_node = StartNode {}; // let original_api_node = CallApiNode { /* ... */ }; // let end_node = EndNode {}; // // let my_workflow_attr = MyApiWorkflowWithAttr { // api_retry_policy: my_policy, // step1_start: start_node, // step2_call_api: original_api_node, // Provide the original node // step3_end: end_node, // }; // The macro automatically wraps step2_call_api with the policy at compile time. This attribute simplifies the workflow definition when using retries.","title":"3. Use the RetryNode in Your Workflow"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/#how-it-works-under-the-hood","text":"What happens when the engine calls process on a RetryNode ? Call Inner: The RetryNode calls the process method of the inner Node it contains, passing along the context and input. Check Result: It examines the Result<Transition, FloxideError> returned by the inner Node. Success? If the result is Ok(Transition::Next(..)) , Ok(Transition::NextAll(..)) , or Ok(Transition::Hold) , the RetryNode simply returns that result immediately. The job is done. Failure? If the result is Ok(Transition::Abort(e)) or Err(e) , the RetryNode catches the error e . Check Policy: It calls self.policy.should_retry(&e, current_attempt) . This checks two things: Have we exceeded max_attempts ? Is this e the kind of error the policy wants to retry (based on RetryError setting)? Retry? If should_retry returns true : Calculate wait time: let backoff = self.policy.backoff_duration(current_attempt); Wait: It calls ctx.wait(backoff).await . Using the WorkflowCtx for waiting is important because it respects overall workflow cancellation or timeouts. Increment current_attempt . Loop back to step 1 (Call Inner again). Give Up? If should_retry returns false : The RetryNode stops trying. It returns the original error e (as Err(e) ). The workflow will then likely handle this as a permanent failure (perhaps using fallback edges defined in workflow! , or ultimately failing the run). Here's a simplified sequence diagram: sequenceDiagram participant Engine as Floxide Engine participant RetryNode as RetryNode.process participant InnerNode as InnerNode.process participant Policy as RetryPolicy participant Ctx as WorkflowCtx Engine->>RetryNode: process(ctx, input) loop Until Success or Max Attempts RetryNode->>InnerNode: process(ctx, input) InnerNode-->>RetryNode: Return Result (e.g., Err(e)) alt Success (Ok(Transition)) RetryNode-->>Engine: Return Ok(Transition) Note over Engine: Processing continues else Failure (Err(e) or Abort(e)) RetryNode->>Policy: should_retry(e, attempt)? Policy-->>RetryNode: Return decision (true/false) alt Retry (decision is true) RetryNode->>Policy: backoff_duration(attempt) Policy-->>RetryNode: Return duration RetryNode->>Ctx: wait(duration) Ctx-->>RetryNode: Wait completes (or Cancelled) RetryNode->>RetryNode: Increment attempt count Note over RetryNode: Loop back to call InnerNode else Give Up (decision is false) RetryNode-->>Engine: Return Err(e) Note over Engine: Error handled (e.g., Abort) end end end","title":"How it Works Under the Hood"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/#deeper-dive-into-code","text":"The core logic resides in floxide-core/src/retry.rs . RetryPolicy Struct: // Simplified from crates/floxide-core/src/retry.rs #[derive(Clone, Debug)] pub struct RetryPolicy { pub max_attempts : usize , pub initial_backoff : Duration , pub max_backoff : Duration , pub strategy : BackoffStrategy , // Enum: Linear, Exponential pub jitter : Option < Duration > , pub retry_error : RetryError , // Enum: All, Cancelled, Timeout, Generic } impl RetryPolicy { // Determines if an error should be retried based on policy rules and attempt count. pub fn should_retry ( & self , error : & FloxideError , attempt : usize ) -> bool { // Check max_attempts and error type based on retry_error setting // ... logic ... } // Calculates the duration to wait before the next attempt. pub fn backoff_duration ( & self , attempt : usize ) -> Duration { // Calculate base duration based on strategy (Linear/Exponential) // Cap at max_backoff // Add jitter if configured // ... logic ... } } This struct holds the configuration, and the methods implement the rules for deciding whether to retry and how long to wait. RetryNode Struct: // Simplified from crates/floxide-core/src/retry.rs #[derive(Clone, Debug)] pub struct RetryNode < N > { // Generic over the inner Node type N /// Inner node to invoke. pub inner : N , /// Policy controlling retry attempts and backoff. pub policy : RetryPolicy , } This simple struct just holds the inner node and the policy to apply to it. RetryNode::process Implementation: // Simplified from crates/floxide-core/src/retry.rs use crate :: node :: Node ; use crate :: context ::{ Context , WorkflowCtx }; // Context needed for wait use crate :: retry :: RetryDelay ; // Trait providing ctx.wait() #[async_trait::async_trait] impl < C , N > Node < C > for RetryNode < N > where C : Context + RetryDelay , // Context must support waiting N : Node < C > + Clone + Send + Sync + ' static , // Inner node requirements N :: Input : Clone + Send + ' static , // ... other bounds ... { type Input = N :: Input ; type Output = N :: Output ; async fn process ( & self , ctx : & C , // The workflow context input : Self :: Input , ) -> Result < Transition < Self :: Output > , FloxideError > { let mut attempt = 1 ; // Start with attempt 1 loop { // Keep trying until success or give up // Call the inner node's process method let result = self . inner . process ( ctx , input . clone ()). await ; match result { // Pass through successful transitions immediately Ok ( Transition :: Next ( out )) => return Ok ( Transition :: Next ( out )), Ok ( Transition :: NextAll ( vs )) => return Ok ( Transition :: NextAll ( vs )), Ok ( Transition :: Hold ) => return Ok ( Transition :: Hold ), // Handle failures (Abort or Err) Ok ( Transition :: Abort ( e )) | Err ( e ) => { tracing :: debug ! ( attempt , error =% e , \"RetryNode: caught error\" ); // Check if policy allows retrying this error at this attempt count if self . policy . should_retry ( & e , attempt ) { let backoff = self . policy . backoff_duration ( attempt ); tracing :: info ! ( attempt , backoff =? backoff , \"RetryNode: retrying\" ); // Wait for the backoff period, respecting context cancellation ctx . wait ( backoff ). await ? ; // Returns Err if cancelled/timeout attempt += 1 ; // Increment attempt count continue ; // Go to the next iteration of the loop } else { // Max attempts reached or non-retryable error tracing :: warn ! ( attempt , error =% e , \"RetryNode: giving up\" ); return Err ( e ); // Return the final error } } } } // End of loop } } This implementation clearly shows the loop structure, the call to the inner node, the error handling, the policy checks ( should_retry , backoff_duration ), the crucial ctx.wait(backoff).await call, and the final return of either success or the persistent error.","title":"Deeper Dive into Code"},{"location":"floxide-tutorial/10__retrypolicy_____retrynode__/#conclusion","text":"RetryPolicy and RetryNode provide a powerful and flexible mechanism for adding automatic retries to individual steps in your Floxide workflows. RetryPolicy defines how to retry (attempts, backoff, error types). RetryNode acts as a wrapper that applies the policy to an existing Node. They work seamlessly with the rest of Floxide, including context-aware waiting ( ctx.wait ) that respects cancellation and timeouts. This significantly improves the fault tolerance of your workflows, especially in distributed environments where transient errors are common. By making individual steps more resilient, you make the entire distributed workflow more robust and reliable. This concludes our introduction to the core concepts of Floxide! You've learned about transitions, nodes, context, workflows, queues, checkpoints, workers, orchestrators, stores, and retries \u2013 everything you need to start building your own easy, distributed workflows.","title":"Conclusion"}]}